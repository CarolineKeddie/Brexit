{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Web Scraping and Spiders with `scrapy`\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrapy'></a>\n",
    "<a scrapy-spiders></a>\n",
    "## What is [Scrapy](http://scrapy.org/)?\n",
    "\n",
    "---\n",
    "\n",
    "> *\"Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them.\"*\n",
    "\n",
    "Below we will walkthrough the creation of a **spider** using scrapy. Spiders are automated processes that will crawl through a webpage or webpages and collect information.\n",
    "\n",
    "> **Note:** This code should be written in a script outside of jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrapy-project'></a>\n",
    "### 1. Create a new Scrapy project\n",
    "\n",
    "In your terminal. `cd` into a directory you want to create your Crawler's folder.  I recommend the desktop for ease of access to the files inside we will need to edit.\n",
    "> `scrapy startproject gumtree`\n",
    "\n",
    "**Should create output that looks like this:**\n",
    "<blockquote>\n",
    "```\n",
    "New Scrapy project 'gumtree', using template directory '/Users/XXXX/anaconda3/lib/python3.6/site-packages/scrapy/templates/project', created in:\n",
    "    /Users/aymericflaisler/gumtree\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd gumtree\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "**That command generates a set of project files:**\n",
    "<blockquote>\n",
    "    \n",
    "\n",
    "```\n",
    "gumtree/\n",
    "    scrapy.cfg\n",
    "    gumtree/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py\n",
    "            ...\n",
    "```\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Generally, these are our files.  We will go into more detail on these soon.\n",
    "\n",
    " * **`scrapy.cfg`:** the project configuration file\n",
    " * **`gumtree/`:** the project’s python module, you’ll later import your code from here.\n",
    " * **`gumtree/items.py`:** the project’s items file.\n",
    " * **`gumtree/pipelines.py`:** the project’s pipelines file.\n",
    " * **`gumtree/settings.py`:** the project’s settings file.\n",
    " * **`gumtree/spiders/`:** a directory where you’ll later put your spiders.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;font-size:49px\" > Warning!</p> <br>\n",
    "\n",
    "Long story, but please add this line to your gumtree/settings.py file before continuing:\n",
    " \n",
    " <blockquote>\n",
    " ```\n",
    " DOWNLOAD_HANDLERS = {'s3': None,}\n",
    " ```\n",
    " </blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;font-size:49px\" > Warning (again)!</p> <br>\n",
    "\n",
    "Some website are blocking scrapy. Let's hide the fact we are doing the requests through the framework.\n",
    "\n",
    "Replace the line starting with `#USER_AGENT` in the same file, with:\n",
    " \n",
    " <blockquote>\n",
    " ```\n",
    " USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:10.0) Gecko/20100101 Firefox/10.0'\n",
    " ```\n",
    " </blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='define-item'></a>\n",
    "### 2. Define an \"item\"\n",
    "\n",
    "Basically, when we define an item, it's telling our new application what it will be collecting.  In essence, an \"item\", is an entity that has attributes (ie: \"title\", \"description\", \"price\", etc) that are descriptive and relate to elements on pages that we will be scraping.  \n",
    "\n",
    "In more precise terms, this is a model (remember the MVC framework from the HTML lesson? ;) ).  Don't worry if this is a foreign concept.  The main idea to understand is that a model has attributes that closely resemble / relate to elements on our target web page(s).\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "# Add this in the items.py file:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class gumtreeItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='spider-crawl'></a>\n",
    "### 3. A spider that crawls\n",
    "\n",
    "An item is a model that resembles data on a webpage.  A spider is something that crawls pages and uses our item model to to get and hold items for us.\n",
    "\n",
    "**Scrapy spiders are python classes.  Let's write our first file, called `gumtree_spider.py` and put it in our `/spiders` directory:**\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class gumtreeSpider(scrapy.Spider):\n",
    "    name = \"gumtree\"\n",
    "    allowed_domains = [\"gumtree.com\"]\n",
    "    start_urls = [\n",
    "        \"https://www.gumtree.com/cars/london/bmw+3+series\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = response.url.split(\"/\")[-1]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```\n",
    "\n",
    "**Next, let's dive in and crawl from our `/gumtree/gumtree` directory:**\n",
    "\n",
    "```\n",
    "> scrapy crawl gumtree\n",
    "```\n",
    "\n",
    "**What just happened?**\n",
    " * Our application requested the URLs from the `start_urls` class attribute.\n",
    " * Ran parse over the content containing the HTML markup, of each request URL.\n",
    " * What else?\n",
    " \n",
    "```python\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.body)\n",
    "```\n",
    "\n",
    "It saved a file in our base project directory.  It should be named based on the end of the URL.  In our case, it should create a file called \"sfc\".  This is taken directly from the Scrapy docs and it's only point is to illustrate the workflow so far.  It is kind of nice to have a reference to our HTML file though.  \n",
    "\n",
    "There might be some errors listed when we crawl, but they are fine for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='xpath-spider'></a>\n",
    "### 4. XPath + parsing with our spider\n",
    "\n",
    "So far, we've defined what fields we'll get, some urls to fetch, and saved some content to a file.  Let's actually do something interesting.\n",
    "\n",
    "**We should let our spider know about the item model we made earlier.  In the head of the `gumtree/gumtree/spiders/gumtree_spider.py`, lets add a new import:**\n",
    "\n",
    "```python\n",
    "from gumtree.items import GumtreeItem\n",
    "from scrapy.selector import Selector\n",
    "```\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "**Let's replace our parse method, to find some data from our gumtree spider response, and map it to our item model, gumtreeItem:**\n",
    "\n",
    "```python\n",
    "def parse(self, response): # define parse function \n",
    "    items = [] # element for storing scraped info\n",
    "\tgt_ = Selector(response) # selector is a function that allows us to grab html from the response(target website)\n",
    "    for elt in gt_.xpath('//article[@class=\"listing-maxi\"]/a'):\n",
    "        item = gumtreeItem()\n",
    "        item['title'] = elt.xpath(\n",
    "            'div[@class=\"listing-content\"]/h2[@class=\"listing-title\"]/text()').extract()\n",
    "        item['link'] = elt.xpath('@href').extract()\n",
    "        item['price'] = elt.xpath(\n",
    "            'div[@class=\"listing-content\"]/span/meta[@itemprop=\"price\"]/@content').extract()\n",
    "\n",
    "        items.append(item)\n",
    "    return items  # shows scraped information as terminal output\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='save-examine'></a>\n",
    "### Save and examine our scraped data\n",
    "\n",
    "By default, we can save our crawled data as csv.  To save our data, we just need to pass a few optional parameters to our crawl call:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "scrapy crawl gumtree -o items.csv -t csv\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "It's always good to iteratively check our data when developing a spider to make sure it's close to what we want. \n",
    "\n",
    "> *Pro tip:  The longer your iterations are between checks, the harder it's going to be to understand what's not working and fix bugs.*\n",
    "\n",
    "You should now have a file called '`items.csv`' in the directory you ran the `scrapy crawl` command from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='follow-links'></a>\n",
    "### Following links for more results\n",
    "\n",
    "100 results is pretty cool but what if we want more?  We need to follow the \"next\" links, and find new pages to grab.  Using the **`parse()`** method of our spider class, we only need to return another type of object.\n",
    "\n",
    "cf.: https://scrapy.readthedocs.io/en/latest/topics/request-response.html#passing-additional-data-to-callback-functions\n",
    "\n",
    "```python\n",
    "class gumtreeSpider(scrapy.Spider):\n",
    "    name = \"gumtree\"\n",
    "    allowed_domains = [\"gumtree.com\"]\n",
    "    start_urls = [\n",
    "        \"https://www.gumtree.com/cars/london/bmw+3+series\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def parse(self, response):  # define parse function\n",
    "        # element for storing scraped info\n",
    "        # selector is a function that allows us to grab html from the response(target website)\n",
    "        gt_ = Selector(response)\n",
    "\n",
    "        for elt in gt_.xpath('//article[@class=\"listing-maxi\"]/a'):\n",
    "            item = GumtreeItem()\n",
    "            item['title'] = elt.xpath(\n",
    "                'div[@class=\"listing-content\"]/h2[@class=\"listing-title\"]/text()').extract()\n",
    "            item['link'] = elt.xpath('@href').extract()\n",
    "            item['price'] = elt.xpath(\n",
    "                'div[@class=\"listing-content\"]/span/meta[@itemprop=\"price\"]/@content').extract()\n",
    "            print(item)\n",
    "            self.items.append(item)\n",
    "\n",
    "        # Does the next page exist?  Let's get it!\n",
    "        next_page = gt_.xpath('//li[@class=\"pagination-next\"]/a/@href')\n",
    "\n",
    "        if (next_page) and ('5' not in next_page.extract()[0]):\n",
    "            url = \"https://www.gumtree.com/\" + next_page.extract()[0]\n",
    "            return self.parse(requests.get(url))\n",
    "        else:\n",
    "            return self.items\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
