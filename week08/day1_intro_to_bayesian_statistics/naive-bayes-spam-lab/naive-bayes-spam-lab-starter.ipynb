{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "## Naive Bayes Spam Filter Using SpamAssassin Data\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, we will write our own spam filter -- one of the many practical uses of Naive Bayes. We will additionally explore methods for visualizing text features in an effort to gain insight and improve our models.\n",
    "\n",
    "### Background\n",
    "\n",
    "The statistical approach for classifying spam was championed by Paul Graham, founder of Y Combinator. We highly recommend you read his classic (and very readable!) essay [A Plan for Spam](http://www.paulgraham.com/spam.html) to gain insight into why Naive Bayes works so well with spam.\n",
    "\n",
    "The reason why Naive Bayes works incredibly well to classify spam is because spam aligns with the independence assumption. Certain keywords in emails -- taken by themselves (e.g. Nigeria / prince) -- typically indicate a spam message.\n",
    "\n",
    "In this lab, the word **ham** indicates an email message that was authorized by the user. Sometimes we receive advertising emails that look like spam, yet we agreed to receive them. This fact can make spam detection more difficult. For a challenge, try classifying the `hard_ham` dataset below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the data\n",
    "\n",
    "We are using the data files from the SpamAssassin dataset:\n",
    "\n",
    "+ https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
    "+ https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
    "+ https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
    "\n",
    "From the command line, you can either use ```curl``` to download into the current directory. For an example of each:\n",
    "\n",
    "    curl http://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2 > 20021010_easy_ham.tar.bz2\n",
    "\n",
    "You can use ```tar xvf <file>``` to extract into the current directory (x - extract, v - verbose, f - read from file). For example:\n",
    "\n",
    "    tar xvf 20021010_easy_ham.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get directory contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./datasets/spam/0000.7b1b73cf36cf9dbc3d64e3f2ee2b91f1',\n",
       " './datasets/spam/0001.bfc8d64d12b325ff385cca8d07b84288',\n",
       " './datasets/spam/0002.24b47bb3ce90708ae29d0aec1da08610',\n",
       " './datasets/spam/0003.4b3d943b8df71af248d12f8b2e7a224a',\n",
       " './datasets/spam/0004.1874ab60c71f0b31b580f313a3f6e777',\n",
       " './datasets/spam/0005.1f42bb885de0ef7fc5cd09d34dc2ba54',\n",
       " './datasets/spam/0006.7a32642f8c22bbeb85d6c3b5f3890a2c',\n",
       " './datasets/spam/0007.859c901719011d56f8b652ea071c1f8b',\n",
       " './datasets/spam/0008.9562918b57e044abfbce260cc875acde',\n",
       " './datasets/spam/0009.c05e264fbf18783099b53dbc9a9aacda']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Ensure base_dir points to the base directory where you extracted your data files!\n",
    "#   Inside this directory should be three folders -- easy_ham, hard_ham, and spam.\n",
    "\n",
    "base_dir = './datasets'\n",
    "easy_ham_files = glob.glob(base_dir + \"/easy_ham/*\")\n",
    "hard_ham_files = glob.glob(base_dir + \"/hard_ham/*\")\n",
    "spam_files = glob.glob(base_dir + \"/spam/*\")\n",
    "spam_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From exmh-workers-admin@redhat.com  Thu Aug 22 12:36:23 2002\n",
      "Return-Path: <exmh-workers-admin@example.com>\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id D03E543C36\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:36:16 -0400 (EDT)\n",
      "Received: from phobos [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:36:16 +0100 (IST)\n",
      "Received: from listman.example.com (listman.example.com [66.187.233.211]) by\n",
      "    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g7MBYrZ04811 for\n",
      "    <zzzz-exmh@example.com>; Thu, 22 Aug 2002 12:34:53 +0100\n",
      "Received: from listman.example.com (localhost.localdomain [127.0.0.1]) by\n",
      "    listman.redhat.com (Postfix) with ESMTP id 8386540858; Thu, 22 Aug 2002\n",
      "    07:35:02 -0400 (EDT)\n",
      "Delivered-To: exmh-workers@listman.example.com\n",
      "Received: from int-mx1.corp.example.com (int-mx1.corp.example.com\n",
      "    [172.16.52.254]) by listman.redhat.com (Postfix) with ESMTP id 10CF8406D7\n",
      "    for <exmh-workers@listman.redhat.com>; Thu, 22 Aug 2002 07:34:10 -0400\n",
      "    (EDT)\n",
      "Received: (from mail@localhost) by int-mx1.corp.example.com (8.11.6/8.11.6)\n",
      "    id g7MBY7g11259 for exmh-workers@listman.redhat.com; Thu, 22 Aug 2002\n",
      "    07:34:07 -0400\n",
      "Received: from mx1.example.com (mx1.example.com [172.16.48.31]) by\n",
      "    int-mx1.corp.redhat.com (8.11.6/8.11.6) with SMTP id g7MBY7Y11255 for\n",
      "    <exmh-workers@redhat.com>; Thu, 22 Aug 2002 07:34:07 -0400\n",
      "Received: from ratree.psu.ac.th ([202.28.97.6]) by mx1.example.com\n",
      "    (8.11.6/8.11.6) with SMTP id g7MBIhl25223 for <exmh-workers@redhat.com>;\n",
      "    Thu, 22 Aug 2002 07:18:55 -0400\n",
      "Received: from delta.cs.mu.OZ.AU (delta.coe.psu.ac.th [172.30.0.98]) by\n",
      "    ratree.psu.ac.th (8.11.6/8.11.6) with ESMTP id g7MBWel29762;\n",
      "    Thu, 22 Aug 2002 18:32:40 +0700 (ICT)\n",
      "Received: from munnari.OZ.AU (localhost [127.0.0.1]) by delta.cs.mu.OZ.AU\n",
      "    (8.11.6/8.11.6) with ESMTP id g7MBQPW13260; Thu, 22 Aug 2002 18:26:25\n",
      "    +0700 (ICT)\n",
      "From: Robert Elz <kre@munnari.OZ.AU>\n",
      "To: Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "Cc: exmh-workers@example.com\n",
      "Subject: Re: New Sequences Window\n",
      "In-Reply-To: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "References: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "    <1029882468.3116.TMDA@deepeddy.vircio.com> <9627.1029933001@munnari.OZ.AU>\n",
      "    <1029943066.26919.TMDA@deepeddy.vircio.com>\n",
      "    <1029944441.398.TMDA@deepeddy.vircio.com>\n",
      "MIME-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Message-Id: <13258.1030015585@munnari.OZ.AU>\n",
      "X-Loop: exmh-workers@example.com\n",
      "Sender: exmh-workers-admin@example.com\n",
      "Errors-To: exmh-workers-admin@example.com\n",
      "X-Beenthere: exmh-workers@example.com\n",
      "X-Mailman-Version: 2.0.1\n",
      "Precedence: bulk\n",
      "List-Help: <mailto:exmh-workers-request@example.com?subject=help>\n",
      "List-Post: <mailto:exmh-workers@example.com>\n",
      "List-Subscribe: <https://listman.example.com/mailman/listinfo/exmh-workers>,\n",
      "    <mailto:exmh-workers-request@redhat.com?subject=subscribe>\n",
      "List-Id: Discussion list for EXMH developers <exmh-workers.example.com>\n",
      "List-Unsubscribe: <https://listman.example.com/mailman/listinfo/exmh-workers>,\n",
      "    <mailto:exmh-workers-request@redhat.com?subject=unsubscribe>\n",
      "List-Archive: <https://listman.example.com/mailman/private/exmh-workers/>\n",
      "Date: Thu, 22 Aug 2002 18:26:25 +0700\n",
      "\n",
      "    Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "18:19:04 Marking 1 hits\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "1 hit\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "using is ...\n",
      "\n",
      "delta$ pick -version\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "delta$ mhparam pick\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "kre\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Exmh-workers mailing list\n",
      "Exmh-workers@redhat.com\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use Python 3's open function, which supports the encoding parameter\n",
    "from io import open\n",
    "\n",
    "# Create list of full-text of all ham and spam emails\n",
    "\n",
    "# read the easy ham files into a list\n",
    "easy_ham_text = []\n",
    "for filename in easy_ham_files:\n",
    "    with open(filename, 'r', encoding='iso-8859-15') as f:\n",
    "        easy_ham_text.append(f.read())\n",
    "\n",
    "# read the easy ham files into a list\n",
    "hard_ham_text = []\n",
    "for filename in hard_ham_files:\n",
    "    with open(filename, 'r', encoding='iso-8859-15') as f:\n",
    "        hard_ham_text.append(f.read())\n",
    "        \n",
    "# read the spam files into a list\n",
    "spam_text = []\n",
    "for filename in spam_files:\n",
    "    with open(filename, 'r', encoding='iso-8859-15') as f:\n",
    "        spam_text.append(f.read())\n",
    "\n",
    "print(easy_ham_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge all of the emails into a single list of emails -- this is our data!\n",
    "ham_and_spam_text = easy_ham_text + spam_text    # extends the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "2551\n",
      "3052\n"
     ]
    }
   ],
   "source": [
    "# How imbalanced is our dataset?\n",
    "\n",
    "print(len(spam_text))\n",
    "print(len(easy_ham_text))\n",
    "print(len(ham_and_spam_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Label the data\n",
    "\n",
    "We now have `ham_and_spam_text`, a single list containing our emails. However, now we need this data to be labeled with what we will predict. In this case, we will make a list of 0s and 1s indicating whether each of these emails is ham (0) or spam (1). Can you make this list, given how we combined the spam and ham into one list above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transform the emails into features\n",
    " \n",
    "We will be using cross validation later to assess performance, so feel free to fit it on the entire dataset for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.A. Fit the model on your data using `CountVectorizer`\n",
    "\n",
    "Using `CountVectorizer` ONLY, transform each email into features. Consider now or later removing stopwords, trying different ngram sizes, making all words lowercase, and/or creating your own features (e.g. presence of an unsubscribe link!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to create a fct to stemmer the words in the email\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Return a callable that handles preprocessing and tokenization\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now apply it using the countvectorizer from sklearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B. Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C. Create a sparse matrix for scikit-learn\n",
    "\n",
    "Create a dense 2-D ndarray `X` from the sparse matrix. Make a 1-D ndarray `y` (the list of labels you created earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Understand and visualize your features\n",
    "\n",
    "Sometimes you may find it difficult to visualize text data. This section provides some exercises that give you insight into how you may modify your text features for improved performance.\n",
    "\n",
    "#### 4.A. Understand sparse matrices and the transform\n",
    "\n",
    "**For email index 1, print(a list of words and counts, sorted by descending count.** Use only the `train_X` sparse matrix along with the `get_feature_names()` method of your vectorizer. The index of each column in `train_X` refers to a word. That word is given by the element at that same index in `get_feature_names()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that the most common words in the email are garbage words from the email header! \n",
    "- You can likely improve your model by filtering these in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.B. Using a histogram, visualize the number of emails each word is in.\n",
    "\n",
    "What distribution is it? From this histogram, will most words in your model be noise or signal? Seeing this histogram, what can you likely do to improve your model? (Hint: To quickly graph this, use `np.sum` on the dense matrix `X` of word counts!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.C. Using a histogram, visualize the number of words each email contains.\n",
    "\n",
    "What is the distribution? Are there any outlier emails? Can you find an explanation why there is there likely a spike in the histogram (e.g. are the emails in this dataset of a particular type?) \n",
    "\n",
    "- Plot the distribution of number of words for spam emails on top of the distribution for ham emails! Would this be a useful additional feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Estimate generalization accuracy\n",
    "\n",
    "Use `cross_val_score` with the models `BernoulliNB` and `MultinomialNB` to assess how well these models classify emails. Can you guess why one may perform better than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Confusion matrix and Classification report\n",
    "\n",
    "Recall that to make a confusion matrix, we will need a specific split. So, use `test_train_split`, manually fit the model using the best performer, then find the confusion matrix and classificaation report (in the `metrics` package). Is your model worse at Type I or Type II errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model on the hard_ham\n",
    "\n",
    "Does it perform as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Improve your model by looking at mispredictions\n",
    "\n",
    "print(the most common words in your false positives of the hard hams versus the spams. (Perhaps write a function of step 4A. Consider using a `collections.Counter` to combine the counts!) Does comparing the most frequent words in the hard ham to those in the spam give you some ideas for how to distinguish between them? What extra features might you add to your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions\n",
    "\n",
    "- Can you improve the score on the hard ham?\n",
    "- Try to improve your model by changing or tweaking the model type. (e.g. LogisticRegression/RandomForests) Why do bigrams result in a lower accuracy? (Because nearly all of them are single-email, so they actually add MORE noise!)\n",
    "- Remove features from your model, e.g. junk words.\n",
    "- Add additional features to your model. Can you specifically come up with ideas that might detect spam vs. ham? For example, does an email have an unsubscribe link?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
