{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Pipelines in Sklearn\n",
    "\n",
    "_Instructor: Aymeric Flaisler_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Learn what a sklearn pipeline is and scenarios they are useful.\n",
    "- Standardize data as part of a pipeline.\n",
    "- Use pipelines with training and testing data.\n",
    "- Practice object oriented programming and building a custom transformation in sklearn.\n",
    "- Put the custom titanic preprocessor into a pipeline.\n",
    "- Investigate the internals of sklearn pipelines.\n",
    "- Practice using the `make_pipeline` function to easily create pipeline objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to pipelines\n",
    "\n",
    "---\n",
    "\n",
    "Often when working with data the same \"process\" is repeated multiple times, which can become tedious to recode. A simple example of this is doing the standardization of data before using regularized regression or other models.\n",
    "\n",
    "Luckily, sklearn has \"Pipelines\" that chain together multiple steps in a data analysis process. By constructing these you can consolidate all of the steps you went through into a single object.\n",
    "\n",
    "This codealong introduces how to use these pipelines and also serves as object oriented programming practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('../datasets/titanic_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-objects'></a>\n",
    "\n",
    "## Loading the pipeline objects\n",
    "\n",
    "---\n",
    "\n",
    "From the `sklearn.pipeline` module we are going to import `Pipeline` and `make_pipeline`.\n",
    "\n",
    "`Pipeline` is the class object that will hold our data analysis process. The `make_pipeline` function is a convenience method that takes in a series of estimators or preprocessing steps and returns a `Pipeline` object.\n",
    "\n",
    "We'll start with the more explicit construction using `Pipeline` and then move on to the convenience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The term \"pipeline\" is jargon for a series of concatenated data transformations. Each stage of a pipeline feeds from the previous stage, i.e. the output of a stage is plugged into the input of the next stage and data flows through the pipeline from beginning to end.\n",
    "\n",
    "\n",
    "![pipeline](../assets/pipeline.png)\n",
    "\n",
    "---\n",
    "\n",
    "Pipelines provide a higher level of abstraction than the individual building blocks of a data science process and are a nice and convenient way to organize analyses.\n",
    "\n",
    "**Let's take a look at the titanic data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch     Fare Embarked  \n",
       "0      0   7.2500        S  \n",
       "1      0  71.2833        C  \n",
       "2      0   7.9250        S  \n",
       "3      0  53.1000        S  \n",
       "4      0   8.0500        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='steps'></a>\n",
    "\n",
    "## Processing steps for the titanic data\n",
    "\n",
    "---\n",
    "\n",
    "There are some preprocessing steps we're going to do before classifying whether or not passengers survived:\n",
    "\n",
    "1. Remove unwanted columns.\n",
    "- Convert categorical string or numeric columns to dummy coded columns.\n",
    "- Standardize the predictor matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we'll do this manually and then later integrate it into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = titanic.drop(['PassengerId', 'Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Pclass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_pclass_cols(df):\n",
    "#     return pd.get_dummies(df, columns=['Pclass'], drop_first=True)\n",
    "\n",
    "def make_pclass_cols(df):\n",
    "    #pclass 1 is reference class\n",
    "    df['Pclass_2'] = df.Pclass.map(lambda x: 1 if x == 2 else 0)\n",
    "    df['Pclass_3'] = df.Pclass.map(lambda x: 1 if x == 3 else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sex_cols(df):\n",
    "    # male is reference class\n",
    "    df['Female'] = df.Sex.map(lambda x: 1 if x == 'female' else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Embarked.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embarked_cols(df):\n",
    "    # embarked S is reference class\n",
    "    df['embarked_C'] = df.Embarked.map(lambda x: 1 if x == 'C' else 0)\n",
    "    df['embarked_Q'] = df.Embarked.map(lambda x: 1 if x == 'Q' else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_pclass_cols(data)\n",
    "data = make_sex_cols(data)\n",
    "data = make_embarked_cols(data)\n",
    "\n",
    "data.drop(['Sex', 'Pclass', 'Embarked'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Female</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   Age  SibSp  Parch     Fare  Pclass_2  Pclass_3  Female  \\\n",
       "0         0  22.0      1      0   7.2500         0         1       0   \n",
       "1         1  38.0      1      0  71.2833         0         0       1   \n",
       "2         1  26.0      0      0   7.9250         0         1       1   \n",
       "3         1  35.0      1      0  53.1000         0         0       1   \n",
       "4         0  35.0      0      0   8.0500         0         1       0   \n",
       "\n",
       "   embarked_C  embarked_Q  \n",
       "0           0           0  \n",
       "1           1           0  \n",
       "2           0           0  \n",
       "3           0           0  \n",
       "4           0           0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardize'></a>\n",
    "## Using a pipeline to standardize the data and fit the model\n",
    "\n",
    "---\n",
    "\n",
    "Now we'll split the data up into the X, y predictor target format, standardize the X matrix, and fit a Logistic Regression model on Survived.\n",
    "\n",
    "First, split into X, y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Survived.values\n",
    "X = data.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52766856,  0.52251079, -0.50678737, ..., -0.75613751,\n",
       "        -0.47261792, -0.20232566],\n",
       "       [ 0.57709388,  0.52251079, -0.50678737, ...,  1.32251077,\n",
       "         2.11587407, -0.20232566],\n",
       "       [-0.25147795, -0.55271372, -0.50678737, ...,  1.32251077,\n",
       "        -0.47261792, -0.20232566],\n",
       "       ...,\n",
       "       [-0.73481151, -0.55271372, -0.50678737, ...,  1.32251077,\n",
       "        -0.47261792, -0.20232566],\n",
       "       [-0.25147795, -0.55271372, -0.50678737, ..., -0.75613751,\n",
       "         2.11587407, -0.20232566],\n",
       "       [ 0.16280796, -0.55271372, -0.50678737, ..., -0.75613751,\n",
       "        -0.47261792,  4.94252683]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardize'></a>\n",
    "## Using a pipeline to standardize the data and fit the model\n",
    "\n",
    "---\n",
    "\n",
    "Now we'll split the data up into the X, y predictor target format, standardize the X matrix, and fit a Logistic Regression model on Survived.\n",
    "\n",
    "First, split into X, y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the LogisticRegression and StandardScaler classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to build one of these pipelines that can combine the steps. Below, we make the standard scaler object as well as the logistic regression object, then put them together into the pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "lr = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
    "\n",
    "\n",
    "lr_pipe = Pipeline(steps=[('ss', ss),\n",
    "                          ('logreg', lr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipelines combine both pre-processing and model building steps into a single object**. \n",
    "\n",
    "Rather than manually building transformations and then feeding them into the models, pipelines tie both of these steps together.\n",
    "\n",
    "Furthermore, pipelines are equipped with the methods of the final estimator step:\n",
    "\n",
    "- `fit()` methods\n",
    "- `predict()` and/or `predict_proba()`\n",
    "- `score()`\n",
    "- ... etc.\n",
    "\n",
    "use the pipeline to fit the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('ss', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logreg', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7935393258426966"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-train-test'></a>\n",
    "\n",
    "## Using pipelines with training and testing data\n",
    "\n",
    "---\n",
    "\n",
    "Next we'll split up this data into training and testing sets. One of the greatest benefits,  to using pipelines is that the preprocessing steps before the model fitting retain the \"fit\" information from the training data to be applied to the testing data.\n",
    "\n",
    "In the pipeline we built above, for example, the first standardization step is \"fit\" on the data we put into it. This means that the `StandardScaler` object takes the mean and standard deviation of that data and performs the procedure with those values.\n",
    "\n",
    "It _also_ means that were we to predict or score on future data, the standard scaler in the pipeline would use the training data's mean and standard deviation to standardize that test data. This is what we want! You definitely don't want to standardize the training and testing data to their own means and standard deviations.\n",
    "\n",
    "There are many scenarios in which the test data is actually data that we have not collected yet. In this case, you need to save the standardization procedure you used on the training data to use on this future data.\n",
    "\n",
    "Split up into training and testing X, y below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline with the training data, then score it on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8089887640449438"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe.fit(Xtrain, ytrain)\n",
    "lr_pipe.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the sake of example, standardize the Xtrain and Xtest separately and show that their normalization parameters differ.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sstrain = StandardScaler()\n",
    "sstest = StandardScaler()\n",
    "lr_sep = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
    "\n",
    "Xtrain_n = sstrain.fit_transform(Xtrain)\n",
    "Xtest_n = sstest.fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.05140449e-01  9.55056180e-02  8.42696629e-02 -5.78354382e+00\n",
      "  2.80898876e-03  3.08988764e-02  2.52808989e-02  5.05617978e-02\n",
      " -2.24719101e-02]\n"
     ]
    }
   ],
   "source": [
    "print sstrain.mean_ - sstest.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='built-in'></a>\n",
    "## Built-in transformations and preprocessing steps\n",
    "\n",
    "---\n",
    "\n",
    "Sklearn comes with a wide variety of useful classes for preprocessing your data prior to model fitting that can be put into pipelines.\n",
    "\n",
    "These can be found in the `sklearn.preprocessing` module and you should feel free to familiarize yourself with them if you want to make use of them in your code:\n",
    "\n",
    "The preprocessing module comes loaded with many very useful pre-processing classes.\n",
    "\n",
    "**Data Manipulators**\n",
    "\n",
    "- Binarizer\n",
    "- KernelCenterer\n",
    "- MaxAbsScaler\n",
    "- MinMaxScaler\n",
    "- Normalizer\n",
    "- OneHotEncoder\n",
    "- PolynomialFeatures\n",
    "- RobustScaler\n",
    "- StandardScaler\n",
    "\n",
    "**Data Imputation**\n",
    "\n",
    "- Imputer\n",
    "\n",
    "**Function Transformer**\n",
    "\n",
    "- FunctionTransformer\n",
    "\n",
    "**Label Manipulators**\n",
    "\n",
    "- LabelBinarizer\n",
    "- LabelEncoder\n",
    "- MultiLabelBinarizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='custom'></a>\n",
    "## Custom transformations\n",
    "\n",
    "---\n",
    "\n",
    "It's not always possible to use a built-in transformation class to do what you want. In fact, it's likely that you're going to run into a scenario where you need a customized preprocessing step before model fitting.\n",
    "\n",
    "Let's take our titanic data, for example. Say we wanted a preprocessor that would remove the columns we didn't want and create the dummy-coded columns before sending it through to the standardization step.\n",
    "\n",
    "Custom transformer classes start with this template code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to import the template classes to create a class that works like an sklearn class\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# our \"TitanicPreprocessor\" is going to do the processing\n",
    "\n",
    "\n",
    "class TitanticPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, *args):\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on this class:\n",
    "\n",
    "1. We have to load in the `BaseEstimator` and `TransformerMixin` classes for our preprocessor to \"inherit\" from in the class definition.\n",
    "- The two required functions are `fit` and `transform`, which will be used to chain the processes together in our pipeline.\n",
    "- The `*args` argument tells the function to expect an arbitrary number of arguments after whatever arguments were listed explicitly.\n",
    "\n",
    "if you are confused about those classes, I recommand reading this article: http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the dummy-coding functions we wrote above to the class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanticPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def make_pclass_cols(self, df):\n",
    "        df['Pclass_2'] = df.Pclass.map(lambda x: 1 if x == 2 else 0)\n",
    "        df['Pclass_3'] = df.Pclass.map(lambda x: 1 if x == 3 else 0)\n",
    "        return df\n",
    "\n",
    "    def make_sex_cols(self, df):\n",
    "        df['Female'] = df.Sex.map(lambda x: 1 if x == 'female' else 0)\n",
    "        return df\n",
    "\n",
    "    def make_embarked_cols(self, df):\n",
    "        df['embarked_C'] = df.Embarked.map(lambda x: 1 if x == 'C' else 0)\n",
    "        df['embarked_Q'] = df.Embarked.map(lambda x: 1 if x == 'Q' else 0)\n",
    "        return df\n",
    "\n",
    "    def transform(self, X, *args):\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a function to remove the unneccessary columns after dummy-coding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanticPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _make_pclass_cols(self, df):\n",
    "        df['Pclass_2'] = df.Pclass.map(lambda x: 1 if x == 2 else 0)\n",
    "        df['Pclass_3'] = df.Pclass.map(lambda x: 1 if x == 3 else 0)\n",
    "        return df\n",
    "\n",
    "    def _make_sex_cols(self, df):\n",
    "        df['Female'] = df.Sex.map(lambda x: 1 if x == 'female' else 0)\n",
    "        return df\n",
    "\n",
    "    def _make_embarked_cols(self, df):\n",
    "        df['embarked_C'] = df.Embarked.map(lambda x: 1 if x == 'C' else 0)\n",
    "        df['embarked_Q'] = df.Embarked.map(lambda x: 1 if x == 'Q' else 0)\n",
    "        return df\n",
    "\n",
    "    def _drop_unused_cols(self, df):\n",
    "        for col in ['PassengerId', 'Name', 'Sex', 'Pclass', 'Embarked']:\n",
    "            try:\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def transform(self, X, *args):\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the `transform` function to perform these preprocessing steps, returning the new DataFrame.**\n",
    "\n",
    "Also, keep track of the final column names in a class attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanticPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "\n",
    "    def _make_pclass_cols(self, df):\n",
    "        df['Pclass_2'] = df.Pclass.map(lambda x: 1 if x == 2 else 0)\n",
    "        df['Pclass_3'] = df.Pclass.map(lambda x: 1 if x == 3 else 0)\n",
    "        return df\n",
    "\n",
    "    def _make_sex_cols(self, df):\n",
    "        df['Female'] = df.Sex.map(lambda x: 1 if x == 'female' else 0)\n",
    "        return df\n",
    "\n",
    "    def _make_embarked_cols(self, df):\n",
    "        df['embarked_C'] = df.Embarked.map(lambda x: 1 if x == 'C' else 0)\n",
    "        df['embarked_Q'] = df.Embarked.map(lambda x: 1 if x == 'Q' else 0)\n",
    "        return df\n",
    "\n",
    "    def _drop_unused_cols(self, df):\n",
    "        for col in ['PassengerId', 'Name', 'Sex', 'Pclass', 'Embarked']:\n",
    "            try:\n",
    "                df = df.drop(col, axis=1)\n",
    "            except:\n",
    "                pass\n",
    "        return df\n",
    "\n",
    "    def transform(self, X, *args):\n",
    "        X = self._make_pclass_cols(X)\n",
    "        X = self._make_sex_cols(X)\n",
    "        X = self._make_embarked_cols(X)\n",
    "        X = self._drop_unused_cols(X)\n",
    "        self.feature_names = X.columns\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='custom-pipe'></a>\n",
    "## Use the custom `TitanticPreprocessor` in a pipeline\n",
    "---\n",
    "\n",
    "We'll put it before the `StandardScaler` in our original pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TitanticPreprocessor()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tprep = TitanticPreprocessor()\n",
    "tprep.fit(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprep = TitanticPreprocessor()\n",
    "ss = StandardScaler()\n",
    "lr = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
    "\n",
    "lr_pipe = Pipeline(steps=[('titanic_prep', tprep),\n",
    "                          ('ss', ss),\n",
    "                          ('logreg', lr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit on the training data and test on the testing data like before, with the new pipeline. You'll need to create a new X, y with the original non-manually preprocessed data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = titanic.Survived.values\n",
    "X = titanic.drop('Survived', axis=1)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7808988764044944"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe.fit(Xtrain, ytrain)\n",
    "lr_pipe.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='internals'></a>\n",
    "## Looking at pipeline internals with `.get_params()`\n",
    "\n",
    "---\n",
    "\n",
    "Use the `.get_params()` function on the pipeline object to get out all of the parameters from the different steps as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg': LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'logreg__C': 0.1,\n",
       " 'logreg__class_weight': None,\n",
       " 'logreg__dual': False,\n",
       " 'logreg__fit_intercept': True,\n",
       " 'logreg__intercept_scaling': 1,\n",
       " 'logreg__max_iter': 100,\n",
       " 'logreg__multi_class': 'ovr',\n",
       " 'logreg__n_jobs': 1,\n",
       " 'logreg__penalty': 'l1',\n",
       " 'logreg__random_state': None,\n",
       " 'logreg__solver': 'liblinear',\n",
       " 'logreg__tol': 0.0001,\n",
       " 'logreg__verbose': 0,\n",
       " 'logreg__warm_start': False,\n",
       " 'memory': None,\n",
       " 'ss': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'ss__copy': True,\n",
       " 'ss__with_mean': True,\n",
       " 'ss__with_std': True,\n",
       " 'steps': [('titanic_prep', TitanticPreprocessor()),\n",
       "  ('ss', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('logreg',\n",
       "   LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False))],\n",
       " 'titanic_prep': TitanticPreprocessor()}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pull out the feature names we stored by accessing our preprocessor object from the dictionary, then pulling out the attribute from that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Age', u'SibSp', u'Parch', u'Fare', u'Pclass_2', u'Pclass_3',\n",
       "       u'Female', u'embarked_C', u'embarked_Q'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe.get_params()['titanic_prep'].feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='make-pipe'></a>\n",
    "## The `make_pipeline()` convenience function\n",
    "\n",
    "---\n",
    "\n",
    "`make_pipeline()` essentially does the same thing as `Pipeline`, the only difference being that you just insert your objects as arguments to the function and it will create the pipeline for you. This means that it will name the steps itself, rather than you doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_pipe = make_pipeline(TitanticPreprocessor(),\n",
    "                          StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('titanticpreprocessor', TitanticPreprocessor()), ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_pipe.fit... etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
