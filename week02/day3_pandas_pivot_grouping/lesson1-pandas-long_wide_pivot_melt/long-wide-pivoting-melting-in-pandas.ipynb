{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## `pandas` Long Format, Wide Format, Pivot Tables, and Melting\n",
    "_Instructor: Aymeric Flaisler_\n",
    "___\n",
    "<br>\n",
    "\n",
    "This lesson is all about **transforming data** using `pandas`. Data transformation is the reorganization of your data set's rows and columns into a different, potentially **more useful shape and format**. \n",
    "\n",
    "The benefits of transforming your data include **better access to relevant information** and **streamlined data manipulation**. As you become more familiar with data sets and their associated operations, you will develop an intuition and appreciation for when it's better to **work row-wise or column-wise**.\n",
    "\n",
    "Different data formats are better for different tasks. It takes time and experience to learn the distinctions. But, for now, we'll introduce the **common structures, transformations, and how to apply these transformations**.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the differences between **long and wide format data**.\n",
    "- Understand **pivot tables**.\n",
    "- Practice transforming data between **long and wide** formats.\n",
    "- Practice creating pivot tables.\n",
    "- Learn how to avoid **common pitfalls and obstacles** in data transformation with `pandas`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Wide Format Data](#wide_format)\n",
    "- [Load and Examine the NPAS Data](#load_nerdy)\n",
    "- [Long Format Data](#long_format)\n",
    "- [Using `pandas`' `.pivot_table()` Function: Long to Wide Format](#pivot_tables)\n",
    "- [MultiIndex/Hierarchical Indices in `pandas`](#multiindex)\n",
    "- [Using `pandas`' `.melt()` Function: Wide to Long Format](#melt)\n",
    "- [Summarizing Data With `.pivot_table()` and Aggregate  Functions](#pivot_table_summarizing)\n",
    "- [The Inner Workings of the MultiIndex](#examining_multiindex)\n",
    "- [Getting Rid of the MultiIndex: \"Flattening\" Data](#multiindex_to_flat)\n",
    "- [A Preface: Merging and Joining With Long and Wide Format Data](#merging_joining_preface)\n",
    "- [`pandas`' `.merge()` function: Joining Long Format vs. Wide Format Data](#pandas_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wide_format'></a>\n",
    "\n",
    "### Wide Format Data\n",
    "\n",
    "---\n",
    "\n",
    "Between \"wide\" and \"long,\" **wide format data is the more intuitive**. It's also a common format for `.csv` files. You've already viewed multiple data sets in wide format throughout this course.\n",
    "\n",
    "Wide format data is structured so that:\n",
    "\n",
    "- Unique IDs, subjects, observations, etc. are represented as **rows**.\n",
    "- Distinct information categories (**variables**) are represented as columns. In other words, there is a **column for every \"variable\"** with its own unique values.\n",
    "- This format can often be a more compact matrix, particularly if little or no information is missing.\n",
    "- It is **not as useful for SQL-style operations**: It can make it much harder or even impossible to **join tables together on a value**.\n",
    "- It can be useful in `pandas` when you need to perform operations on variables **across columns**; for example, multiplying columns together to create a new column.\n",
    "- It is the data format required for statistical modeling (with few exceptions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_nerdy'></a>\n",
    "\n",
    "### Load and Examine the \"Nerdy Personality Attributes\" Data Set\n",
    "\n",
    "---\n",
    "\n",
    "This is a pre-cleaned and modified version of the full \"Nerdy Personality Attributes\" survey, which asked subjects to rate themselves based on questions related to \"nerdiness\" as well as more general personality traits such as openness and extraversion. Researches also collected demographic information from the subjects.\n",
    "\n",
    "You can find the raw data [here](http://personality-testing.info/_rawdata/), along with many other sociological surveys.\n",
    "\n",
    "In this modified version, for the sake of our example, some of the subjects provided data for the survey but not the demographic variables. Because there are missing values and the data is \"messy,\" we have a data cleaning problem.\n",
    "\n",
    "**Load the data (which is in wide format).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerdy_wide_f = './datasets/NPAS_parsed_trunc_wide_missing.csv'\n",
    "\n",
    "# load data and print the dimensions\n",
    "nerdy_wide = pd.read_csv(nerdy_wide_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is in a familiar format in which each column is a variable and each row contains an observation for that variable, corresponding to a distinct subject.\n",
    "\n",
    "*Wide format implies that all of the information for one distinct subject **will be represented in the columns corresponding to that row**. A single subject should not be represented in multiple rows of data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's print the header:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check to see how many null values there are per column.**\n",
    "\n",
    "*Tips:* An easy way is to use the `.isnull()` method associated with the `.sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's count the null values by column:\n",
    "\n",
    "# ..... .isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 691 missing demographic variables are intentional (I specifically enforced that only 700 of the subjects would have demographic information).\n",
    "\n",
    "However, we can see that the `major` variable has 970 missing values. This was not an intentional change.\n",
    "\n",
    "At this point, if we were to just **drop all the rows that have any null values, we would lose at least 970 rows** because of the missing `major` variable.\n",
    "\n",
    "With a numeric column, this would be hard to avoid without \"imputing\" some number to fill in those values. In the simplest case, **imputing the mean or median for missing numeric values** is a common fix (but not ideal).\n",
    "\n",
    "With a **categorical variable** like `major`, we have the luxury of replacing the missing values with a new category label that stands for \"missing.\" \n",
    "\n",
    "**Replace the missing `major` column values with `unknown`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a mask for the missing values in the major column:\n",
    "\n",
    "\n",
    "# set missing values in major to \"unknown\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970\n"
     ]
    }
   ],
   "source": [
    "# if all goes right you should not have any missing values left\n",
    "print (nerdy_wide.major.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='long_format'></a>\n",
    "\n",
    "### Long Format Data\n",
    "\n",
    "---\n",
    "\n",
    "Now, we can load the same data — this time in the format commonly called \"long.\"\n",
    "\n",
    "Long format data is structured so that:\n",
    "\n",
    "- There are potentially multiple `ID` (identification) columns.\n",
    "- There are pairs of columns such as `variable:value` that match a variable key to a value (In the simplest case, there would be a single `variable` column and a single `value` column).\n",
    "- The `variable` column corresponds to the multiple variable columns in a wide format data set. Instead of a column for each variable, you have a row for each `variable:value` pair *per ID*. \n",
    "- This is a standard format for SQL databases because it makes it easier to join different tables together with keys.\n",
    "\n",
    "**Load the long format of the same data below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerdy_long_f = './datasets/NPAS_parsed_trunc_long_missing.csv'\n",
    "\n",
    "# load long data and print the dimensions\n",
    "nerdy_long = pd.read_csv(nerdy_long_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the long format data has far more rows than the wide data set but only three columns.\n",
    "\n",
    "Below you can view the three columns: `subject_id`, `variable`, and `value`.\n",
    "\n",
    "**`subject_id:`**\n",
    "- This is the primary \"key\" or `ID` column. Each `subject_id` will have corresponding entries in the `variable` column — one for each row.\n",
    "\n",
    "**`variable:`**\n",
    "- This column indicates the variable with which the item in the `value` column corresponds.\n",
    "\n",
    "**`value:`**\n",
    "\n",
    "- This contains all values for all variables for all IDs. Essentially, every cell in the wide data set except the `subject_id` is listed in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the header:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the unique values in the `variable` column.**\n",
    "\n",
    "You can see that the unique values in the `variable` column correspond to the column headers in the wide format data.\n",
    "\n",
    "*Tips: use the .unique() method*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the unique values in the variable column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count the unique subject ids:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace the missing values in `major` with `unknown` in the long format data set.**\n",
    "\n",
    "The process for replacing data will be different because of the format. Using logical selection masks with `pandas`' `.loc` syntax is the preferable way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the missing values in major:\n",
    "sum(nerdy_long.value.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the missing values for major in the long dataset with \"unknown\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_id      0\n",
      "variable        0\n",
      "value         279\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check that there is no missing values left:\n",
    "print(nerdy_long[nerdy_long.variable == 'major'].isnull().sum())\n",
    "\n",
    "# you should get only 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pivot_tables'></a>\n",
    "\n",
    "### `Pandas`' `.pivot_table()` Function: Long to Wide Format\n",
    "\n",
    "---\n",
    "\n",
    "The `pd.pivot_table()` function is a powerful tool for both transforming data from long to wide format as well as summarizing data with user-supplied functions.\n",
    "\n",
    "First, we'll look at transforming the long format data back into the wide format using the `.pivot_table()` function.\n",
    "\n",
    "**Important parameters for the `.pivot_table()` function include:**\n",
    "\n",
    "> The `pivot_table()` function takes a DataFrame to pivot as its first argument. \n",
    "    \n",
    "- **`columns`**: This is the list of columns in the long format data to be transformed back into columns in the wide format. After pivoting, each unique value in the long format column becomes a header in the wide format.\n",
    "- **`values`**: A single column indicating the values to use when pivoting and filling the new wide format columns.\n",
    "- **`index`**: Columns in the long format data that are index variables. These will be left as single columns, not spread out by unique value like in the `columns` parameter.\n",
    "- **`aggfunc`**: Often `.pivot_table()` is used to perform a summary of the data. `aggfunc` stands for \"aggregation function.\" It's required and defaults to `np.mean()`. You can also insert your own function, which we'll demonstrate below.\n",
    "- **`fill_value`**: If a cell is missing for the wide format data, this value will fill it in.\n",
    "    \n",
    "Next we'll put in our own function — `select_item_or_nan()` — to the `aggfunc` keyword argument. Because my `subject_id` column has a single variable value for each ID, I just want the single element in the long format value cell. My data is messy, so I have to write a function to check for places it could break. \n",
    "\n",
    "**Note:** Passed into my function, `x` will be a Series object. I pull out the first element of that using the `.iloc` indexer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make sure value has only values:\n",
    "\n",
    "*Note: The lambda operator or lambda function is used for creating small, one-time and anonymous function objects in Python. This is not the object of this lesson. We will cover it at a later stage. Do not worry about understanding it for now.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask with true or false if we can convert to a numerical value\n",
    "mask = nerdy_long.value.map(lambda x: is_float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now remove non numeric values using the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerdy_long_only_num = nerdy_long[mask].copy()\n",
    "nerdy_long_only_num.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69874, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerdy_long_only_num.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the column `value`  from the dataframe `nerdy_long_only_num` to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "# nerdy_long_only_num['value'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally pivot the data on subject_id and variable using .pivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nerdy_long_only_num.pivot(index='subject_id',columns='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with all the data and .pivot_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we define a custom function that returns a value if it exist and NaN if not\n",
    "def select_item_or_nan(x):\n",
    "    x = x.iloc[0]\n",
    "#     print(x, type(x))\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a few seconds to run.\n",
    "nerdy_wide_pv = pd.pivot_table(nerdy_long, columns=['variable'], values='value',\n",
    "                            index=['subject_id'], aggfunc=select_item_or_nan , fill_value=np.nan)\n",
    "# 'pv' for 'pivot version.'\n",
    "nerdy_wide_pv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multiindex'></a>\n",
    "\n",
    "### MultiIndex/Hierarchical Indices in `pandas`\n",
    "\n",
    "---\n",
    "\n",
    "First, let's reload a fresh copy of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reload the data\n",
    "nerdy_long = pd.read_csv('./datasets/nerdy_long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_item_or_nan(x):\n",
    "#     print(type(x))\n",
    "    x = x.iloc[0]\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a few seconds to run.\n",
    "nerdy_wide_pv = pd.pivot_table(nerdy_long, columns=['variable'], values='value',\n",
    "                            index=['subject_id'], aggfunc=select_item_or_nan , fill_value=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the header, you can see that the format of the new wide data **is *not* the same as our originally loaded wide format**. `pandas` implements something called **MultiIndexing** or **hierarchical indexing**, which allows for \"tiered\" row and column labels.\n",
    "\n",
    "Right now the MultiIndexing is not terrible but can get **confusing and annoying**, which we will experience later in this lesson.\n",
    "\n",
    "The main difference is that we now have a `variable` name in the top left corner, which is **\"labeling\"** our columns (and corresponds to the name of our original column in the long format data). The row indexer has become our **single key/ID variable**, `subject_id`. The columns are what we would expect here: **Each one is a variable**, like in the original wide format data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variable</th>\n",
       "      <th>academic_over_social</th>\n",
       "      <th>age</th>\n",
       "      <th>anxious</th>\n",
       "      <th>bookish</th>\n",
       "      <th>books_over_parties</th>\n",
       "      <th>calm</th>\n",
       "      <th>collect_books</th>\n",
       "      <th>conventional</th>\n",
       "      <th>critical</th>\n",
       "      <th>dependable</th>\n",
       "      <th>diagnosed_autistic</th>\n",
       "      <th>disorganized</th>\n",
       "      <th>education</th>\n",
       "      <th>engnat</th>\n",
       "      <th>enjoy_learning</th>\n",
       "      <th>excited_about_research</th>\n",
       "      <th>extraverted</th>\n",
       "      <th>familysize</th>\n",
       "      <th>gender</th>\n",
       "      <th>hand</th>\n",
       "      <th>hobbies_over_people</th>\n",
       "      <th>in_advanced_classes</th>\n",
       "      <th>intelligence_over_appearance</th>\n",
       "      <th>interested_science</th>\n",
       "      <th>introspective</th>\n",
       "      <th>libraries_over_publicspace</th>\n",
       "      <th>like_dry_topics</th>\n",
       "      <th>like_hard_material</th>\n",
       "      <th>like_science_fiction</th>\n",
       "      <th>like_superheroes</th>\n",
       "      <th>major</th>\n",
       "      <th>married</th>\n",
       "      <th>online_over_inperson</th>\n",
       "      <th>opennness</th>\n",
       "      <th>play_many_videogames</th>\n",
       "      <th>playes_rpgs</th>\n",
       "      <th>prefer_fictional_people</th>\n",
       "      <th>race_arab</th>\n",
       "      <th>race_asian</th>\n",
       "      <th>race_black</th>\n",
       "      <th>race_hispanic</th>\n",
       "      <th>race_native_american</th>\n",
       "      <th>race_native_austrailian</th>\n",
       "      <th>race_nerdy</th>\n",
       "      <th>race_white</th>\n",
       "      <th>read_tech_reports</th>\n",
       "      <th>religion</th>\n",
       "      <th>reserved</th>\n",
       "      <th>socially_awkward</th>\n",
       "      <th>strange_person</th>\n",
       "      <th>sympathetic</th>\n",
       "      <th>urban</th>\n",
       "      <th>voted</th>\n",
       "      <th>was_odd_child</th>\n",
       "      <th>watch_science_shows</th>\n",
       "      <th>writing_novel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>biophysics</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>biology</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variable   academic_over_social   age anxious bookish books_over_parties calm  \\\n",
       "subject_id                                                                      \n",
       "0                           5.0   NaN     1.0     5.0                5.0  7.0   \n",
       "1                           2.0  50.0     4.0     4.0                4.0  6.0   \n",
       "2                           5.0  22.0     7.0     5.0                5.0  2.0   \n",
       "3                           5.0   NaN     4.0     4.0                5.0  7.0   \n",
       "4                           4.0   NaN     3.0     5.0                5.0  6.0   \n",
       "\n",
       "variable   collect_books conventional critical dependable diagnosed_autistic  \\\n",
       "subject_id                                                                     \n",
       "0                    5.0          1.0      1.0        7.0                2.0   \n",
       "1                    5.0          1.0      3.0        5.0                2.0   \n",
       "2                    5.0          1.0      6.0        3.0                2.0   \n",
       "3                    5.0          1.0      2.0        7.0                2.0   \n",
       "4                    4.0          2.0      5.0        4.0                2.0   \n",
       "\n",
       "variable   disorganized education engnat enjoy_learning  \\\n",
       "subject_id                                                \n",
       "0                   1.0       NaN    NaN            5.0   \n",
       "1                   5.0       4.0    1.0            3.0   \n",
       "2                   6.0       3.0    2.0            5.0   \n",
       "3                   1.0       NaN    NaN            1.0   \n",
       "4                   5.0       NaN    NaN            4.0   \n",
       "\n",
       "variable   excited_about_research extraverted familysize gender hand  \\\n",
       "subject_id                                                             \n",
       "0                             5.0         1.0        NaN    NaN  NaN   \n",
       "1                             4.0         5.0        3.0    2.0  1.0   \n",
       "2                             5.0         1.0        2.0    2.0  1.0   \n",
       "3                             5.0         7.0        NaN    NaN  NaN   \n",
       "4                             4.0         2.0        NaN    NaN  NaN   \n",
       "\n",
       "variable   hobbies_over_people in_advanced_classes  \\\n",
       "subject_id                                           \n",
       "0                          4.0                 5.0   \n",
       "1                          1.0                 4.0   \n",
       "2                          5.0                 5.0   \n",
       "3                          5.0                 5.0   \n",
       "4                          4.0                 4.0   \n",
       "\n",
       "variable   intelligence_over_appearance interested_science introspective  \\\n",
       "subject_id                                                                 \n",
       "0                                   3.0                3.0           5.0   \n",
       "1                                   3.0                4.0           3.0   \n",
       "2                                   5.0                5.0           5.0   \n",
       "3                                   5.0                5.0           2.0   \n",
       "4                                   4.0                4.0           1.0   \n",
       "\n",
       "variable   libraries_over_publicspace like_dry_topics like_hard_material  \\\n",
       "subject_id                                                                 \n",
       "0                                 5.0             3.0                5.0   \n",
       "1                                 5.0             1.0                3.0   \n",
       "2                                 4.0             5.0                4.0   \n",
       "3                                 5.0             3.0                5.0   \n",
       "4                                 4.0             4.0                5.0   \n",
       "\n",
       "variable   like_science_fiction like_superheroes       major married  \\\n",
       "subject_id                                                             \n",
       "0                           5.0              5.0         NaN     NaN   \n",
       "1                           4.0              4.0  biophysics     1.0   \n",
       "2                           5.0              3.0     biology     1.0   \n",
       "3                           5.0              5.0         NaN     NaN   \n",
       "4                           4.0              4.0         NaN     NaN   \n",
       "\n",
       "variable   online_over_inperson opennness play_many_videogames playes_rpgs  \\\n",
       "subject_id                                                                   \n",
       "0                           4.0       6.0                  5.0         3.0   \n",
       "1                           3.0       5.0                  1.0         4.0   \n",
       "2                           5.0       6.0                  5.0         5.0   \n",
       "3                           5.0       7.0                  5.0         5.0   \n",
       "4                           5.0       7.0                  3.0         4.0   \n",
       "\n",
       "variable   prefer_fictional_people race_arab race_asian race_black  \\\n",
       "subject_id                                                           \n",
       "0                              5.0       0.0        0.0        0.0   \n",
       "1                              3.0       0.0        0.0        0.0   \n",
       "2                              5.0       0.0        0.0        0.0   \n",
       "3                              4.0       0.0        0.0        0.0   \n",
       "4                              5.0       0.0        0.0        0.0   \n",
       "\n",
       "variable   race_hispanic race_native_american race_native_austrailian  \\\n",
       "subject_id                                                              \n",
       "0                    0.0                  0.0                     0.0   \n",
       "1                    0.0                  0.0                     0.0   \n",
       "2                    0.0                  0.0                     0.0   \n",
       "3                    0.0                  0.0                     0.0   \n",
       "4                    0.0                  0.0                     0.0   \n",
       "\n",
       "variable   race_nerdy race_white read_tech_reports religion reserved  \\\n",
       "subject_id                                                             \n",
       "0                 0.0        1.0               5.0      NaN      7.0   \n",
       "1                 0.0        1.0               4.0      1.0      5.0   \n",
       "2                 0.0        1.0               5.0      1.0      7.0   \n",
       "3                 0.0        1.0               4.0      NaN      2.0   \n",
       "4                 0.0        1.0               5.0      NaN      6.0   \n",
       "\n",
       "variable   socially_awkward strange_person sympathetic urban voted  \\\n",
       "subject_id                                                           \n",
       "0                       5.0            5.0         7.0   NaN   NaN   \n",
       "1                       5.0            4.0         5.0   2.0   1.0   \n",
       "2                       5.0            5.0         2.0   1.0   1.0   \n",
       "3                       5.0            5.0         6.0   NaN   NaN   \n",
       "4                       0.0            5.0         5.0   NaN   NaN   \n",
       "\n",
       "variable   was_odd_child watch_science_shows writing_novel  \n",
       "subject_id                                                  \n",
       "0                    5.0                 5.0           3.0  \n",
       "1                    3.0                 5.0           1.0  \n",
       "2                    5.0                 5.0           4.0  \n",
       "3                    5.0                 5.0           4.0  \n",
       "4                    5.0                 4.0           1.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the header of the widened dataset\n",
    "nerdy_wide_pv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the null values from our recreated wide format data. How many unique subjects do we have?**\n",
    "\n",
    "Remember our `subject_id` is now the **index**, so we can access it using the `.index` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop the null values and count unique subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the `subject_id` index back into a column (ie: reset the index without dropping it)**\n",
    "\n",
    "We can use the DataFrame function `.reset_index()` to move `subject_id` into a column and create a new index. We now have a DataFrame with the same format we loaded the original wide format data in previously. The only exception is that we still have the `variable` column label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the index to a column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the column label.**\n",
    "\n",
    "You can remove the column label (which can be confusing during print statements) by setting the `.columns.name` attribute to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the columns label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='melt'></a>\n",
    "\n",
    "### Using pandas' `.melt()` Function: Wide to Long Format\n",
    "\n",
    "---\n",
    "First, let's reload a fresh copy of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerdy_wide_flat = pd.read_csv('./datasets/nerdy_wide_flat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`.melt()`** is a function that essentially performs the inverse of `.pivot_table()` on DataFrames.\n",
    "\n",
    "`.melt()` takes a DataFrame as its first argument. Additional arguments typically used with this function are:\n",
    "\n",
    "- **`id_vars`**: The column or columns that will be ID variables. ID variables contain data points specified by the `variable` and `value` columns.\n",
    "- **`value_vars`**: A list that specifies which columns should be converted into single `value` and `variable` columns.\n",
    "- **`var_name`**: The header name of the `variable` column (default='variable').\n",
    "- **`value_name`**: The header name of the `value` column (default='value').\n",
    "\n",
    "**First, subset the wide format data into just columns: `['subject_id','anxious','booking','calm','major']`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the wide data:\n",
    "# nerdy_subset = nerdy_wide_flat[...??]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `.melt()` on the subset with `id_vars=['subject_id','major']`.**\n",
    "\n",
    "Print out the shape of the data and the header. The non-ID columns and their values are now represented by the `variable:value` column pair.\n",
    "\n",
    "**Note**: When you only specify the `id_vars`, the remaining columns become part of the `variable` and `value` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nerdy_sub_long = pd.melt(nerdy_subset, id_vars=['subject_id','major'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't specify `major` as an `id_var`, it will end up in the `variable` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### with two value_vars\n",
    "# nerdy_sub_long = pd.melt(nerdy_subset, id_vars='subject_id')\n",
    "# print nerdy_subset.shape, nerdy_sub_long.shape\n",
    "# nerdy_sub_long.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### with all value_vars\n",
    "# nerdy_sub_long = pd.melt(nerdy_wide_flat, id_vars=['subject_id','major'], \n",
    "#                          value_vars=['anxious','bookish','calm'])\n",
    "# print nerdy_wide_flat.shape, nerdy_sub_long.shape\n",
    "# nerdy_sub_long.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more `id_vars` that we specify, the flatter our DataFrame will be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can achieve the same result without having to subset the DataFrame first by simply specifying the `value_vars` keyword argument. The output DataFrame will then only contain the data specified in the `id_vars` and `value_vars` arguments.\n",
    "\n",
    "**Create the same DataFrame with `.melt()` on the full wide data set, but select the columns to use with the `value_vars` argument.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nerdy_sub_long = pd.melt(nerdy_wide_flat, id_vars=['subject_id','major'], \n",
    "#                          value_vars=['anxious','bookish','calm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `value` column is still a string, so we can convert it to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure the value is a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pivot_table_summarizing'></a>\n",
    "\n",
    "### Summarizing Your Data With  `.pivot_table()` and Aggregate Functions\n",
    "\n",
    "---\n",
    "First, let's reload a fresh copy of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerdy_sub_long = pd.read_csv('./datasets/nerdy_sub_long.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of you who have experience with Excel, `pandas`' `.pivot_table()` accomplishes the same thing. It's more powerful but harder to use than the spreadsheet version.\n",
    "\n",
    "`.pivot_table()` can take in a variable, value, and index to group by and apply aggregate functions to summarize the data. \n",
    "\n",
    "**Note**: Be careful that your index variable is not pulling out unique rows (For example, `subject_id` by variable would only have one value to send into the aggregate functions).\n",
    "\n",
    "Below, I am calling the `.pivot_table()` function with:\n",
    "\n",
    "- The long format data as the first argument.\n",
    "- `variable` specified as the columns that indicate the variable names (groups).\n",
    "- `value` specified as the column that contains the data per variable.\n",
    "- `major` as the index; the rows will be grouped by `major`.\n",
    "- `np.mean`, `np.median`, `np.std`, and `len` as aggregate functions. These will be calculated for each `major-by-variable` group.\n",
    "- A `fill_value` of `np.nan` for cells in the output table that have no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerdy_major_summary = pd.pivot_table(nerdy_sub_long, columns=['variable'], values='value',\n",
    "                                     index=['major'], aggfunc=[np.mean, np.median, np.std, len],\n",
    "                                     fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output DataFrame gives you a \"hierarchical\" column index — the three variables for each aggregate function. The row index is the `major` groups.\n",
    "\n",
    "If you apply more index variables, the row indices will also become hierarchical! However, this can quickly make for a bloated DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the header of the pivot table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='examining_multiindex'></a>\n",
    "\n",
    "### The Inner Workings of the MultiIndex\n",
    "\n",
    "--- \n",
    "\n",
    "The `.names` attribute on the index and columns will show you the hierarchy of labels. The row index is `'major'`, and the two column indices are `None` and `'variable'` (The aggregate functions get no label from `.pivot_table()` in this case). \n",
    "\n",
    "If you print out the columns, you can see the data set has become a `pandas` `MultiIndex` object that has levels, labels, and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['major']\n",
      "[None, 'variable']\n",
      "MultiIndex(levels=[['mean', 'median', 'std', 'len'], ['anxious', 'bookish', 'calm']],\n",
      "           labels=[[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]],\n",
      "           names=[None, 'variable'])\n"
     ]
    }
   ],
   "source": [
    "print(nerdy_major_summary.index.names)\n",
    "print(nerdy_major_summary.columns.names)\n",
    "print(nerdy_major_summary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing along the hierarchical column headers SHOULD NOT be done with chained bracket keys — i.e., including the top-level column label in the first bracket, and so on, down to the bottom level.\n",
    "\n",
    "Instead you should use a tuple like index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nerdy_major_summary.loc[:,('mean','anxious')].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nerdy_major_summary.loc[:,('mean',['anxious','bookish'])].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multiindex_to_flat'></a>\n",
    "\n",
    "### Getting Rid of the MultiIndex: \"Flattening\" Data\n",
    "\n",
    "---\n",
    "\n",
    "MultiIndex DataFrames hold great potential and are a cool concept. That being said, the overhead and confusion on how to subset/mask them is most often not worth it, especially when your data needs to be formatted for insertion into a model.\n",
    "\n",
    "The most reliable way to \"flatten\" a MultiIndexed DataFrame is with the `.to_records()` function. To make this a new DataFrame, it needs to be wrapped in a `pd.DataFrame()` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(nerdy_major_summary.to_records())\n",
    "# nerdy_major_flat = pd.DataFrame(nerdy_major_summary.to_records())\n",
    "# nerdy_major_flat.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the new column names are tuples of the hierarchy of MultiIndexed columns. For example, you could convert these to new, more easily indexed columns with something like a list comprehension.\n",
    "\n",
    "The **`.eval()`** function takes a string and trys to evaluate it as if it were a Python command.\n",
    "\n",
    "**Use a list comprehension and the `.eval()` function to convert the flattened MultiIndexed columns to something more readable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the column names with list comprehension and eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='merging_joining_preface'></a>\n",
    "\n",
    "### A Preface (independent practice): Merging and Joining With Long and Wide Format Data\n",
    "\n",
    "---\n",
    "\n",
    "You will be merging and joining data sets extensively throughout this course and in your future careers. However, it is important to note the differences between merging long and wide data sets together.\n",
    "\n",
    "**Load in the data used above, but now split it so that the demographic variables are in one data set and the survey question answers are in another.** \n",
    "\n",
    "These data sets are in a wide format, and they both contain `subject_id`s to identify the questions' categories. \n",
    "\n",
    "As you may recall, the demographic responses have fewer observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_demos_file = './datasets/NPAS_parsed_trunc_demo_sample.csv'\n",
    "n_survey_file = './datasets/NPAS_parsed_trunc_survey.csv'\n",
    "# load the files\n",
    "demos_subset = pd.read_csv(n_demos_file)\n",
    "survey = pd.read_csv(n_survey_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the header of the demos and survey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pandas_merge'></a>\n",
    "\n",
    "### Use  `pandas`' `.merge()` function: Joining Long Format vs. Wide Format Data\n",
    "\n",
    "---\n",
    "\n",
    "As we have seen yesterday, the `.merge()` function comes built into a DataFrame. The first argument is another DataFrame you want to merge it with, and the `on` keyword argument is the key(s) by which you want the DataFrames to be \"matched.\"\n",
    "\n",
    "We are specifying `how='inner'` here, which means that the key must be present in both DataFrames to have the corresponding rows included in the output. Because the demographics data set has fewer `subject_id`s, it will only merge the `subject_id` rows from the survey data set that are also present in the demographics data set.\n",
    "\n",
    "**Combine the survey and demographic wide format data sets using `.merge()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demos_survey = demos_subset.merge(survey, on=..?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the merged data header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the demographic and survey data into long format using `.melt()`.**\n",
    "\n",
    "- For the demographic DataFrame, specify two `id_vars` — `gender` and `subject_id`.\n",
    "- For the survey DataFrame, only specify `subject_id` for `id_vars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# melt the demographic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# melt the survey data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge the long form data sets together, just like we did previously with the wide format data.**\n",
    "\n",
    "Here, we will still merge on `subject_id`, using `'inner'` for the `how` variable. We have duplicate named columns in each of these DataFrames (`variable` and `value`). We can specify `suffixes=('_survey','_demo')` to give the instances of the survey and demographic DataFrames appropriate column names when they are joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the survey and demo data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
