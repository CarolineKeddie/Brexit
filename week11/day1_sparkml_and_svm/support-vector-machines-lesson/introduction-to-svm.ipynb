{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Support Vector Machines (SVM)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand how the SVM builds its decision threshold\n",
    "- Understand the concept of the maximum margin hyperplane\n",
    "- Visualize the linearly separable case in classification\n",
    "- Understand the math behind finding the maximum margin hyperplane\n",
    "- Understand the hinge loss for SVM\n",
    "- Understand how the regularization constant C allows SVMs to fit non-linearly separable problems\n",
    "- See how the kernel trick transforms problems from non-linearly separable to linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to SVMs](#intro)\n",
    "- [How does the SVM classifiy?](#how-classify)\n",
    "- [Intuition behind the decision boundary](#intuition)\n",
    "- [Why maximize the margin?](#why-max-margin)\n",
    "- [SVM origins: the perceptron algorithm](#perceptron)\n",
    "- [The maximum margin hyperplane](#mmh)\n",
    "    - [Finding the maximum margin](#finding-mmh)\n",
    "- [The hinge loss](#hinge)\n",
    "    - [\"Slack\"](#slack)\n",
    "    - [The regularizing hyperparameter C](#hyper-c)\n",
    "- [The kernel trick](#kernel)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to SVMs\n",
    "\n",
    "---\n",
    "\n",
    "The Support Vector Machine (SVM) algorithm is a different approach to classification.\n",
    "\n",
    "SVM still fits a decision boundary like a logistic regression, but uses a different loss function called the \"hinge loss\" (as opposed to the log loss in logistic regression).\n",
    "\n",
    "This lesson will overview the details of the SVM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='how-classify'></a>\n",
    "\n",
    "## How does the SVM classify?\n",
    "\n",
    "---\n",
    "\n",
    "It's important to start with the intuition for SVM with the special _**linearly separable**_ classification case.\n",
    "\n",
    "If classification of observations is \"linearly separable\", SVM fits the **\"decision boundary\"** that is defined by the largest margin between the closest points for each class. This is commonly called the **\"maximum margin hyperplane (MMH)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM](./assets/linear_separability_vs_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='intuition'></a>\n",
    "\n",
    "## Intuition behind the SVM decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "SVM's criterion for a decision surface is one that is _maximally far away from any data point between classes_. The distance from the decision boundary to the closest data point determines the \"margin\" of the classifier.\n",
    "\n",
    "The points SVM uses to fit the decision boundary are called \"support vectors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/Margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='why-max-margin'></a>\n",
    "\n",
    "## Why maximize the margin?\n",
    "\n",
    "---\n",
    "\n",
    "**SVM solves for a decision boundary that should theoretically minimize the generalization error.** \n",
    "\n",
    "Observations that are near the decision boundary between the classes are the observations with the most \"ambiguous\" labels. They are the observations that are approaching equal probability to be one class or the other (given the predictors).\n",
    "\n",
    "SVM, instead of considering all the observations \"equally\" in the loss function it minimizes, defines it's fit using the most ambiguous points. Its decision boundary is safe in that errors in new measured observations are not likely to cause the SVM to mis-classify.\n",
    "\n",
    "The SVM is concerned with generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='perceptron'></a>\n",
    "\n",
    "## SVM origins: the perceptron algorithm\n",
    "\n",
    "---\n",
    "\n",
    "The perceptron algorithm is a linear function of weights $w$ and predictors $X$ that assigns points to binary classes. If the function returns a value greater than 0 then the observation is classified as 1, otherwise it is classified as zero.\n",
    "\n",
    "$f(X)$ below is the perceptron function to determine the classes. $b$ here is known as the \"bias\", which is analagous to the intercept term.\n",
    "\n",
    "$$ f(X) = \\begin{cases}1 & \\text{if }w^T x + b > 0\\\\0 & \\text{otherwise}\\end{cases} $$\n",
    "\n",
    "If the points are linearly seperable, then solving the perceptron is guaranteed to converge on a solution, but that solution is not necessarily optimal for future observations. This led to the invention of the SVM, which finds the optimal discriminator: the maximum margin hyperplane.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mmh'></a>\n",
    "\n",
    "## The maximum margin hyperplane\n",
    "\n",
    "---\n",
    "\n",
    "We choose a normalizing constant such that the distance from the plane defined by\n",
    "\n",
    "$$ w^T x + b = 0$$\n",
    "\n",
    "to the closest points (known as \"support vectors\") of either classes will be 1:\n",
    "\n",
    "$$ w^T x_{+} + b = 1 \\\\ w^T x_{-} + b = -1 $$\n",
    "\n",
    "If the norm for the weights is $||w||$, the size of the margin is\n",
    "\n",
    "$$ \\text{margin} = \\frac{2}{||w||} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/linear_sep_support_vecs_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='finding-mmh'></a>\n",
    "### Finding the maximum margin\n",
    "\n",
    "We want to find a distance between these points that is the widest possible. Therefore, we are looking to maximize the value $\\frac{1}{||w||}$.\n",
    "\n",
    "So, maximize the weights with constraints on what the function can output. When the target is 1, the function needs to be 1 or greater. When the target is 0 (or -1), the function needs to output -1 or lower.\n",
    "\n",
    "$$ \\underset{w}{\\text{max}} \\frac{1}{||w||} \\quad \\text{subject to} \\begin{cases}\\text{if } y_i = 1 & w^T x_i + b \\ge 1 \\\\ \\text{if } y_i = -1 & w^T x_i + b \\le -1 \\end{cases} \\text{for } i \\text{ in } N$$\n",
    "\n",
    "Note here that $y$ is specified as either 1 or -1, as opposed to the 0, 1 that we are used to. This is convenient for formulating the optimization problem in a concise way. \n",
    "\n",
    "Additionally, we can change the optimization problem into a minimization problem by requiring\n",
    "\n",
    "$$ \\underset{w}{\\text{min }} \\frac{||w||^2}{2} \\quad \\text{subject to} \\quad y_i(w^T x_i + b) \\ge 1 $$\n",
    "\n",
    "which works out to be the same as above with $y_i=1$ or $y_i = -1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![linearly separable SVM MMH margin](./assets/linear_sep_support_vecs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hinge'></a>\n",
    "\n",
    "## The hinge loss and non-linearly separable cases\n",
    "\n",
    "---\n",
    "\n",
    "In cases where there is no line or plane that can separate all of the points perfectly, we need to introduce the capacity for model error. Using the constraint above that $y_i(w^T x_i + b) \\ge 1$, we can introduce the **hinge loss function**:\n",
    "\n",
    "$$ \n",
    "\\text{hinge loss} = \\sum_{i=1}^n \\max\\left(0, 1 - y_i(w^T x_i + b)\\right) $$\n",
    "\n",
    "where now, if the point is on the correct side (correctly classified), the value will be 0. If the point is not $\\ge 1$, this function will be greater than zero.\n",
    "\n",
    "Using $f(x_i) = (w^T x_i + b)$, \n",
    "\n",
    "$$\\text{hinge loss} = \\sum_{i=1}^N {\\rm max}\\left(0, 1 - y_i \\: f(x_i)\\right)$$ \n",
    "\n",
    "If $y_i\\, f(x_i) > 1$, the point lies _outside_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $y_i\\, f(x_i) = 1$ the point is _on_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $y_i\\, f(x_i) < 1$ the point lies _inside_ the margin and **does** contribute to the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Hinge loss](./assets/hinge_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='slack'></a>\n",
    "\n",
    "### Hinge loss and \"slack\"\n",
    "\n",
    "When we have a scenario where it is not possible to perfectly separate, we allow some of the points to be on the wrong side by an amount $\\xi_i\\ge 0$ (we do not measure how much points are on the correct side). \n",
    "We will allow only for a certain total budget of being on the wrong side:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N\\xi_i\\le {\\rm constant}\n",
    "$$\n",
    "\n",
    "We control this budget using a regularization constant $C$. Large values of $C$ strongly penalize being on the wrong side, low values of $C$ hardly penalize being on the wrong side. The $\\xi_i$ are called **slack variables**.\n",
    "\n",
    "The modified hinge loss with a regularization constant $C$ thus becomes\n",
    "\n",
    "$$ \\min \\frac{||w||^2}{2} + C\\sum_{i=1}^N \\xi_i \\quad \\text{subject to} \\quad y_i(w^T x_i + b) \\ge 1 - \\xi_i \\quad \\text{and} \\quad \\xi_i\\ge 0$$\n",
    "\n",
    "where the $\\xi_i$ are the errors of the classifier, and $C$ is a regularization constant that determines how much the classification errors matter (relative to the maximization of the margin).\n",
    "\n",
    "The function that the SVM minimizes to find the boundary is:\n",
    "\n",
    "$$  \\underset{w}{\\text{min }} \\frac{||w||^2}{2} + C\\sum_{i=1}^N {\\rm max}\\left(0, 1 - y_i(w^T x_i + b)\\right) $$\n",
    "\n",
    "A small $C$ creates a wider margin because errors will matter less. A large $C$ creates a tighter margin because errors matter more. An infinite $C$ parameter is a \"hard\" margin, which always minimizes error over the size of the boundary.\n",
    "\n",
    "We are trying to minimize the norm of the weights $||w||$ and thus maximize the margin, but now we are also trying to minimize our errors at the same time. We have a balance in our minimization between how wide the margin should be and how much error we tolerate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hyper-c'></a>\n",
    "### The regularizing hyper-parameter $C$\n",
    "\n",
    "By setting the hyper-parameter $C$ we can control the extent to which our SVM is tolerant to misclassification errors. It is sometimes called the \"soft-margin constant\". \n",
    "\n",
    "$C$ affects the strength of the \"penalty\", similar to the $C$ terms in the Ridge and Lasso. \n",
    "\n",
    "By multiplying the sum of the errors, which are the distances from the margins to the points inside the margin, it allows the SVM to classify non-linearly separable problems by allowing errors to occur. \n",
    "\n",
    "The lower the value of $C$, the more misclassified observation errors are allowed. These errors are known as \"slack variables\". Reducing the effect of errors on the loss function puts the emphasis on widening the margin.\n",
    "\n",
    "For those interested in exploring the math more, [there is a good tutorial here.](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/#more-457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./assets/slack_variables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![soft margin](./assets/soft_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='kernel'></a>\n",
    "## The \"kernel trick\" for non-linearly separable problems\n",
    "\n",
    "---\n",
    "\n",
    "The \"kernel trick\" allows an SVM to classify non-linearly separable problems. It is a big reason why SVMs are so popular.\n",
    "\n",
    "The idea behind the kernel trick is that you can arbitrarily transform your observations that _have no linear separability_ by putting them into a different \"dimensional space\" where they DO have linear separability, fit an SVM in that higher dimensional space, and then invert the transformation of the data and the model itself back into the original space.\n",
    "\n",
    "This is done by \"wrapping\" your predictors in a kernel function that transforms them into this higher dimensional space. \n",
    "\n",
    "[Check out these lecture slides for more detail.](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)\n",
    "\n",
    "The following pictures should give you a general intuition for what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kernel transform viz](./assets/kernel_trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![polynomial kernel](./assets/nonlinear-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![gaussian kernel](./assets/nonlinear-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel functions\n",
    "\n",
    "dth-degree polynomial:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\left(r+\\gamma\\langle x,x'\\rangle\\right)^d\n",
    "$$\n",
    "\n",
    "radial basis:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\exp\\left(-\\gamma \\|x-x'\\|^2\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "sigmoid:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\tanh\\left(\\gamma\\langle x,x'\\rangle + r\\right)\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "$$\n",
    "\\langle x,x'\\rangle = \\sum_{i=1}^p x_i x'_i\n",
    "$$\n",
    "\n",
    "stands for the scalar product,\n",
    "$d$, $\\gamma$, $r$ are tuning constants.\n",
    "\n",
    "Sklearn chooses the following names for these parameters in its SVM-classifier:\n",
    "\n",
    "- $d$ $\\rightarrow$ degree (default 3)\n",
    "- $\\gamma$ $\\rightarrow$ gamma (default 1)\n",
    "- $r$ $\\rightarrow$ coef0 (default 0)\n",
    "\n",
    "The parameters only have an effect if they appear in the kernel definition.\n",
    "\n",
    "A prediction for any point $x$ is made by calculating the kernel for this point paired up with all the support vectors:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x) = \\sum_{i=1}^N \\hat\\alpha_i y_i K(x,x_i) + \\hat{\\beta}_0\n",
    "$$\n",
    "\n",
    "The coefficients $\\hat{\\alpha}_i$ are the so-called dual coefficients. They are only different from zero if $x_i$ is a support vector, so the sum effectively runs only over the support vectors.\n",
    "\n",
    "SVM docs on [SKLearn](http://scikit-learn.org/stable/modules/svm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- [For a really great resource check out these slides (some of which are cannabalized in this lecture).](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "- [This website is also a great resource, on a slightly more technical level.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)\n",
    "- SVM docs on [SKLearn](http://scikit-learn.org/stable/modules/svm.html)\n",
    "- Iris example on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#example-svm-plot-iris-py)\n",
    "- Hyperplane walkthrough on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py)\n",
    "- A comprehensive [user guide](http://pyml.sourceforge.net/doc/howto.pdf) to SVM. My fav!\n",
    "- A [blog post tutorial](http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/) of understanding the linear algebra behind SVM hyperplanes. Check [part 3](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/) of this blog on finding the optimal hyperplane\n",
    "- This [Quora discussion](https://www.quora.com/How-do-you-teach-Support-Vector-Machine-to-a-beginner-in-Machine-Learning) includes a high-level overview plus a [20min video](https://www.youtube.com/watch?v=aDbsJ_S3tIA) walking through the core \"need-to-knows\"\n",
    "- A [slideshow introduction](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf) to the optimization considerations of SVM\n",
    "- A second [slideshow overview from UCF](http://www.cs.ucf.edu/courses/cap6412/fall2009/papers/Berwick2003.pdf) on the highnotes of SVM\n",
    "- Andrew Ng's [notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) on SVM from CS 229\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=eHsErlPJWUU) (1hr+) from one of my fav lecturers (Dr Yasser) on SVM. He does a followup on [kernel tricks](https://www.youtube.com/watch?v=XUj5JbQihlU) too\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=_PwhiWxHK8o) (50min) (from MIT Opencoursewar)\n",
    "- An infamous [paper](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf) (cited 7000+ times!) on why SVM is a great text classifier\n",
    "- An [advanced discussion](http://www.icml-2011.org/papers/386_icmlpaper.pdf) of SVMs as probabilistic models"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
