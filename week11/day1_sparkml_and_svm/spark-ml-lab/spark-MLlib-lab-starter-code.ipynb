{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Predicting Flight Delays with Spark ML \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using features that have already been prepared with PySpark to predict flight delays via regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#First-unzip-the-data\" data-toc-modified-id=\"First-unzip-the-data-1\">First unzip the data</a></span></li><li><span><a href=\"#Import-the-required-modules\" data-toc-modified-id=\"Import-the-required-modules-2\">Import the required modules</a></span></li><li><span><a href=\"#Load-and-Inspect-our-JSON-Training-Data-(should-take-around-a-min-with-Spark):\" data-toc-modified-id=\"Load-and-Inspect-our-JSON-Training-Data-(should-take-around-a-min-with-Spark):-3\">Load and Inspect our JSON Training Data (should take around a min with Spark):</a></span></li><li><span><a href=\"#Extract-the-head-of-the-dataframe\" data-toc-modified-id=\"Extract-the-head-of-the-dataframe-4\">Extract the head of the dataframe</a></span></li><li><span><a href=\"#Determine-the-number-of-rows-in-the-dataframe\" data-toc-modified-id=\"Determine-the-number-of-rows-in-the-dataframe-5\">Determine the number of rows in the dataframe</a></span></li><li><span><a href=\"#Check-for-null-values-in-the-features-before-using-Spark-ML\" data-toc-modified-id=\"Check-for-null-values-in-the-features-before-using-Spark-ML-6\">Check for null values in the features before using Spark ML</a></span></li><li><span><a href=\"#Add-a-Route-variable-to-replace-FlightNum\" data-toc-modified-id=\"Add-a-Route-variable-to-replace-FlightNum-7\">Add a Route variable to replace FlightNum</a></span></li><li><span><a href=\"#Categorize-or-&quot;bucketize&quot;-the-arrival-delay-field-using-a-DataFrame-UDF\" data-toc-modified-id=\"Categorize-or-&quot;bucketize&quot;-the-arrival-delay-field-using-a-DataFrame-UDF-8\">Categorize or \"bucketize\" the arrival delay field using a DataFrame UDF</a></span></li><li><span><a href=\"#Use-pyspark.ml.feature.Bucketizer-to-bucketize-ArrDelay\" data-toc-modified-id=\"Use-pyspark.ml.feature.Bucketizer-to-bucketize-ArrDelay-9\">Use <code>pyspark.ml.feature.Bucketizer</code> to bucketize <code>ArrDelay</code></a></span></li><li><span><a href=\"#Turn-categorical-fields-into-categorical-feature-vectors\" data-toc-modified-id=\"Turn-categorical-fields-into-categorical-feature-vectors-10\">Turn categorical fields into categorical feature vectors</a></span></li><li><span><a href=\"#Cross-validate,-train-and-evaluate-a-classifier-of-your-choice-(from-MLlib)\" data-toc-modified-id=\"Cross-validate,-train-and-evaluate-a-classifier-of-your-choice-(from-MLlib)-11\">Cross validate, train and evaluate a classifier of your choice (from MLlib)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  flight_delay_sample.jsonl.zip\n",
      "  inflating: flight_delay_sample.jsonl  \n"
     ]
    }
   ],
   "source": [
    "!unzip flight_delay_sample.jsonl.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warnings\n",
    "from pyspark.sql import SQLContext\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect our JSON Training Data (should take around a min with Spark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"ArrDelay\", DoubleType(), True),     # \"ArrDelay\":5.0\n",
    "  StructField(\"CRSArrTime\", TimestampType(), True),    # \"CRSArrTime\":\"2015-12-31T03:20:00.000-08:00\"\n",
    "  StructField(\"CRSDepTime\", TimestampType(), True),    # \"CRSDepTime\":\"2015-12-31T03:05:00.000-08:00\"\n",
    "  StructField(\"Carrier\", StringType(), True),     # \"Carrier\":\"WN\"\n",
    "  StructField(\"DayOfMonth\", IntegerType(), True), # \"DayOfMonth\":31\n",
    "  StructField(\"DayOfWeek\", IntegerType(), True),  # \"DayOfWeek\":4\n",
    "  StructField(\"DayOfYear\", IntegerType(), True),  # \"DayOfYear\":365\n",
    "  StructField(\"DepDelay\", DoubleType(), True),     # \"DepDelay\":14.0\n",
    "  StructField(\"Dest\", StringType(), True),        # \"Dest\":\"SAN\"\n",
    "  StructField(\"Distance\", DoubleType(), True),     # \"Distance\":368.0\n",
    "  StructField(\"FlightDate\", DateType(), True),    # \"FlightDate\":\"2015-12-30T16:00:00.000-08:00\"\n",
    "  StructField(\"FlightNum\", StringType(), True),   # \"FlightNum\":\"6109\"\n",
    "  StructField(\"Origin\", StringType(), True),      # \"Origin\":\"TUS\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = sqlContext.read.json(\n",
    "  \"flight_delay_sample.jsonl\",\n",
    "  schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the head of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the number of rows in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null values in the features before using Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Route variable to replace FlightNum\n",
    "\n",
    "Take the origin and the destination and create a column `route` as a concatenation of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize or \"bucketize\" the arrival delay field using a DataFrame UDF\n",
    "\n",
    "Create some label to characterize the delay from 0 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the function in pyspark.sql.functions.udf (Spark User Defined Functions) with...\n",
    "# pyspark.sql.types.StructField information\n",
    "\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketize_arr_delay(arr_delay):\n",
    "    bucket = None\n",
    "    if arr_delay <= -15.0:\n",
    "        bucket = 0.0\n",
    "    elif arr_delay > -15.0 and arr_delay <= 0.0:\n",
    "        bucket = 1.0\n",
    "    elif arr_delay > 0.0 and arr_delay <= 30.0:\n",
    "        bucket = 2.0\n",
    "    elif arr_delay > 30.0:\n",
    "        bucket = 3.0\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|ArrDelay|ArrDelayBucket|\n",
      "+--------+--------------+\n",
      "|    13.0|           2.0|\n",
      "|    36.0|           3.0|\n",
      "|   -21.0|           0.0|\n",
      "|   -14.0|           1.0|\n",
      "|    16.0|           2.0|\n",
      "+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_function_udf = udf(bucketize_arr_delay, StringType())\n",
    "\n",
    "# Add a categorical column via pyspark.sql.DataFrame.withColumn\n",
    "# Replace features with the dataframe containing the route column\n",
    "manual_bucketized_features = features.withColumn(\n",
    "  \"ArrDelayBucket\",\n",
    "  dummy_function_udf(features['ArrDelay'])\n",
    ")\n",
    "manual_bucketized_features.select(\"ArrDelay\", \"ArrDelayBucket\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `pyspark.ml.feature.Bucketizer` to bucketize `ArrDelay`\n",
    "\n",
    "Same as before but this time with `pyspark.ml.feature.Bucketizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn categorical fields into categorical feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn categorical fields into categorical feature vectors, then drop intermediate fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validate, train and evaluate a classifier of your choice (from MLlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "# training_data, test_data = \n",
    "\n",
    "# Instantiate and fit your model\n",
    "\n",
    "# Evaluate model using test data\n",
    "\n",
    "# Check a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the unzipped file once you do not need it any more\n",
    "!rm flight_delay_sample.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
