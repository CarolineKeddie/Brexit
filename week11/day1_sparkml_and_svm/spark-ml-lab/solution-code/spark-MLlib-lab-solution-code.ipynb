{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Predicting Flight Delays with Spark ML \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using features that have already been prepared with PySpark to predict flight delays via regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#First-unzip-the-data\" data-toc-modified-id=\"First-unzip-the-data-1\">First unzip the data</a></span></li><li><span><a href=\"#Import-the-required-modules\" data-toc-modified-id=\"Import-the-required-modules-2\">Import the required modules</a></span></li><li><span><a href=\"#Load-and-Inspect-our-JSON-Training-Data-(should-take-around-a-min-with-Spark):\" data-toc-modified-id=\"Load-and-Inspect-our-JSON-Training-Data-(should-take-around-a-min-with-Spark):-3\">Load and Inspect our JSON Training Data (should take around a min with Spark):</a></span></li><li><span><a href=\"#Extract-the-head-of-the-dataframe\" data-toc-modified-id=\"Extract-the-head-of-the-dataframe-4\">Extract the head of the dataframe</a></span></li><li><span><a href=\"#Determine-the-number-of-rows-in-the-dataframe\" data-toc-modified-id=\"Determine-the-number-of-rows-in-the-dataframe-5\">Determine the number of rows in the dataframe</a></span></li><li><span><a href=\"#Check-for-null-values-in-the-features-before-using-Spark-ML\" data-toc-modified-id=\"Check-for-null-values-in-the-features-before-using-Spark-ML-6\">Check for null values in the features before using Spark ML</a></span></li><li><span><a href=\"#Add-a-Route-variable-to-replace-FlightNum\" data-toc-modified-id=\"Add-a-Route-variable-to-replace-FlightNum-7\">Add a Route variable to replace FlightNum</a></span></li><li><span><a href=\"#Categorize-or-&quot;bucketize&quot;-the-arrival-delay-field-using-a-DataFrame-UDF\" data-toc-modified-id=\"Categorize-or-&quot;bucketize&quot;-the-arrival-delay-field-using-a-DataFrame-UDF-8\">Categorize or \"bucketize\" the arrival delay field using a DataFrame UDF</a></span></li><li><span><a href=\"#Use-pyspark.ml.feature.Bucketizer-to-bucketize-ArrDelay\" data-toc-modified-id=\"Use-pyspark.ml.feature.Bucketizer-to-bucketize-ArrDelay-9\">Use <code>pyspark.ml.feature.Bucketizer</code> to bucketize <code>ArrDelay</code></a></span></li><li><span><a href=\"#Turn-categorical-fields-into-categorical-feature-vectors\" data-toc-modified-id=\"Turn-categorical-fields-into-categorical-feature-vectors-10\">Turn categorical fields into categorical feature vectors</a></span></li><li><span><a href=\"#Cross-validate,-train-and-evaluate-a-classifier-of-your-choice-(from-MLlib)\" data-toc-modified-id=\"Cross-validate,-train-and-evaluate-a-classifier-of-your-choice-(from-MLlib)-11\">Cross validate, train and evaluate a classifier of your choice (from MLlib)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../flight_delay_sample.jsonl.zip\n",
      "  inflating: flight_delay_sample.jsonl  \n"
     ]
    }
   ],
   "source": [
    "!unzip ../flight_delay_sample.jsonl.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warnings\n",
    "from pyspark.sql import SQLContext\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect our JSON Training Data (should take around a min with Spark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"ArrDelay\", DoubleType(), True),     # \"ArrDelay\":5.0\n",
    "  StructField(\"CRSArrTime\", TimestampType(), True),    # \"CRSArrTime\":\"2015-12-31T03:20:00.000-08:00\"\n",
    "  StructField(\"CRSDepTime\", TimestampType(), True),    # \"CRSDepTime\":\"2015-12-31T03:05:00.000-08:00\"\n",
    "  StructField(\"Carrier\", StringType(), True),     # \"Carrier\":\"WN\"\n",
    "  StructField(\"DayOfMonth\", IntegerType(), True), # \"DayOfMonth\":31\n",
    "  StructField(\"DayOfWeek\", IntegerType(), True),  # \"DayOfWeek\":4\n",
    "  StructField(\"DayOfYear\", IntegerType(), True),  # \"DayOfYear\":365\n",
    "  StructField(\"DepDelay\", DoubleType(), True),     # \"DepDelay\":14.0\n",
    "  StructField(\"Dest\", StringType(), True),        # \"Dest\":\"SAN\"\n",
    "  StructField(\"Distance\", DoubleType(), True),     # \"Distance\":368.0\n",
    "  StructField(\"FlightDate\", DateType(), True),    # \"FlightDate\":\"2015-12-30T16:00:00.000-08:00\"\n",
    "  StructField(\"FlightNum\", StringType(), True),   # \"FlightNum\":\"6109\"\n",
    "  StructField(\"Origin\", StringType(), True),      # \"Origin\":\"TUS\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = sqlContext.read.json(\n",
    "  \"flight_delay_sample.jsonl\",\n",
    "  schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the head of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ArrDelay=13.0, CRSArrTime=datetime.datetime(2015, 1, 1, 18, 10), CRSDepTime=datetime.datetime(2015, 1, 1, 15, 30), Carrier='AA', DayOfMonth=1, DayOfWeek=4, DayOfYear=1, DepDelay=14.0, Dest='DFW', Distance=569.0, FlightDate=datetime.date(2014, 12, 31), FlightNum='1024', Origin='ABQ'),\n",
       " Row(ArrDelay=36.0, CRSArrTime=datetime.datetime(2015, 1, 1, 11, 45), CRSDepTime=datetime.datetime(2015, 1, 1, 9, 0), Carrier='AA', DayOfMonth=1, DayOfWeek=4, DayOfYear=1, DepDelay=-2.0, Dest='DFW', Distance=569.0, FlightDate=datetime.date(2014, 12, 31), FlightNum='336', Origin='ABQ'),\n",
       " Row(ArrDelay=-21.0, CRSArrTime=datetime.datetime(2015, 1, 1, 19, 30), CRSDepTime=datetime.datetime(2015, 1, 1, 17, 55), Carrier='AA', DayOfMonth=1, DayOfWeek=4, DayOfYear=1, DepDelay=-1.0, Dest='DFW', Distance=731.0, FlightDate=datetime.date(2014, 12, 31), FlightNum='125', Origin='ATL'),\n",
       " Row(ArrDelay=-14.0, CRSArrTime=datetime.datetime(2015, 1, 1, 10, 25), CRSDepTime=datetime.datetime(2015, 1, 1, 8, 55), Carrier='AA', DayOfMonth=1, DayOfWeek=4, DayOfYear=1, DepDelay=-4.0, Dest='DFW', Distance=731.0, FlightDate=datetime.date(2014, 12, 31), FlightNum='1455', Origin='ATL'),\n",
       " Row(ArrDelay=16.0, CRSArrTime=datetime.datetime(2015, 1, 1, 15, 15), CRSDepTime=datetime.datetime(2015, 1, 1, 13, 45), Carrier='AA', DayOfMonth=1, DayOfWeek=4, DayOfYear=1, DepDelay=15.0, Dest='DFW', Distance=731.0, FlightDate=datetime.date(2014, 12, 31), FlightNum='1473', Origin='ATL')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "features.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the number of rows in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250761"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null values in the features before using Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrDelay :\t 0\n",
      "CRSArrTime :\t nan\n",
      "CRSDepTime :\t nan\n",
      "Carrier :\t 0\n",
      "DayOfMonth :\t 0\n",
      "DayOfWeek :\t 0\n",
      "DayOfYear :\t 0\n",
      "DepDelay :\t 0\n",
      "Dest :\t 0\n",
      "Distance :\t 0\n",
      "FlightDate :\t nan\n",
      "FlightNum :\t 0\n",
      "Origin :\t 0\n"
     ]
    }
   ],
   "source": [
    "for col in features.columns:\n",
    "    try:\n",
    "        print(col,':\\t', features.na.df.where(isnan(col)).count())\n",
    "    except:\n",
    "        print(col,':\\t', 'nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Route variable to replace FlightNum\n",
    "\n",
    "Take the origin and the destination and create a column `route` as a concatenation of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|Origin|Dest|  Route|\n",
      "+------+----+-------+\n",
      "|   ABQ| DFW|ABQ-DFW|\n",
      "|   ABQ| DFW|ABQ-DFW|\n",
      "|   ATL| DFW|ATL-DFW|\n",
      "|   ATL| DFW|ATL-DFW|\n",
      "|   ATL| DFW|ATL-DFW|\n",
      "+------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_with_route = features.withColumn(\n",
    "  'Route',\n",
    "  concat(\n",
    "    features.Origin,\n",
    "    lit('-'),\n",
    "    features.Dest\n",
    "  )\n",
    ")\n",
    "features_with_route.select(\"Origin\", \"Dest\", \"Route\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize or \"bucketize\" the arrival delay field using a DataFrame UDF\n",
    "\n",
    "Create some label to characterize the delay from 0 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the function in pyspark.sql.functions.udf (Spark User Defined Functions) with...\n",
    "# pyspark.sql.types.StructField information\n",
    "\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketize_arr_delay(arr_delay):\n",
    "    bucket = None\n",
    "    if arr_delay <= -15.0:\n",
    "        bucket = 0.0\n",
    "    elif arr_delay > -15.0 and arr_delay <= 0.0:\n",
    "        bucket = 1.0\n",
    "    elif arr_delay > 0.0 and arr_delay <= 30.0:\n",
    "        bucket = 2.0\n",
    "    elif arr_delay > 30.0:\n",
    "        bucket = 3.0\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|ArrDelay|ArrDelayBucket|\n",
      "+--------+--------------+\n",
      "|    13.0|           2.0|\n",
      "|    36.0|           3.0|\n",
      "|   -21.0|           0.0|\n",
      "|   -14.0|           1.0|\n",
      "|    16.0|           2.0|\n",
      "|    13.0|           2.0|\n",
      "|    25.0|           2.0|\n",
      "|    14.0|           2.0|\n",
      "|   -29.0|           0.0|\n",
      "|    -3.0|           1.0|\n",
      "|    -8.0|           1.0|\n",
      "|    -1.0|           1.0|\n",
      "|    18.0|           2.0|\n",
      "|   -15.0|           0.0|\n",
      "|   -11.0|           1.0|\n",
      "|   -16.0|           0.0|\n",
      "|    16.0|           2.0|\n",
      "|    42.0|           3.0|\n",
      "|    25.0|           2.0|\n",
      "|    -4.0|           1.0|\n",
      "+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_function_udf = udf(bucketize_arr_delay, StringType())\n",
    "\n",
    "# Add a categorical column via pyspark.sql.DataFrame.withColumn\n",
    "manual_bucketized_features = features_with_route.withColumn(\n",
    "  \"ArrDelayBucket\",\n",
    "  dummy_function_udf(features['ArrDelay'])\n",
    ")\n",
    "manual_bucketized_features.select(\"ArrDelay\", \"ArrDelayBucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `pyspark.ml.feature.Bucketizer` to bucketize `ArrDelay`\n",
    "\n",
    "Same as before but this time with `pyspark.ml.feature.Bucketizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|ArrDelay|ArrDelayBucket|\n",
      "+--------+--------------+\n",
      "|    13.0|           2.0|\n",
      "|    36.0|           3.0|\n",
      "|   -21.0|           0.0|\n",
      "|   -14.0|           1.0|\n",
      "|    16.0|           2.0|\n",
      "|    13.0|           2.0|\n",
      "|    25.0|           2.0|\n",
      "|    14.0|           2.0|\n",
      "|   -29.0|           0.0|\n",
      "|    -3.0|           1.0|\n",
      "|    -8.0|           1.0|\n",
      "|    -1.0|           1.0|\n",
      "|    18.0|           2.0|\n",
      "|   -15.0|           1.0|\n",
      "|   -11.0|           1.0|\n",
      "|   -16.0|           0.0|\n",
      "|    16.0|           2.0|\n",
      "|    42.0|           3.0|\n",
      "|    25.0|           2.0|\n",
      "|    -4.0|           1.0|\n",
      "+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = [-float(\"inf\"), -15.0, 0, 30.0, float(\"inf\")]\n",
    "\n",
    "bucketizer = Bucketizer(\n",
    "  splits=splits,\n",
    "  inputCol=\"ArrDelay\",\n",
    "  outputCol=\"ArrDelayBucket\"\n",
    ")\n",
    "ml_bucketized_features = bucketizer.transform(features_with_route)\n",
    "\n",
    "# Check the bucket's output\n",
    "ml_bucketized_features.select(\"ArrDelay\", \"ArrDelayBucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn categorical fields into categorical feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+-------------+----------------+---------------+---------------+------------+----------+-----------+\n",
      "|ArrDelay|          CRSArrTime|          CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|FlightDate|FlightNum|Origin|  Route|ArrDelayBucket|Carrier_index|DayOfMonth_index|DayOfWeek_index|DayOfYear_index|Origin_index|Dest_index|Route_index|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+-------------+----------------+---------------+---------------+------------+----------+-----------+\n",
      "|    13.0|2015-01-01 18:10:...|2015-01-01 15:30:...|     AA|         1|        4|        1|    14.0| DFW|   569.0|2014-12-31|     1024|   ABQ|ABQ-DFW|           2.0|          4.0|             2.0|            1.0|           26.0|        57.0|       1.0|     1322.0|\n",
      "|    36.0|2015-01-01 11:45:...|2015-01-01 09:00:...|     AA|         1|        4|        1|    -2.0| DFW|   569.0|2014-12-31|      336|   ABQ|ABQ-DFW|           3.0|          4.0|             2.0|            1.0|           26.0|        57.0|       1.0|     1322.0|\n",
      "|   -21.0|2015-01-01 19:30:...|2015-01-01 17:55:...|     AA|         1|        4|        1|    -1.0| DFW|   731.0|2014-12-31|      125|   ATL|ATL-DFW|           0.0|          4.0|             2.0|            1.0|           26.0|         0.0|       1.0|       17.0|\n",
      "|   -14.0|2015-01-01 10:25:...|2015-01-01 08:55:...|     AA|         1|        4|        1|    -4.0| DFW|   731.0|2014-12-31|     1455|   ATL|ATL-DFW|           1.0|          4.0|             2.0|            1.0|           26.0|         0.0|       1.0|       17.0|\n",
      "|    16.0|2015-01-01 15:15:...|2015-01-01 13:45:...|     AA|         1|        4|        1|    15.0| DFW|   731.0|2014-12-31|     1473|   ATL|ATL-DFW|           2.0|          4.0|             2.0|            1.0|           26.0|         0.0|       1.0|       17.0|\n",
      "|    13.0|2015-01-01 16:50:...|2015-01-01 15:25:...|     AA|         1|        4|        1|     9.0| DFW|   731.0|2014-12-31|      194|   ATL|ATL-DFW|           2.0|          4.0|             2.0|            1.0|           26.0|         0.0|       1.0|       17.0|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+-------------+----------------+---------------+---------------+------------+----------+-----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn categorical fields into categorical feature vectors, then drop intermediate fields\n",
    "for column in [\"Carrier\", \"DayOfMonth\", \"DayOfWeek\", \"DayOfYear\",\n",
    "               \"Origin\", \"Dest\", \"Route\"]:\n",
    "    string_indexer = StringIndexer(\n",
    "        inputCol=column,\n",
    "        outputCol=column + \"_index\"\n",
    "    )\n",
    "    ml_bucketized_features = string_indexer.fit(ml_bucketized_features)\\\n",
    "        .transform(ml_bucketized_features)\n",
    "\n",
    "# Check out the indices\n",
    "ml_bucketized_features.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle continuous, numeric fields by combining them into one feature vector\n",
    "numeric_columns = [\"DepDelay\", \"Distance\"]\n",
    "index_columns = [\"Carrier_index\", \"DayOfMonth_index\",\n",
    "                 \"DayOfWeek_index\", \"DayOfYear_index\", \"Origin_index\",\n",
    "                 \"Origin_index\", \"Dest_index\", \"Route_index\"]\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=numeric_columns + index_columns,\n",
    "    outputCol=\"Features_vec\"\n",
    ")\n",
    "final_vectorized_features = vector_assembler.transform(ml_bucketized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+\n",
      "|ArrDelay|          CRSArrTime|          CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|FlightDate|FlightNum|Origin|  Route|ArrDelayBucket|        Features_vec|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+\n",
      "|    13.0|2015-01-01 18:10:...|2015-01-01 15:30:...|     AA|         1|        4|        1|    14.0| DFW|   569.0|2014-12-31|     1024|   ABQ|ABQ-DFW|           2.0|[14.0,569.0,4.0,2...|\n",
      "|    36.0|2015-01-01 11:45:...|2015-01-01 09:00:...|     AA|         1|        4|        1|    -2.0| DFW|   569.0|2014-12-31|      336|   ABQ|ABQ-DFW|           3.0|[-2.0,569.0,4.0,2...|\n",
      "|   -21.0|2015-01-01 19:30:...|2015-01-01 17:55:...|     AA|         1|        4|        1|    -1.0| DFW|   731.0|2014-12-31|      125|   ATL|ATL-DFW|           0.0|[-1.0,731.0,4.0,2...|\n",
      "|   -14.0|2015-01-01 10:25:...|2015-01-01 08:55:...|     AA|         1|        4|        1|    -4.0| DFW|   731.0|2014-12-31|     1455|   ATL|ATL-DFW|           1.0|[-4.0,731.0,4.0,2...|\n",
      "|    16.0|2015-01-01 15:15:...|2015-01-01 13:45:...|     AA|         1|        4|        1|    15.0| DFW|   731.0|2014-12-31|     1473|   ATL|ATL-DFW|           2.0|[15.0,731.0,4.0,2...|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the index columns\n",
    "for column in index_columns:\n",
    "    final_vectorized_features = final_vectorized_features.drop(column)\n",
    "\n",
    "# Check out the features\n",
    "final_vectorized_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validate, train and evaluate a classifier of your choice (from MLlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "training_data, test_data = final_vectorized_features.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Instantiate and fit a random forest classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "  featuresCol=\"Features_vec\",\n",
    "  labelCol=\"ArrDelayBucket\",\n",
    "  maxBins=4657,\n",
    "  maxMemoryInMB=1024\n",
    ")\n",
    "model = rfc.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5914904310333426\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|ArrDelay|          CRSArrTime|          CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|FlightDate|FlightNum|Origin|  Route|ArrDelayBucket|        Features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|     5.0|2015-01-01 17:15:...|2015-01-01 15:05:...|     WN|         1|        4|        1|     6.0| TPA|   837.0|2014-12-31|      474|   IND|IND-TPA|           2.0|[6.0,837.0,0.0,2....|[2.22270705952843...|[0.11113535297642...|       2.0|\n",
      "|    36.0|2015-01-02 07:20:...|2015-01-02 06:00:...|     MQ|         2|        5|        2|    13.0| DFW|   327.0|2015-01-01|     3079|   JLN|JLN-DFW|           3.0|[13.0,327.0,7.0,1...|[1.31597373016805...|[0.06579868650840...|       2.0|\n",
      "|    -1.0|2015-01-03 00:03:...|2015-01-02 20:30:...|     US|         2|        5|        2|    -5.0| PHX|  1009.0|2015-01-01|      577|   PDX|PDX-PHX|           1.0|[-5.0,1009.0,6.0,...|[3.87838782234685...|[0.19391939111734...|       1.0|\n",
      "|    22.0|2015-01-03 07:08:...|2015-01-02 23:55:...|     DL|         2|        5|        2|     7.0| ATL|  2092.0|2015-01-01|     1862|   SMF|SMF-ATL|           2.0|[7.0,2092.0,1.0,1...|[1.83388230927776...|[0.09169411546388...|       2.0|\n",
      "|   -17.0|2015-01-03 09:46:...|2015-01-03 07:25:...|     AS|         3|        6|        3|    -7.0| LAS|   867.0|2015-01-02|      612|   SEA|SEA-LAS|           0.0|[-7.0,867.0,9.0,0...|[4.11818192148086...|[0.20590909607404...|       1.0|\n",
      "|    37.0|2015-01-03 19:25:...|2015-01-03 18:25:...|     EV|         3|        6|        3|    32.0| DFW|   190.0|2015-01-02|     2547|   SHV|SHV-DFW|           3.0|[32.0,190.0,2.0,0...|[0.76366686131961...|[0.03818334306598...|       3.0|\n",
      "+--------+--------------------+--------------------+-------+----------+---------+---------+--------+----+--------+----------+---------+------+-------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "# Check a sample\n",
    "predictions.sample(False, 0.001, 18).orderBy(\"CRSDepTime\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10d9f88d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unzipped file once you do not need it any more\n",
    "!rm flight_delay_sample.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
