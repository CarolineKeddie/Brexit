{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Multithreaded scraping \n",
    "\n",
    "This is an example of setting up a web scrape of multiple product reviews from Amazon. At this point we have already scraped a bunch of products from Amazon, and now we want to scrape the reviews left for each of the products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I know from the previous scrape how many reviews a product has, and that each product review page has 10 reviews, I can utilise the power of parallel requests to scrape reveiews in parallel rather than in a sequential manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules for multi threading and scraping\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "import time\n",
    "import requests\n",
    "from lxml import html  \n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for picking out the salient details from a review block\n",
    "\n",
    "\n",
    "def get_asin(review):\n",
    "    xpath_asin = \".//a[@data-hook='review-title']/@href\"\n",
    "    return review.xpath(xpath_asin)[0][-10:]\n",
    "\n",
    "def get_review_id(review):\n",
    "    return review.xpath(\"@id\")[0]\n",
    "\n",
    "\n",
    "def get_stars(review):\n",
    "    xpath_stars = \".//i[@data-hook='review-star-rating']//text()\"\n",
    "    return review.xpath(xpath_stars)[0][0]\n",
    "\n",
    "\n",
    "def get_title(review):\n",
    "    xpath_title = \".//a[@data-hook='review-title']//text()\"\n",
    "    return review.xpath(xpath_title)[0]\n",
    "\n",
    "\n",
    "def get_comment(review):\n",
    "    xpath_comment = \".//span[@data-hook='review-body']//text()\"\n",
    "    if review.xpath(xpath_comment) != []:\n",
    "        return review.xpath(xpath_comment)[0]\n",
    "    else: \n",
    "        return \"QQQQQQQQQ\" \n",
    "\n",
    "\n",
    "def get_author(review):\n",
    "    xpath_author = \".//a[@data-hook='review-author']/@href\"\n",
    "    if review.xpath(xpath_author) != [] and len(review.xpath(xpath_author)[0]) > 26:\n",
    "        return review.xpath(xpath_author)[0][26:]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_date(review):\n",
    "    xpath_date = \".//span[@data-hook='review-date']//text()\"\n",
    "    return review.xpath(xpath_date)[0][3:]\n",
    "\n",
    "\n",
    "def get_verified(review):\n",
    "    xpath_verified = \".//span[@data-hook='avp-badge']//text()\"\n",
    "    if review.xpath(xpath_verified) != []:\n",
    "        return review.xpath(xpath_verified)[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_helpful_count(review):\n",
    "    xpath_helpful = \".//span[@data-hook='helpful-vote-statement']//text()\"\n",
    "    if review.xpath(xpath_helpful) != []:\n",
    "        score = review.xpath(xpath_helpful)[0].split()[0]\n",
    "        if score == \"One\":\n",
    "            return 1\n",
    "        else:\n",
    "            return score\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_image_count(review):\n",
    "    xpath_image = \".//img[@data-hook='review-image-tile']\"\n",
    "    if review.xpath(xpath_image) != []:\n",
    "        return len(review.xpath(xpath_image))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_author_status(review):\n",
    "    xpath_status = \".//span[@data-hook='review-author']/following-sibling::span[@class='a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-bold']/text()\"\n",
    "    if review.xpath(xpath_status) != []:\n",
    "        return review.xpath(xpath_status)[0]\n",
    "    else:\n",
    "        return \"none\"\n",
    "    \n",
    "def get_video_block(review):\n",
    "    xpath_video = \"div/div/span/div[starts-with(@id,'video-block')]\"\n",
    "    if review.xpath(xpath_video) != []:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will define the function to go through each review on a review page, and extract the features to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_2(page):\n",
    "    \"\"\"\n",
    "    Will Return a dictionary of all review details from a given Amazon review page\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    review_dict = {\n",
    "    'asin': [],\n",
    "    'page': [],\n",
    "    'stars' : [],\n",
    "    'author': [],\n",
    "    'date': [],\n",
    "    'title':[],\n",
    "    'comment': [],\n",
    "    'verified': [],\n",
    "    'helpful': [],\n",
    "    'pics': [],\n",
    "    'video': [],\n",
    "    'comment_id': [],\n",
    "    'author_status':[]\n",
    "    }\n",
    "    \n",
    "    #set up the request - use a fake user agent\n",
    "    headers = {'User-Agent': ua.safari}\n",
    "    r = requests.get(page, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print('status error',r.status_code,page)\n",
    "\n",
    "    #get test response from request\n",
    "    reviews_page = r.text\n",
    "\n",
    "    #parse the page\n",
    "    parser = html.fromstring(reviews_page)\n",
    "\n",
    "    # get the individual products\n",
    "    xpath_review = \"//div[@data-hook='review']\"\n",
    "    reviews = parser.xpath(xpath_review)\n",
    "\n",
    "    for review in reviews:\n",
    "        #add returned values to the list within the dictionary\n",
    "        review_dict['asin'].append(get_asin(review))\n",
    "        review_dict['page'].append(page)\n",
    "        review_dict['stars'].append(get_stars(review))\n",
    "        review_dict['title'].append(get_title(review))\n",
    "        review_dict['comment'].append(get_comment(review))\n",
    "        review_dict['author'].append(get_author(review))\n",
    "        review_dict['date'].append(get_date(review))\n",
    "        review_dict['comment_id'].append(get_review_id(review))\n",
    "        review_dict['verified'].append(get_verified(review))\n",
    "        review_dict['helpful'].append(get_helpful_count(review))\n",
    "        review_dict['author_status'].append(get_author_status(review))\n",
    "        review_dict['pics'].append(get_image_count(review))\n",
    "        review_dict['video'].append(get_video_block(review))\n",
    "    \n",
    "#    print(review_dict) ## Debugging\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the multi threading, I will loop through my products dataframe, and for each product, generate a request for the appropriate number of review page scrapes based on the 10 reviews per page fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def async_get(urls):\n",
    "    \"\"\"\n",
    "    will return a list of dictionaries containing the relevant review information\n",
    "    from each url passed to it\n",
    "    \"\"\"\n",
    "    ls_=[]                                          # set up a list to store the results\n",
    "    pool = ThreadPool(18)                           # Create a Threadpool with 18 threads maximum\n",
    "    results = pool.map_async(get_reviews_2, urls)   # map the get_reviews_2 function asynchronously to all urls\n",
    "    results.wait()                                  # wait for the results to come in\n",
    "    ls_.append(results.get())                       # add the returned dictionary from get_reviews_2 to the ls_ list\n",
    "    pool.close()                                    # close the pool once all threads have finished\n",
    "    pool.join()                                     # close open threads\n",
    "    return ls_                                      # return the list of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Let's get some review data for one playmobil product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to generate some product review pages\n",
    "\n",
    "url = \"https://www.amazon.co.uk/Playmobil-5568-City-ChildrenÂ´s-Playground/product-reviews/B00IF1VVFO/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=\"\n",
    "urls = []\n",
    "for i in range(1,18):\n",
    "    urls.append(url + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3902678489685059\n"
     ]
    }
   ],
   "source": [
    "# Example Run of the above process\n",
    "start = time.time()\n",
    "\n",
    "playmobil_reviews = async_get(urls)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# playmobil_reviews ## uncomment this line to view the list of dictionaries returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

