{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Visualizing CARTs with admissions data\n",
    "\n",
    "_Instructor: Aymeric Flaisler_\n",
    "\n",
    "---\n",
    "\n",
    "Using the admissions data from earlier in the course, build CARTs, look at how they work visually, and compare their performance to more standard, parametric models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Install and load the packages required to visually show decision tree branching\n",
    "\n",
    "You will need to first:\n",
    "\n",
    "1. Install `graphviz` with homebrew. The command will be `brew install graphviz`\n",
    "- Install `pydotplus` with `conda install pydotplus`\n",
    "- Load the packages as shown below (you may need to restart the kernel after the installations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIREMENTS:\n",
    "# pip install pydotplus\n",
    "# brew install graphviz\n",
    "\n",
    "# Use graphviz to make a chart of the regression tree decision points:\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Load in admissions data and other python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit = pd.read_csv('./datasets/admissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Create regression and classification X, y data\n",
    "\n",
    "The regression data will be:\n",
    "\n",
    "    Xr = [admit, gre, prestige]\n",
    "    yr = gpa\n",
    "    \n",
    "The classification data will be:\n",
    "\n",
    "    Xc = [gre, gpa, prestige]\n",
    "    yc = admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We focus on data we have for the time being.\n",
    "# dont want to spend an unessary amount of time cleaning.\n",
    "admit = admit.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "admit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr = admit[['admit','gre','prestige']]\n",
    "yr = admit.gpa.values\n",
    "\n",
    "Xc = admit[['gpa','gre','prestige']]\n",
    "yc = admit.admit.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Cross-validate regression and logistic regression on the data\n",
    "\n",
    "Fit a linear regression for the regression problem and a logistic for the classification problem. Cross-validate the R2 and accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross val Linear Reg with 4 folds\n",
    "# reg_scores = cross_val_score(...\n",
    "\n",
    "# cross val Logistic Reg with 4 folds\n",
    "# cls_scores = cross_val_score(...\n",
    "\n",
    "#get scores\n",
    "print(reg_scores, np.mean(reg_scores))\n",
    "print(cls_scores, np.mean(cls_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models\n",
    "linreg = LinearRegression().fit(Xr, yr) #R2\n",
    "logreg = LogisticRegression().fit(Xc, yc) #accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Building regression trees\n",
    "\n",
    "With `DecisionTreeRegressor`:\n",
    "\n",
    "1. Build 4 models with different parameters for `max_depth`: `max_depth=1`, `max_depth=2`, `max_depth=3`, and `max_depth=None`\n",
    "2. Cross-validate the R2 scores of each of the models and compare to the linear regression earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 4 models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the 4 models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validate the 4 models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the 4 models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Visualizing the regression tree decisions\n",
    "\n",
    "Use the template code below to create charts that show the logic/branching of your four decision tree regressions from above.\n",
    "\n",
    "#### Interpreting a regression tree diagram\n",
    "\n",
    "- First line is the condition used to split that node (go left if true, go right if false)\n",
    "- `samples` is the number of observations in that node before splitting\n",
    "- `mse` is the mean squared error calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "- `value` is the mean response value in that node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMPLATE CODE\n",
    "# from sklearn.externals.six import StringIO  \n",
    "# from IPython.display import Image  \n",
    "# from sklearn.tree import export_graphviz\n",
    "# import pydotplus\n",
    "\n",
    "# # initialize the output file object\n",
    "# dot_data = StringIO() \n",
    "\n",
    "# # my fit DecisionTreeRegressor object here is: dtr1\n",
    "# # for feature_names i put the columns of my Xr matrix\n",
    "# export_graphviz(dtr1, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True,\n",
    "#                 feature_names=Xr.columns)  \n",
    "\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png()) \n",
    "# To save the image of the tree\n",
    "# graph.write_png('./dtr1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Building classification trees\n",
    "\n",
    "With `DecisionTreeClassifier`:\n",
    "\n",
    "1. Again build 4 models with different parameters for `max_depth`: `max_depth=1`, `max_depth=2`, `max_depth=3`, and `max_depth=None`\n",
    "2. Cross-validate the accuracy scores of each of the models and compare to the logistic regression earlier.\n",
    "\n",
    "Note that now you'll be using the classification task where we are predicting `admit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 4 models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the 4 models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validate the 4 models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the 4 models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8. Visualize the classification trees\n",
    "\n",
    "The plotting code will be the same as for regression, you just need to change the model you're using for each plot and the feature names.\n",
    "\n",
    "The output changes somewhat from the regression tree chart. Earlier it would give the MSE of that node, but now there is a line called `value` that tells you the count of each class at that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE CODE for max_depth = 1\n",
    "# dot_data = StringIO()  \n",
    "\n",
    "# export_graphviz(dtc1, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True,\n",
    "#                 feature_names=Xc.columns)  \n",
    "\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9. Using GridSearchCV to find the best decision tree classifier\n",
    "\n",
    "As decision trees that are unrestricted will just end up over fitting the training data. Decision tree regression and classification models in sklearn offer a variety of ways to \"pre-prune\" (by restricting the how many times the tree can branch and what it can use).\n",
    "\n",
    "Measure           | What it does\n",
    "------------------|-------------\n",
    "max_depth         | How many nodes deep can the decision tree go?\n",
    "max_features      | Is there a cut off to the number of features to use?\n",
    "max_leaf_nodes    | How many leaves can be generated per node?\n",
    "min_samples_leaf  | How many samples need to be included at a leaf, at a minimum?  \n",
    "min_samples_split | How many samples need to be included at a node, at a minimum?\n",
    "\n",
    "It is not always best to search over _all_ of these in a grid search, unless you have a small dataset. Many of them while not redundant are going to have very similar effects on your model's fit.\n",
    "\n",
    "Check out the documentation here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "---\n",
    "\n",
    "#### Switch over to the college stats dataset\n",
    "\n",
    "We are going to be predicting whether or not a college is public or private. Set up your X, y variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pd.read_csv('./datasets/College.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your X, y variables accordingly\n",
    "y = \n",
    "X ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 10. Building classification trees\n",
    "\n",
    "With `DecisionTreeClassifier`:\n",
    "\n",
    "1. Build 4 models with different parameters for `max_depth`: `max_depth=1`, `max_depth=2`, `max_depth=3`, and `max_depth=None`\n",
    "2. Cross-validate the accuracy scores of each of the models and compare to the logistic regression earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 4 trees\n",
    "# dtc1 = DecisionTreeClassifier(max_depth=1)\n",
    "# dtc2 = DecisionTreeClassifier(max_depth=2)\n",
    "# dtc3 = DecisionTreeClassifier(max_depth=3)\n",
    "# dtcN = DecisionTreeClassifier(max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit 4 trees\n",
    "# dtc1.fit(X, y)\n",
    "# dtc2.fit(X, y)\n",
    "# dtc3.fit(X, y)\n",
    "# dtcN.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CV to evaluate the 4 trees\n",
    "# dtc1_scores = cross_val_score(dtc1, X, y, cv=4)\n",
    "# dtc2_scores = cross_val_score(dtc2, X, y, cv=4)\n",
    "# dtc3_scores = cross_val_score(dtc3, X, y, cv=4)\n",
    "# dtcN_scores = cross_val_score(dtcN, X, y, cv=4)\n",
    "\n",
    "# print(dtc1_scores, np.mean(dtc1_scores))\n",
    "# print(dtc2_scores, np.mean(dtc2_scores))\n",
    "# print(dtc3_scores, np.mean(dtc3_scores))\n",
    "# print(dtcN_scores, np.mean(dtcN_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 11. Set up and run the gridsearch on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch params\n",
    "# dtc_params = {'max_depth':..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the gridsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gridsearchCV model to fit the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 12. print out the \"feature importances\"\n",
    "\n",
    "The model has an attribute called `.feature_importances_` which can tell us which features were most important vs. others. It ranges from 0 to 1, with 1 being the most important.\n",
    "\n",
    "An easy way to think about the feature importance is how much that particular variable was used to make decisions. Really though, it also takes into account how much that feature contributed to splitting up the class or reducing the variance.\n",
    "\n",
    "A feature with higher feature importance reduced the criterion (impurity) more than the other features.\n",
    "\n",
    "Below, show the feature importances for each variable predicting private vs. not, sorted by most important feature to least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill and print(the dataframe:\n",
    "fi = pd.DataFrame({\n",
    "    'feature': ...\n",
    "    'importance': ...\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
