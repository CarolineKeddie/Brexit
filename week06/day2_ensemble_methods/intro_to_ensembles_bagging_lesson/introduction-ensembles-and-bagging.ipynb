{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "# Ensemble Methods - Decision Trees and Bagging\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEARNING OBJECTIVES\n",
    "\n",
    "\n",
    "### Core\n",
    "\n",
    "- Explain the power of using ensemble classifiers\n",
    "- Know the difference between a base classifier and an ensemble classifier\n",
    "- Describe how bagging creates different training sets with bootstrapping and fits a base classifier to each of them\n",
    "- Use the bagging classifier in sklearn\n",
    "\n",
    "\n",
    "### Target\n",
    "\n",
    "- Describe\n",
    "    - The statistical problem\n",
    "    - The computational problem\n",
    "    - The representational problem\n",
    "- Describe how the bagging classifier makes predictions through a majority vote\n",
    "\n",
    "### Stretch\n",
    "\n",
    "- Describe how the bagging classifier can be stronger than any of its ensemble members\n",
    "- Be aware that to make an improvement the individual classifiers have to beat the baseline and should not all make the same predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#LEARNING-OBJECTIVES\" data-toc-modified-id=\"LEARNING-OBJECTIVES-1\">LEARNING OBJECTIVES</a></span><ul class=\"toc-item\"><li><span><a href=\"#Core\" data-toc-modified-id=\"Core-1.1\">Core</a></span></li><li><span><a href=\"#Target\" data-toc-modified-id=\"Target-1.2\">Target</a></span></li><li><span><a href=\"#Stretch\" data-toc-modified-id=\"Stretch-1.3\">Stretch</a></span></li></ul></li><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2\">Introduction</a></span></li><li><span><a href=\"#Ensembles\" data-toc-modified-id=\"Ensembles-3\">Ensembles</a></span></li><li><span><a href=\"#The-Hypothesis-space\" data-toc-modified-id=\"The-Hypothesis-space-4\">The Hypothesis space</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Statistical-Problem\" data-toc-modified-id=\"The-Statistical-Problem-4.1\">The Statistical Problem</a></span></li><li><span><a href=\"#The-Computational-Problem\" data-toc-modified-id=\"The-Computational-Problem-4.2\">The Computational Problem</a></span></li><li><span><a href=\"#The-Representational-Problem\" data-toc-modified-id=\"The-Representational-Problem-4.3\">The Representational Problem</a></span></li><li><span><a href=\"#Characteristics-of-Ensemble-methods\" data-toc-modified-id=\"Characteristics-of-Ensemble-methods-4.4\">Characteristics of Ensemble methods</a></span></li></ul></li><li><span><a href=\"#Bagging\" data-toc-modified-id=\"Bagging-5\">Bagging</a></span></li><li><span><a href=\"#Bagging-Classifier-in-Scikit-Learn\" data-toc-modified-id=\"Bagging-Classifier-in-Scikit-Learn-6\">Bagging Classifier in Scikit Learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-Import-the-car-evaluation-data\" data-toc-modified-id=\"1.-Import-the-car-evaluation-data-6.1\">1. Import the car evaluation data</a></span></li><li><span><a href=\"#2.-Encode-the-features-properly\" data-toc-modified-id=\"2.-Encode-the-features-properly-6.2\">2. Encode the features properly</a></span></li><li><span><a href=\"#3.-Create-a-train-test-split-and-cross-validate-a-KNN-classifier\" data-toc-modified-id=\"3.-Create-a-train-test-split-and-cross-validate-a-KNN-classifier-6.3\">3. Create a train-test split and cross-validate a KNN classifier</a></span></li><li><span><a href=\"#4.-Research-and-describe-the-max_samples-and-max_features-hyperparameters-of-the-bagging-classifier\" data-toc-modified-id=\"4.-Research-and-describe-the-max_samples-and-max_features-hyperparameters-of-the-bagging-classifier-6.4\">4. Research and describe the <code>max_samples</code> and <code>max_features</code> hyperparameters of the bagging classifier</a></span></li><li><span><a href=\"#5.-Fit-a-BaggingClassifier-with-a-KNN-base-estimator\" data-toc-modified-id=\"5.-Fit-a-BaggingClassifier-with-a-KNN-base-estimator-6.5\">5. Fit a <code>BaggingClassifier</code> with a KNN base estimator</a></span></li><li><span><a href=\"#6.-Cross-validate-a-decision-tree-classifier\" data-toc-modified-id=\"6.-Cross-validate-a-decision-tree-classifier-6.6\">6. Cross-validate a decision tree classifier</a></span></li><li><span><a href=\"#7.-Fit-a-BaggingClassifier-with-a-decision-tree-base-estimator\" data-toc-modified-id=\"7.-Fit-a-BaggingClassifier-with-a-decision-tree-base-estimator-6.7\">7. Fit a <code>BaggingClassifier</code> with a decision tree base estimator</a></span></li><li><span><a href=\"#8.--Of-the-Hypothesis-Space-problems-we-discussed-earlier,-which-ones-are-solved-by-bagging?\" data-toc-modified-id=\"8.--Of-the-Hypothesis-Space-problems-we-discussed-earlier,-which-ones-are-solved-by-bagging?-6.8\">8.  Of the Hypothesis Space problems we discussed earlier, which ones are solved by bagging?</a></span><ul class=\"toc-item\"><li><span><a href=\"#--Statistical?\" data-toc-modified-id=\"--Statistical?-6.8.1\">- Statistical?</a></span></li><li><span><a href=\"#--Computational?\" data-toc-modified-id=\"--Computational?-6.8.2\">- Computational?</a></span></li><li><span><a href=\"#--Representational?\" data-toc-modified-id=\"--Representational?-6.8.3\">- Representational?</a></span></li></ul></li><li><span><a href=\"#Bonus:-Tune-the-bagging-classifiers-with-grid-search\" data-toc-modified-id=\"Bonus:-Tune-the-bagging-classifiers-with-grid-search-6.9\">Bonus: Tune the bagging classifiers with grid search</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7\">Conclusion</a></span></li><li><span><a href=\"#ADDITIONAL-RESOURCES\" data-toc-modified-id=\"ADDITIONAL-RESOURCES-8\">ADDITIONAL RESOURCES</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous lessons we learned about Decision Trees as powerful tools for model building. Today we will learn about ensemble techniques: ways to combine different models in order to obtain a more powerful model.\n",
    "\n",
    "Before we dive into the subject, let's recap a few things learned so far:\n",
    "\n",
    "* What classifiers have we learned about so far? Which one is your favourite?\n",
    "\n",
    "* How did we assess the \"goodness\" of a particular model?\n",
    "\n",
    "* How could we improve the performance of a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "Ensemble techniques are supervised learning methods to improve predictive accuracy by combining several base models in order to enlarge the space of possible hypotheses to represent our data. Ensembles are often much more accurate than the base classifiers that compose them.\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "- In **averaging methods**, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimators because its variance is reduced.\n",
    "\n",
    "Examples of this family include **Bagging methods** and **Random Forests**.\n",
    "\n",
    "- The other family of ensemble methods are **boosting methods**, where base estimators are built sequentially and one tries to reduce the bias of the combined estimators. The motivation is to combine several weak models to produce a powerful ensemble. We will discuss these in a future lesson.\n",
    "\n",
    "Examples of this family include **AdaBoosts** and **Gradient Tree Boosting**.\n",
    "\n",
    "![Ensemble](./assets/images/Ensemble.png)\n",
    "\n",
    "\n",
    "- When might this be useful?\n",
    "- How will forming ensembles affect model interpretability?\n",
    "- Can you think of a business case where we may want to get a very, very accurate model (despite it being more complex)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hypothesis space\n",
    "\n",
    "In any supervised learning task, our goal is to make predictions of the true classification function $f$ by learning the classifier $h$ (our estimate of the true function $f$). In other words we are searching in a certain hypothesis space for the most appropriate function to describe the relationship between our features and the target.\n",
    "\n",
    "* Can you give an example of how this search is performed using one of the classifiers you know?\n",
    "\n",
    "\n",
    "* What reasons could be preventing our hypothesis to reach a perfect score?\n",
    "\n",
    "There could be several reasons why a base classifier doesn't perform terribly well in trying to approximate the true classification function of\n",
    "\n",
    "- statistical\n",
    "- computational\n",
    "- representational\n",
    "\n",
    "origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Statistical Problem\n",
    "If the amount of training data available is small, the base classifier will have difficulty converging to $f$.\n",
    "\n",
    "An ensemble classifier can mitigate this problem by \"averaging out\" base classifier predictions to improve convergence. This can be pictorially represented as a search in a space where multiple partial perspectives are combined to obtain a better picture of the goal.\n",
    "\n",
    "![Statistical Problem](./assets/images/statistical.png)\n",
    "\n",
    "(source: [T. Dietterich: Ensemble Methods in Machine Learning](http://www.cs.iastate.edu/~jtian/cs573/Papers/Dietterich-ensemble-00.pdf))\n",
    "\n",
    "The true function $f$ is best approximated as an average of the base classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Computational Problem\n",
    "Even with sufficient training data, it may still be computationally difficult to find the best classifier $h$.\n",
    "\n",
    "For example, if our base classifier is a decision tree, an exhaustive search of the hypothesis space of all possible classifiers is extremely complex (NP-complete).\n",
    "\n",
    "In fact this is why we used a heuristic algorithm (greedy search).\n",
    "\n",
    "An ensemble composed of several _Base Classifiers_ with different starting points can provide a better approximation to $f$ than any individual _Base Classifier_.\n",
    "\n",
    "![Computational Problem](./assets/images/computational.png)\n",
    "\n",
    "The true function $f$ is often best approximated by using several starting points to explore the hypothesis space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Representational Problem\n",
    "Sometimes $f$ cannot be expressed in terms of our hypothesis at all. To illustrate this, suppose we use a decision tree as our base classifier. A decision tree works by forming a rectilinear partition of the feature space, i.e it always cuts at a fixed value along a feature.\n",
    "\n",
    "![Decision Tree boundary](./assets/images/dtcut.png)\n",
    "\n",
    "But what if $f$ is a diagonal line?\n",
    "\n",
    "Then it cannot be represented by finitely many rectilinear segments, and therefore the true decision boundary cannot be obtained by a decision tree classifier.\n",
    "\n",
    "However, it may still be possible to approximate $f$ or even to expand the space of representable functions using ensemble methods.\n",
    "\n",
    "![Representational Problem](./assets/images/representational.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Ensemble methods\n",
    "\n",
    "In order for an ensemble classifier to outperform a single base classifier, the following conditions must be met:\n",
    "\n",
    "- **accuracy**: base classifiers outperform random guessing\n",
    "- **diversity**: misclassifications must occur on different training examples\n",
    "\n",
    "![Ensemble performance](./assets/images/ensemble_performance.png)\n",
    "\n",
    "\n",
    "* What base classifiers would you combine to have different perspectives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "- _Bagging or bootstrap aggregating_ is a method that involves manipulating the training set by resampling. \n",
    "- We learn $k$ base classifiers on $k$ different samples of training data.  \n",
    "- These samples are independently created by resampling the training data using uniform weights (e.g. a uniform sampling distribution). In other words each model in the ensemble votes with equal weight. \n",
    "- In order to reduce model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. \n",
    "- As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy.\n",
    "\n",
    "|Original|1|2|3|4|5|6|7|8|\n",
    "|----|----|----|----|----|----|----|----|----|\n",
    "|Training set 1|2|7|8|3|7|6|3|1|\n",
    "|Training set 2|7|8|5|6|4|2|7|1|\n",
    "|Training set 3|3|6|2|7|5|6|2|2|\n",
    "|Training set 4|4|5|1|4|6|4|3|8|\n",
    "\n",
    "- Given a standard training set $D$ of size $n$, bagging generates $m$ new training sets $D_i$, each of size $n'$, by sampling from $D$ uniformly and with replacement. \n",
    "- By sampling with replacement, some observations may be repeated in each $D_i$. \n",
    "- The $m$ models are fitted using the above $m$ samples and combined by averaging the output (for regression) or voting (for classification).\n",
    "\n",
    "Bagging reduces the variance in our generalization error by aggregating multiple base classifiers together (provided they satisfy our earlier requirements).\n",
    "\n",
    "If the base classifier is stable then the ensemble error is primarily due to bias, and bagging may not be effective.\n",
    "\n",
    "Since each sample of training data is equally likely, bagging is not very susceptible to overfitting with noisy data.\n",
    "\n",
    "As they provide a way to reduce overfitting, **bagging** methods work best with strong and complex models (e.g., **fully developed decision trees**), in contrast with **boosting** methods which usually work best with weak models (e.g., **shallow decision trees**).\n",
    "\n",
    "* Can you propose another sample to add to those above? Call out the numbers you would include."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier in Scikit Learn \n",
    "\n",
    "In scikit-learn, bagging methods are offered as a unified `BaggingClassifier` meta-estimator (resp. `BaggingRegressor`), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, `max_samples` and `max_features` control the size of the subsets (in terms of samples and features), while `bootstrap` and `bootstrap_features` control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization error can be estimated with the out-of-bag samples by setting `oob_score=True`.\n",
    "\n",
    "As an example, we will compare the performance of a simple KNN classifier versus the Bagging Classifier on the car acceptability dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the car evaluation data\n",
    "\n",
    "Use `acceptability` as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../../../resource-datasets/car_evaluation/car.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc\n",
       "2  vhigh  vhigh     2       2    small   high         unacc\n",
       "3  vhigh  vhigh     2       2      med    low         unacc\n",
       "4  vhigh  vhigh     2       2      med    med         unacc"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encode the features properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a train-test split and cross-validate a KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Research and describe the `max_samples` and `max_features` hyperparameters of the bagging classifier\n",
    "\n",
    "The `BaggingClassifier` meta-estimator has several parameters.\n",
    "\n",
    "Look at the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) for a detailed description of each and find out what `max_samples` and `max_features` do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fit a `BaggingClassifier` with a KNN base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cross-validate a decision tree classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fit a `BaggingClassifier` with a decision tree base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  Of the Hypothesis Space problems we discussed earlier, which ones are solved by bagging?\n",
    "\n",
    "#### - Statistical?\n",
    "#### - Computational?\n",
    "#### - Representational?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Tune the bagging classifiers with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "In this lesson we have learned about Ensemble Models and Bagging Classifiers. We have learned how they improve the performance of individual base models thanks to their better ability to approximate the real prediction function in a supervised learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDITIONAL RESOURCES\n",
    "\n",
    "- [Ensemble models on wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "- [Bagging on wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating)\n",
    "- [Ensemble methods on Scikit Learn](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Bagging Classifier documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "- [Bias Varias Decomposition Scikit Learn Example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#example-ensemble-plot-bias-variance-py)\n",
    "- [T. Dietterich: Ensemble Methods in Machine Learning](http://www.cs.iastate.edu/~jtian/cs573/Papers/Dietterich-ensemble-00.pdf)\n",
    "- [N.C. Oza and K. Tumer: Classifier Ensembles: Select Real World Applications](https://www.researchgate.net/profile/Nikunj_Oza/publication/222425707_Classifier_ensembles_Select_real-world_applications/links/0c960514cd67f0fdde000000/Classifier-ensembles-Select-real-world-applications.pdf)\n",
    "- [KDNuggets article 1](http://www.kdnuggets.com/2016/02/ensemble-methods-techniques-produce-improved-machine-learning.html)\n",
    "- [KDNuggets article 2](http://www.kdnuggets.com/faq/simple-data-mining-case-study.html)\n",
    "- [Ensemble Methods book](http://www.amazon.com/dp/1608452840)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
