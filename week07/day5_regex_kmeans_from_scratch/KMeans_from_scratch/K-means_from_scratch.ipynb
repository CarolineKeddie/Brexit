{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from itertools import combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means from scratch\n",
    "\n",
    "We are going to implement the K-means algorithm from scratch, and test it on the classic Fisher's Iris  dataset.\n",
    "\n",
    "K-means Algorithm Summary:\n",
    "- Initialize your k cluster centroids (randomly select k different observations)\n",
    "- Compute the distance between each point and every centroid\n",
    "- Assign each data point to the centroid closest to it\n",
    "- Move the cluster centroid to the center (mean) of all the points assigned to it\n",
    "- Repeat until you reach stopping criteria (either convergence or maximum iterations)\n",
    "- Typically k-means is performed on scaled data.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "You should implement k-means using either a functional or object-oriented approach. An object-oriented implementation might have a KMeans class with a \"fit\" method, and a functional implementation might have a \"k_means\" function that takes the data and numer of clusters as arguments and returns the centroids and assignemnts.\n",
    "\n",
    "Load the dataset with sklearn.datasets.load_iris(), but since we will be hand coding our Kmeans in numpy we only need to get the features into an array. Create a numpy array of the features of the iris dataset. Do not use the labels for the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Numpy, initialize your cluster centers by selecting random data points. We will try our algorithm with multiple different k but let us start with 10. Pick at random 10 of our initial points. (Hint: try using http://docs.python.org/2/library/random.html#random.sample  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each one of your data points, compute the Euclidean distance between it and every centroid. Assign the point to the closest centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update each centroid by moving it to the center of all the points assigned to it. (E.g. make it the average of all the points in the cluster.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat steps 3 and 4 until convergence or max_iter is reached. If no cluster assignments change between iterations, then the algorithm has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "def k_means(X, k=5, max_iter=1000):\n",
    "    \"\"\"Performs k means\n",
    "\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - k - number of clusters\n",
    "    - max_iter - maximum iteratations\n",
    "\n",
    "    Returns:\n",
    "    - clusters - dict mapping cluster centers to observations\n",
    "    \"\"\"\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (Bonus): Selecting k\n",
    "\n",
    "Often it is tough to pick an ideal k in advance. We can force k in our case if we want a predetermined number of sections/topics. But it is most likely better to vary k and let the algorithm tell us what it wants. We can do choose an optimal k using the elbow method  .\n",
    "\n",
    "Run the algorithm with increasing values of k. For each, compute the sum of squared error (SSE  ). This is the distance of each point to its final centroid, squared, and summed over all datas points. Plot this for each value of k and try to find an elbow. Determining the number of clusters. Is there an optimal # of K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another metric to assess how well your data has been clustered is the Silhouette coefficient. Using scikit-learn's metric package compute the silhouette coefficient  of the clusters produced by your own Kmeans implementation on the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the centroid assignments. Create a plot of the cluster assignments on the iris data. Each data point should be colored according to its assignment. First make a 2-d plot of each pair of features for the iris dataset. If you are feeling fancy make a 3-d plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your cluster results with scikit-learn Kmeans  . Since K-means is a stochastic algorithm (random initialization) your result will be slightly (but hopefully not too) different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
