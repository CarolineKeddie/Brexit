{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Gridsearching Hyperparameters\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"gridsearch_meme.png\" style=\"width: 500px;\">\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand what the terms gridsearch and hyperparameter refer to.\n",
    "- Understand how to manually build a gridsearching procedure.\n",
    "- Apply sklearn's `GridSearchCV` object with basketball data to optimize a KNN model.\n",
    "- Practice using and evaluating attributes of the gridsearch object.\n",
    "- Understand the pitfalls of searching large hyperparameter spaces.\n",
    "- Practice the gridsearch procedure independently optimizing regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture: What part of the modelling process are we focusing on this morning?\n",
    "\n",
    "---\n",
    "\n",
    "As we looked at yesterday, one general approach you can use for modelling questions would look like this:\n",
    "\n",
    "- STEP ONE: Cleaning, descriptive stats, correlation heatmap, plots & visualisations. Find baseline accuracy if it's a classifier.\n",
    "\n",
    "- STEP TWO: Set up predictor matrix (X) and target array (y).  Dummify if necessary.\n",
    "\n",
    "- STEP THREE: Train/test split and StandardScaler( )\n",
    "\n",
    "- STEP FOUR: Use cross-validation to optimise the hyperparameters for your model. You might try different types of models at this stage as well, and you might use GridSearchCV (or any of the other CVs like RidgeCV).\n",
    "\n",
    "- STEP FIVE: Once you're happy with your hyperparameters, fit your model on your whole training data and test it on your whole testing data.\n",
    "\n",
    "- STEP SIX: Then you might want to evaluate the performance of the model (R2 score, accuracy, confusion matrix, etc); find the actual predictions that your model is providing and store them in a dataframe; plot your predictions against your actual target variable to visualise how well your model performed; investigate feature importance with .coef_ if you have a parametric model.\n",
    "\n",
    "This morning, we're learning more about STEP FOUR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## What is \"Gridsearching\"? What are \"hyperparameters\"?\n",
    "\n",
    "---\n",
    "\n",
    "Models often have specifications that can be set. For example, when we choose a linear regression, we may decide to add a penalty to the loss function such as the Ridge or the Lasso. Those penalties require the regularization strength, alpha, to be set. \n",
    "\n",
    "**Model parameters are called hyperparameters.**\n",
    "\n",
    "Hyperparameters are different than the parameters of the model resulting from a fit, such as the coefficients. The hyperparameters are set prior to the fit and determine the behavior of the model.\n",
    "\n",
    "There are often more than one kind of hyperparamter to set for a model. For example, in the KNN algorithm, \n",
    "- we have a hyperparameter to set the number of neighbors. \n",
    "- We also have a hyperparameter for the weights: uniform or distance?\n",
    "\n",
    "We want to know the *optimal* hyperparameter settings, the set that results in the best model evaluation. \n",
    "\n",
    "**The search for the optimal set of hyperparameters is called gridsearching.**\n",
    "\n",
    "Gridsearching gets its name from the fact that we are searching over a **\"grid\" of parameters**. For example, imagine the `n_neighbors` hyperparameters on the x-axis and `weights` on the y-axis, and we need to test all points on the grid.\n",
    "\n",
    "**Gridsearching uses cross-validation internally to evaluate the performance of each set of hyperparameters.** More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basketball-data'></a>\n",
    "\n",
    "## Basketball data\n",
    "\n",
    "---\n",
    "\n",
    "To explore the process of gridsearching over sets of hyperparameters, we will use some basketball data. The data below has statistics for 4 different seasons of NBA basketball: 2013-2016.\n",
    "- This data includes aggregate statistical data for each game. \n",
    "- The data of each game is aggregated by match for all players.\n",
    "- Scraped from http://www.basketball-refrence.com\n",
    "\n",
    "Many of the columns in the dataset represent the mean of a statistic across the last 10 games, for example. Non-target statistics are for *prior* games, they do not include information about player performance in the current game.\n",
    "\n",
    "**We are interested in predicting whether the home team will win the game or not.** This is a classification problem.\n",
    "\n",
    "\n",
    "### Load the data and create the target and predictor matrix\n",
    "- The target will be a binary column of whether the home team wins.\n",
    "- The predictors should be numeric statistics columns.\n",
    "\n",
    "Exclude these columns from the predictor matrix:\n",
    "\n",
    "    ['GameId','GameDate','GameTime','HostName',\n",
    "     'GuestName','total_score','total_line','game_line',\n",
    "     'winner','loser','host_wins','Season']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### STEP ONE: Cleaning, descriptive stats, correlation heatmap, plots & visualisations. Find baseline accuracy if it's a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/basketball_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>GameId</th>\n",
       "      <th>GameDate</th>\n",
       "      <th>GameTime</th>\n",
       "      <th>HostName</th>\n",
       "      <th>GuestName</th>\n",
       "      <th>total_score</th>\n",
       "      <th>total_line</th>\n",
       "      <th>game_line</th>\n",
       "      <th>Host_HostRank</th>\n",
       "      <th>Host_GameRank</th>\n",
       "      <th>Guest_GuestRank</th>\n",
       "      <th>Guest_GameRank</th>\n",
       "      <th>host_win_count</th>\n",
       "      <th>host_lose_count</th>\n",
       "      <th>guest_win_count</th>\n",
       "      <th>guest_lose_count</th>\n",
       "      <th>game_behind</th>\n",
       "      <th>winner</th>\n",
       "      <th>loser</th>\n",
       "      <th>host_place_streak</th>\n",
       "      <th>guest_place_streak</th>\n",
       "      <th>hq1_avg10</th>\n",
       "      <th>hq2_avg10</th>\n",
       "      <th>hq3_avg10</th>\n",
       "      <th>hq4_avg10</th>\n",
       "      <th>hPace_avg10</th>\n",
       "      <th>heFG%_avg10</th>\n",
       "      <th>hTOV%_avg10</th>\n",
       "      <th>hORB%_avg10</th>\n",
       "      <th>hFT/FGA_avg10</th>\n",
       "      <th>hORtg_avg10</th>\n",
       "      <th>hFG_avg10</th>\n",
       "      <th>hFGA_avg10</th>\n",
       "      <th>hFG%_avg10</th>\n",
       "      <th>h3P_avg10</th>\n",
       "      <th>h3PA_avg10</th>\n",
       "      <th>h3P%_avg10</th>\n",
       "      <th>hFT_avg10</th>\n",
       "      <th>hFTA_avg10</th>\n",
       "      <th>hFT%_avg10</th>\n",
       "      <th>hORB_avg10</th>\n",
       "      <th>hDRB_avg10</th>\n",
       "      <th>hTRB_avg10</th>\n",
       "      <th>hAST_avg10</th>\n",
       "      <th>hSTL_avg10</th>\n",
       "      <th>hBLK_avg10</th>\n",
       "      <th>hTOV_avg10</th>\n",
       "      <th>hPF_avg10</th>\n",
       "      <th>hPTS_avg10</th>\n",
       "      <th>hTS%_avg10</th>\n",
       "      <th>h3PAR_avg10</th>\n",
       "      <th>hFTr_avg10</th>\n",
       "      <th>hDRB%_avg10</th>\n",
       "      <th>hTRB%_avg10</th>\n",
       "      <th>hAST%_avg10</th>\n",
       "      <th>hSTL%_avg10</th>\n",
       "      <th>hBLK%_avg10</th>\n",
       "      <th>hDRtg_avg10</th>\n",
       "      <th>gq1_avg10</th>\n",
       "      <th>gq2_avg10</th>\n",
       "      <th>gq3_avg10</th>\n",
       "      <th>gq4_avg10</th>\n",
       "      <th>gPace_avg10</th>\n",
       "      <th>geFG%_avg10</th>\n",
       "      <th>gTOV%_avg10</th>\n",
       "      <th>gORB%_avg10</th>\n",
       "      <th>gFT/FGA_avg10</th>\n",
       "      <th>gORtg_avg10</th>\n",
       "      <th>gFG_avg10</th>\n",
       "      <th>gFGA_avg10</th>\n",
       "      <th>gFG%_avg10</th>\n",
       "      <th>g3P_avg10</th>\n",
       "      <th>g3PA_avg10</th>\n",
       "      <th>g3P%_avg10</th>\n",
       "      <th>gFT_avg10</th>\n",
       "      <th>gFTA_avg10</th>\n",
       "      <th>gFT%_avg10</th>\n",
       "      <th>gORB_avg10</th>\n",
       "      <th>gDRB_avg10</th>\n",
       "      <th>gTRB_avg10</th>\n",
       "      <th>gAST_avg10</th>\n",
       "      <th>gSTL_avg10</th>\n",
       "      <th>gBLK_avg10</th>\n",
       "      <th>gTOV_avg10</th>\n",
       "      <th>gPF_avg10</th>\n",
       "      <th>gPTS_avg10</th>\n",
       "      <th>gTS%_avg10</th>\n",
       "      <th>g3PAR_avg10</th>\n",
       "      <th>gFTr_avg10</th>\n",
       "      <th>gDRB%_avg10</th>\n",
       "      <th>gTRB%_avg10</th>\n",
       "      <th>gAST%_avg10</th>\n",
       "      <th>gSTL%_avg10</th>\n",
       "      <th>gBLK%_avg10</th>\n",
       "      <th>gDRtg_avg10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212090LAL</td>\n",
       "      <td>2012-12-09</td>\n",
       "      <td>6:30 pm</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>Utah Jazz</td>\n",
       "      <td>227.0</td>\n",
       "      <td>207.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>Utah Jazz</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>25.5</td>\n",
       "      <td>24.2</td>\n",
       "      <td>23.1</td>\n",
       "      <td>93.57</td>\n",
       "      <td>0.5059</td>\n",
       "      <td>13.28</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0.2379</td>\n",
       "      <td>109.54</td>\n",
       "      <td>37.3</td>\n",
       "      <td>81.8</td>\n",
       "      <td>0.4549</td>\n",
       "      <td>8.4</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.3560</td>\n",
       "      <td>19.4</td>\n",
       "      <td>29.8</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>13.9</td>\n",
       "      <td>34.3</td>\n",
       "      <td>48.2</td>\n",
       "      <td>23.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>17.2</td>\n",
       "      <td>102.4</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.3681</td>\n",
       "      <td>74.79</td>\n",
       "      <td>53.76</td>\n",
       "      <td>61.45</td>\n",
       "      <td>8.45</td>\n",
       "      <td>9.09</td>\n",
       "      <td>98.77</td>\n",
       "      <td>23.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.1</td>\n",
       "      <td>24.8</td>\n",
       "      <td>90.35</td>\n",
       "      <td>0.4791</td>\n",
       "      <td>13.96</td>\n",
       "      <td>31.63</td>\n",
       "      <td>0.2237</td>\n",
       "      <td>106.00</td>\n",
       "      <td>36.7</td>\n",
       "      <td>84.3</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>7.4</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.3891</td>\n",
       "      <td>18.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.7611</td>\n",
       "      <td>14.6</td>\n",
       "      <td>29.0</td>\n",
       "      <td>43.6</td>\n",
       "      <td>22.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.2230</td>\n",
       "      <td>0.2981</td>\n",
       "      <td>69.22</td>\n",
       "      <td>50.05</td>\n",
       "      <td>61.57</td>\n",
       "      <td>8.63</td>\n",
       "      <td>10.31</td>\n",
       "      <td>110.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212100PHI</td>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>201.0</td>\n",
       "      <td>186.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.5</td>\n",
       "      <td>23.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>90.77</td>\n",
       "      <td>0.4756</td>\n",
       "      <td>10.42</td>\n",
       "      <td>23.05</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>103.67</td>\n",
       "      <td>36.7</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>7.2</td>\n",
       "      <td>18.6</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>15.3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.7442</td>\n",
       "      <td>10.5</td>\n",
       "      <td>29.9</td>\n",
       "      <td>40.4</td>\n",
       "      <td>22.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>95.9</td>\n",
       "      <td>0.5111</td>\n",
       "      <td>0.2205</td>\n",
       "      <td>0.2496</td>\n",
       "      <td>74.07</td>\n",
       "      <td>46.68</td>\n",
       "      <td>62.22</td>\n",
       "      <td>8.89</td>\n",
       "      <td>7.93</td>\n",
       "      <td>106.11</td>\n",
       "      <td>22.7</td>\n",
       "      <td>23.9</td>\n",
       "      <td>20.5</td>\n",
       "      <td>23.2</td>\n",
       "      <td>90.89</td>\n",
       "      <td>0.4635</td>\n",
       "      <td>14.95</td>\n",
       "      <td>27.20</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>99.55</td>\n",
       "      <td>33.2</td>\n",
       "      <td>78.6</td>\n",
       "      <td>0.4249</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>17.9</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.7465</td>\n",
       "      <td>11.6</td>\n",
       "      <td>31.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>15.7</td>\n",
       "      <td>19.4</td>\n",
       "      <td>90.3</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.3095</td>\n",
       "      <td>71.46</td>\n",
       "      <td>49.48</td>\n",
       "      <td>59.83</td>\n",
       "      <td>6.48</td>\n",
       "      <td>9.46</td>\n",
       "      <td>107.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212100HOU</td>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>240.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>27.4</td>\n",
       "      <td>25.2</td>\n",
       "      <td>95.89</td>\n",
       "      <td>0.5196</td>\n",
       "      <td>13.66</td>\n",
       "      <td>25.80</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>111.92</td>\n",
       "      <td>37.7</td>\n",
       "      <td>82.8</td>\n",
       "      <td>0.4566</td>\n",
       "      <td>10.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>0.3763</td>\n",
       "      <td>21.7</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.7811</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.9</td>\n",
       "      <td>42.9</td>\n",
       "      <td>22.8</td>\n",
       "      <td>8.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>107.4</td>\n",
       "      <td>0.5665</td>\n",
       "      <td>0.3437</td>\n",
       "      <td>0.3414</td>\n",
       "      <td>72.69</td>\n",
       "      <td>50.45</td>\n",
       "      <td>60.73</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.26</td>\n",
       "      <td>105.69</td>\n",
       "      <td>26.7</td>\n",
       "      <td>23.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>28.5</td>\n",
       "      <td>93.53</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>13.95</td>\n",
       "      <td>21.94</td>\n",
       "      <td>0.1998</td>\n",
       "      <td>113.31</td>\n",
       "      <td>41.5</td>\n",
       "      <td>82.9</td>\n",
       "      <td>0.5044</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>0.3828</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>0.7823</td>\n",
       "      <td>8.1</td>\n",
       "      <td>34.3</td>\n",
       "      <td>42.4</td>\n",
       "      <td>25.7</td>\n",
       "      <td>7.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>14.8</td>\n",
       "      <td>17.7</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.5915</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>74.26</td>\n",
       "      <td>50.99</td>\n",
       "      <td>61.82</td>\n",
       "      <td>8.30</td>\n",
       "      <td>6.85</td>\n",
       "      <td>101.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212110BRK</td>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>197.0</td>\n",
       "      <td>195.5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>27.9</td>\n",
       "      <td>22.9</td>\n",
       "      <td>22.4</td>\n",
       "      <td>23.1</td>\n",
       "      <td>87.74</td>\n",
       "      <td>0.4884</td>\n",
       "      <td>12.86</td>\n",
       "      <td>32.49</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>110.14</td>\n",
       "      <td>36.0</td>\n",
       "      <td>82.6</td>\n",
       "      <td>0.4376</td>\n",
       "      <td>8.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>17.1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>13.9</td>\n",
       "      <td>29.2</td>\n",
       "      <td>43.1</td>\n",
       "      <td>22.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>13.6</td>\n",
       "      <td>18.7</td>\n",
       "      <td>97.5</td>\n",
       "      <td>0.5271</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>0.2805</td>\n",
       "      <td>71.36</td>\n",
       "      <td>51.28</td>\n",
       "      <td>62.69</td>\n",
       "      <td>7.45</td>\n",
       "      <td>9.00</td>\n",
       "      <td>107.43</td>\n",
       "      <td>26.5</td>\n",
       "      <td>25.3</td>\n",
       "      <td>24.6</td>\n",
       "      <td>23.4</td>\n",
       "      <td>90.22</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>11.32</td>\n",
       "      <td>23.64</td>\n",
       "      <td>0.1931</td>\n",
       "      <td>110.24</td>\n",
       "      <td>36.4</td>\n",
       "      <td>82.9</td>\n",
       "      <td>0.4412</td>\n",
       "      <td>11.8</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.3879</td>\n",
       "      <td>15.7</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0.7519</td>\n",
       "      <td>10.4</td>\n",
       "      <td>29.7</td>\n",
       "      <td>40.1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>20.1</td>\n",
       "      <td>100.3</td>\n",
       "      <td>0.5473</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>74.23</td>\n",
       "      <td>47.88</td>\n",
       "      <td>52.07</td>\n",
       "      <td>9.31</td>\n",
       "      <td>7.64</td>\n",
       "      <td>109.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212110DET</td>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>7:30 pm</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>Denver Nuggets</td>\n",
       "      <td>195.0</td>\n",
       "      <td>203.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Denver Nuggets</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>24.6</td>\n",
       "      <td>26.6</td>\n",
       "      <td>23.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>90.18</td>\n",
       "      <td>0.5210</td>\n",
       "      <td>12.99</td>\n",
       "      <td>26.43</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>111.00</td>\n",
       "      <td>37.6</td>\n",
       "      <td>78.3</td>\n",
       "      <td>0.4819</td>\n",
       "      <td>6.1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>18.8</td>\n",
       "      <td>26.3</td>\n",
       "      <td>0.7152</td>\n",
       "      <td>10.6</td>\n",
       "      <td>29.5</td>\n",
       "      <td>40.1</td>\n",
       "      <td>21.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>13.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>100.1</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.1905</td>\n",
       "      <td>0.3389</td>\n",
       "      <td>70.89</td>\n",
       "      <td>49.33</td>\n",
       "      <td>57.26</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.40</td>\n",
       "      <td>105.30</td>\n",
       "      <td>24.4</td>\n",
       "      <td>29.1</td>\n",
       "      <td>23.2</td>\n",
       "      <td>24.4</td>\n",
       "      <td>92.56</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>15.32</td>\n",
       "      <td>30.31</td>\n",
       "      <td>0.2257</td>\n",
       "      <td>109.28</td>\n",
       "      <td>38.5</td>\n",
       "      <td>79.4</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>6.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.7114</td>\n",
       "      <td>11.8</td>\n",
       "      <td>29.3</td>\n",
       "      <td>41.1</td>\n",
       "      <td>21.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>20.4</td>\n",
       "      <td>101.1</td>\n",
       "      <td>0.5605</td>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>68.45</td>\n",
       "      <td>50.40</td>\n",
       "      <td>56.33</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.83</td>\n",
       "      <td>114.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season        GameId    GameDate GameTime            HostName  \\\n",
       "0    2013  201212090LAL  2012-12-09  6:30 pm  Los Angeles Lakers   \n",
       "1    2013  201212100PHI  2012-12-10  7:00 pm  Philadelphia 76ers   \n",
       "2    2013  201212100HOU  2012-12-10  7:00 pm     Houston Rockets   \n",
       "3    2013  201212110BRK  2012-12-11  7:00 pm       Brooklyn Nets   \n",
       "4    2013  201212110DET  2012-12-11  7:30 pm     Detroit Pistons   \n",
       "\n",
       "           GuestName  total_score  total_line  game_line  Host_HostRank  \\\n",
       "0          Utah Jazz        227.0       207.5        7.5             13   \n",
       "1    Detroit Pistons        201.0       186.5        5.5             13   \n",
       "2  San Antonio Spurs        240.0       212.0       -7.0             12   \n",
       "3    New York Knicks        197.0       195.5       -3.5             12   \n",
       "4     Denver Nuggets        195.0       203.5       -4.5             11   \n",
       "\n",
       "   Host_GameRank  Guest_GuestRank  Guest_GameRank  host_win_count  \\\n",
       "0             21               13              22               9   \n",
       "1             21               13              23              11   \n",
       "2             20               13              22               9   \n",
       "3             20               13              21              11   \n",
       "4             24               16              22               7   \n",
       "\n",
       "   host_lose_count  guest_win_count  guest_lose_count  game_behind  \\\n",
       "0               11               11                10         -1.5   \n",
       "1                9                7                15          5.0   \n",
       "2               10               17                 4         -7.0   \n",
       "3                8               15                 5         -3.5   \n",
       "4               16               10                11         -4.0   \n",
       "\n",
       "               winner               loser  host_place_streak  \\\n",
       "0           Utah Jazz  Los Angeles Lakers                  1   \n",
       "1  Philadelphia 76ers     Detroit Pistons                  1   \n",
       "2   San Antonio Spurs     Houston Rockets                  2   \n",
       "3     New York Knicks       Brooklyn Nets                  4   \n",
       "4      Denver Nuggets     Detroit Pistons                  1   \n",
       "\n",
       "   guest_place_streak  hq1_avg10  hq2_avg10  hq3_avg10  hq4_avg10  \\\n",
       "0                   1       29.6       25.5       24.2       23.1   \n",
       "1                   2       23.5       23.6       24.5       22.6   \n",
       "2                   2       24.0       30.8       27.4       25.2   \n",
       "3                   1       27.9       22.9       22.4       23.1   \n",
       "4                   4       24.6       26.6       23.3       25.6   \n",
       "\n",
       "   hPace_avg10  heFG%_avg10  hTOV%_avg10  hORB%_avg10  hFT/FGA_avg10  \\\n",
       "0        93.57       0.5059        13.28        32.11         0.2379   \n",
       "1        90.77       0.4756        10.42        23.05         0.1818   \n",
       "2        95.89       0.5196        13.66        25.80         0.2645   \n",
       "3        87.74       0.4884        12.86        32.49         0.2083   \n",
       "4        90.18       0.5210        12.99        26.43         0.2430   \n",
       "\n",
       "   hORtg_avg10  hFG_avg10  hFGA_avg10  hFG%_avg10  h3P_avg10  h3PA_avg10  \\\n",
       "0       109.54       37.3        81.8      0.4549        8.4        23.5   \n",
       "1       103.67       36.7        85.0      0.4328        7.2        18.6   \n",
       "2       111.92       37.7        82.8      0.4566       10.3        28.3   \n",
       "3       110.14       36.0        82.6      0.4376        8.4        24.1   \n",
       "4       111.00       37.6        78.3      0.4819        6.1        14.9   \n",
       "\n",
       "   h3P%_avg10  hFT_avg10  hFTA_avg10  hFT%_avg10  hORB_avg10  hDRB_avg10  \\\n",
       "0      0.3560       19.4        29.8      0.6611        13.9        34.3   \n",
       "1      0.3796       15.3        21.0      0.7442        10.5        29.9   \n",
       "2      0.3763       21.7        27.9      0.7811        11.0        31.9   \n",
       "3      0.3473       17.1        23.0      0.7380        13.9        29.2   \n",
       "4      0.4079       18.8        26.3      0.7152        10.6        29.5   \n",
       "\n",
       "   hTRB_avg10  hAST_avg10  hSTL_avg10  hBLK_avg10  hTOV_avg10  hPF_avg10  \\\n",
       "0        48.2        23.1         7.9         6.0        14.5       17.2   \n",
       "1        40.4        22.8         8.3         5.0        11.0       20.4   \n",
       "2        42.9        22.8         8.4         4.6        15.0       19.7   \n",
       "3        43.1        22.5         6.6         5.8        13.6       18.7   \n",
       "4        40.1        21.5         7.1         4.7        13.5       21.0   \n",
       "\n",
       "   hPTS_avg10  hTS%_avg10  h3PAR_avg10  hFTr_avg10  hDRB%_avg10  hTRB%_avg10  \\\n",
       "0       102.4      0.5399       0.2880      0.3681        74.79        53.76   \n",
       "1        95.9      0.5111       0.2205      0.2496        74.07        46.68   \n",
       "2       107.4      0.5665       0.3437      0.3414        72.69        50.45   \n",
       "3        97.5      0.5271       0.2908      0.2805        71.36        51.28   \n",
       "4       100.1      0.5585       0.1905      0.3389        70.89        49.33   \n",
       "\n",
       "   hAST%_avg10  hSTL%_avg10  hBLK%_avg10  hDRtg_avg10  gq1_avg10  gq2_avg10  \\\n",
       "0        61.45         8.45         9.09        98.77       23.5       22.0   \n",
       "1        62.22         8.89         7.93       106.11       22.7       23.9   \n",
       "2        60.73         8.76         7.26       105.69       26.7       23.2   \n",
       "3        62.69         7.45         9.00       107.43       26.5       25.3   \n",
       "4        57.26         7.84         7.40       105.30       24.4       29.1   \n",
       "\n",
       "   gq3_avg10  gq4_avg10  gPace_avg10  geFG%_avg10  gTOV%_avg10  gORB%_avg10  \\\n",
       "0       25.1       24.8        90.35       0.4791        13.96        31.63   \n",
       "1       20.5       23.2        90.89       0.4635        14.95        27.20   \n",
       "2       27.3       28.5        93.53       0.5585        13.95        21.94   \n",
       "3       24.6       23.4        90.22       0.5121        11.32        23.64   \n",
       "4       23.2       24.4        92.56       0.5267        15.32        30.31   \n",
       "\n",
       "   gFT/FGA_avg10  gORtg_avg10  gFG_avg10  gFGA_avg10  gFG%_avg10  g3P_avg10  \\\n",
       "0         0.2237       106.00       36.7        84.3      0.4352        7.4   \n",
       "1         0.2308        99.55       33.2        78.6      0.4249        6.0   \n",
       "2         0.1998       113.31       41.5        82.9      0.5044        9.0   \n",
       "3         0.1931       110.24       36.4        82.9      0.4412       11.8   \n",
       "4         0.2257       109.28       38.5        79.4      0.4870        6.3   \n",
       "\n",
       "   g3PA_avg10  g3P%_avg10  gFT_avg10  gFTA_avg10  gFT%_avg10  gORB_avg10  \\\n",
       "0        18.9      0.3891       18.2        24.2      0.7611        14.6   \n",
       "1        16.9      0.3673       17.9        24.0      0.7465        11.6   \n",
       "2        22.8      0.3828       16.0        20.2      0.7823         8.1   \n",
       "3        30.1      0.3879       15.7        20.8      0.7519        10.4   \n",
       "4        17.3      0.3612       17.8        25.0      0.7114        11.8   \n",
       "\n",
       "   gDRB_avg10  gTRB_avg10  gAST_avg10  gSTL_avg10  gBLK_avg10  gTOV_avg10  \\\n",
       "0        29.0        43.6        22.4         8.0         7.1        15.4   \n",
       "1        31.4        43.0        20.0         5.9         5.8        15.7   \n",
       "2        34.3        42.4        25.7         7.8         4.9        14.8   \n",
       "3        29.7        40.1        19.0         8.5         4.7        11.7   \n",
       "4        29.3        41.1        21.7         7.1         5.0        16.4   \n",
       "\n",
       "   gPF_avg10  gPTS_avg10  gTS%_avg10  g3PAR_avg10  gFTr_avg10  gDRB%_avg10  \\\n",
       "0       21.0        99.0      0.5206       0.2230      0.2981        69.22   \n",
       "1       19.4        90.3      0.5077       0.2144      0.3095        71.46   \n",
       "2       17.7       108.0      0.5915       0.2743      0.2518        74.26   \n",
       "3       20.1       100.3      0.5473       0.3595      0.2544        74.23   \n",
       "4       20.4       101.1      0.5605       0.2173      0.3177        68.45   \n",
       "\n",
       "   gTRB%_avg10  gAST%_avg10  gSTL%_avg10  gBLK%_avg10  gDRtg_avg10  \n",
       "0        50.05        61.57         8.63        10.31       110.87  \n",
       "1        49.48        59.83         6.48         9.46       107.91  \n",
       "2        50.99        61.82         8.30         6.85       101.41  \n",
       "3        47.88        52.07         9.31         7.64       109.24  \n",
       "4        50.40        56.33         7.67         7.83       114.86  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'GameId', 'GameDate', 'GameTime', 'HostName', 'GuestName',\n",
       "       'total_score', 'total_line', 'game_line', 'Host_HostRank',\n",
       "       'Host_GameRank', 'Guest_GuestRank', 'Guest_GameRank', 'host_win_count',\n",
       "       'host_lose_count', 'guest_win_count', 'guest_lose_count', 'game_behind',\n",
       "       'winner', 'loser', 'host_place_streak', 'guest_place_streak',\n",
       "       'hq1_avg10', 'hq2_avg10', 'hq3_avg10', 'hq4_avg10', 'hPace_avg10',\n",
       "       'heFG%_avg10', 'hTOV%_avg10', 'hORB%_avg10', 'hFT/FGA_avg10',\n",
       "       'hORtg_avg10', 'hFG_avg10', 'hFGA_avg10', 'hFG%_avg10', 'h3P_avg10',\n",
       "       'h3PA_avg10', 'h3P%_avg10', 'hFT_avg10', 'hFTA_avg10', 'hFT%_avg10',\n",
       "       'hORB_avg10', 'hDRB_avg10', 'hTRB_avg10', 'hAST_avg10', 'hSTL_avg10',\n",
       "       'hBLK_avg10', 'hTOV_avg10', 'hPF_avg10', 'hPTS_avg10', 'hTS%_avg10',\n",
       "       'h3PAR_avg10', 'hFTr_avg10', 'hDRB%_avg10', 'hTRB%_avg10',\n",
       "       'hAST%_avg10', 'hSTL%_avg10', 'hBLK%_avg10', 'hDRtg_avg10', 'gq1_avg10',\n",
       "       'gq2_avg10', 'gq3_avg10', 'gq4_avg10', 'gPace_avg10', 'geFG%_avg10',\n",
       "       'gTOV%_avg10', 'gORB%_avg10', 'gFT/FGA_avg10', 'gORtg_avg10',\n",
       "       'gFG_avg10', 'gFGA_avg10', 'gFG%_avg10', 'g3P_avg10', 'g3PA_avg10',\n",
       "       'g3P%_avg10', 'gFT_avg10', 'gFTA_avg10', 'gFT%_avg10', 'gORB_avg10',\n",
       "       'gDRB_avg10', 'gTRB_avg10', 'gAST_avg10', 'gSTL_avg10', 'gBLK_avg10',\n",
       "       'gTOV_avg10', 'gPF_avg10', 'gPTS_avg10', 'gTS%_avg10', 'g3PAR_avg10',\n",
       "       'gFTr_avg10', 'gDRB%_avg10', 'gTRB%_avg10', 'gAST%_avg10',\n",
       "       'gSTL%_avg10', 'gBLK%_avg10', 'gDRtg_avg10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3768, 96)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014    998\n",
       "2016    985\n",
       "2015    984\n",
       "2013    801\n",
       "Name: Season, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Season.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Utah Jazz\n",
       "1    Philadelphia 76ers\n",
       "2     San Antonio Spurs\n",
       "3       New York Knicks\n",
       "4        Denver Nuggets\n",
       "Name: winner, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.winner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#let's create a column called 'host_wins' which will indicate\n",
    "#whether the host team won the game or not\n",
    "data['host_wins'] = (data['HostName'] == data['winner']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.594214\n",
       "0    0.405786\n",
       "Name: host_wins, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at the baseline accuracy for this data\n",
    "baseline = data['host_wins'].value_counts(normalize=True) \n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At this point, we could definitely do a bit more EDA!  We could...\n",
    "- use .describe() to find some descriptive statistics\n",
    "- create some plots & visualisations to better understand the shapes and relationships in our data\n",
    "- use a correlation heatmap to help us find variables that could predict 'host_wins'\n",
    "\n",
    "...but we're going to skip that for this lesson!  Be aware that if you're undertaking your own data investigation, skipping EDA is a bad idea. (Obviously.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### STEP TWO: Set up predictor matrix (X) and target array (y).  Dummify if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictors = [c for c in data.columns if c not in ['GameId','GameDate','GameTime','HostName',\n",
    "                                                   'GuestName','total_score','total_line','game_line',\n",
    "                                                   'winner','loser','host_wins','Season']]\n",
    "X = data[predictors]\n",
    "y = data['host_wins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Host_HostRank</th>\n",
       "      <th>Host_GameRank</th>\n",
       "      <th>Guest_GuestRank</th>\n",
       "      <th>Guest_GameRank</th>\n",
       "      <th>host_win_count</th>\n",
       "      <th>host_lose_count</th>\n",
       "      <th>guest_win_count</th>\n",
       "      <th>guest_lose_count</th>\n",
       "      <th>game_behind</th>\n",
       "      <th>host_place_streak</th>\n",
       "      <th>guest_place_streak</th>\n",
       "      <th>hq1_avg10</th>\n",
       "      <th>hq2_avg10</th>\n",
       "      <th>hq3_avg10</th>\n",
       "      <th>hq4_avg10</th>\n",
       "      <th>hPace_avg10</th>\n",
       "      <th>heFG%_avg10</th>\n",
       "      <th>hTOV%_avg10</th>\n",
       "      <th>hORB%_avg10</th>\n",
       "      <th>hFT/FGA_avg10</th>\n",
       "      <th>hORtg_avg10</th>\n",
       "      <th>hFG_avg10</th>\n",
       "      <th>hFGA_avg10</th>\n",
       "      <th>hFG%_avg10</th>\n",
       "      <th>h3P_avg10</th>\n",
       "      <th>h3PA_avg10</th>\n",
       "      <th>h3P%_avg10</th>\n",
       "      <th>hFT_avg10</th>\n",
       "      <th>hFTA_avg10</th>\n",
       "      <th>hFT%_avg10</th>\n",
       "      <th>hORB_avg10</th>\n",
       "      <th>hDRB_avg10</th>\n",
       "      <th>hTRB_avg10</th>\n",
       "      <th>hAST_avg10</th>\n",
       "      <th>hSTL_avg10</th>\n",
       "      <th>hBLK_avg10</th>\n",
       "      <th>hTOV_avg10</th>\n",
       "      <th>hPF_avg10</th>\n",
       "      <th>hPTS_avg10</th>\n",
       "      <th>hTS%_avg10</th>\n",
       "      <th>h3PAR_avg10</th>\n",
       "      <th>hFTr_avg10</th>\n",
       "      <th>hDRB%_avg10</th>\n",
       "      <th>hTRB%_avg10</th>\n",
       "      <th>hAST%_avg10</th>\n",
       "      <th>hSTL%_avg10</th>\n",
       "      <th>hBLK%_avg10</th>\n",
       "      <th>hDRtg_avg10</th>\n",
       "      <th>gq1_avg10</th>\n",
       "      <th>gq2_avg10</th>\n",
       "      <th>gq3_avg10</th>\n",
       "      <th>gq4_avg10</th>\n",
       "      <th>gPace_avg10</th>\n",
       "      <th>geFG%_avg10</th>\n",
       "      <th>gTOV%_avg10</th>\n",
       "      <th>gORB%_avg10</th>\n",
       "      <th>gFT/FGA_avg10</th>\n",
       "      <th>gORtg_avg10</th>\n",
       "      <th>gFG_avg10</th>\n",
       "      <th>gFGA_avg10</th>\n",
       "      <th>gFG%_avg10</th>\n",
       "      <th>g3P_avg10</th>\n",
       "      <th>g3PA_avg10</th>\n",
       "      <th>g3P%_avg10</th>\n",
       "      <th>gFT_avg10</th>\n",
       "      <th>gFTA_avg10</th>\n",
       "      <th>gFT%_avg10</th>\n",
       "      <th>gORB_avg10</th>\n",
       "      <th>gDRB_avg10</th>\n",
       "      <th>gTRB_avg10</th>\n",
       "      <th>gAST_avg10</th>\n",
       "      <th>gSTL_avg10</th>\n",
       "      <th>gBLK_avg10</th>\n",
       "      <th>gTOV_avg10</th>\n",
       "      <th>gPF_avg10</th>\n",
       "      <th>gPTS_avg10</th>\n",
       "      <th>gTS%_avg10</th>\n",
       "      <th>g3PAR_avg10</th>\n",
       "      <th>gFTr_avg10</th>\n",
       "      <th>gDRB%_avg10</th>\n",
       "      <th>gTRB%_avg10</th>\n",
       "      <th>gAST%_avg10</th>\n",
       "      <th>gSTL%_avg10</th>\n",
       "      <th>gBLK%_avg10</th>\n",
       "      <th>gDRtg_avg10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>25.5</td>\n",
       "      <td>24.2</td>\n",
       "      <td>23.1</td>\n",
       "      <td>93.57</td>\n",
       "      <td>0.5059</td>\n",
       "      <td>13.28</td>\n",
       "      <td>32.11</td>\n",
       "      <td>0.2379</td>\n",
       "      <td>109.54</td>\n",
       "      <td>37.3</td>\n",
       "      <td>81.8</td>\n",
       "      <td>0.4549</td>\n",
       "      <td>8.4</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.3560</td>\n",
       "      <td>19.4</td>\n",
       "      <td>29.8</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>13.9</td>\n",
       "      <td>34.3</td>\n",
       "      <td>48.2</td>\n",
       "      <td>23.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>17.2</td>\n",
       "      <td>102.4</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.3681</td>\n",
       "      <td>74.79</td>\n",
       "      <td>53.76</td>\n",
       "      <td>61.45</td>\n",
       "      <td>8.45</td>\n",
       "      <td>9.09</td>\n",
       "      <td>98.77</td>\n",
       "      <td>23.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.1</td>\n",
       "      <td>24.8</td>\n",
       "      <td>90.35</td>\n",
       "      <td>0.4791</td>\n",
       "      <td>13.96</td>\n",
       "      <td>31.63</td>\n",
       "      <td>0.2237</td>\n",
       "      <td>106.00</td>\n",
       "      <td>36.7</td>\n",
       "      <td>84.3</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>7.4</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.3891</td>\n",
       "      <td>18.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.7611</td>\n",
       "      <td>14.6</td>\n",
       "      <td>29.0</td>\n",
       "      <td>43.6</td>\n",
       "      <td>22.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.2230</td>\n",
       "      <td>0.2981</td>\n",
       "      <td>69.22</td>\n",
       "      <td>50.05</td>\n",
       "      <td>61.57</td>\n",
       "      <td>8.63</td>\n",
       "      <td>10.31</td>\n",
       "      <td>110.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.5</td>\n",
       "      <td>23.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>90.77</td>\n",
       "      <td>0.4756</td>\n",
       "      <td>10.42</td>\n",
       "      <td>23.05</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>103.67</td>\n",
       "      <td>36.7</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>7.2</td>\n",
       "      <td>18.6</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>15.3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.7442</td>\n",
       "      <td>10.5</td>\n",
       "      <td>29.9</td>\n",
       "      <td>40.4</td>\n",
       "      <td>22.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>95.9</td>\n",
       "      <td>0.5111</td>\n",
       "      <td>0.2205</td>\n",
       "      <td>0.2496</td>\n",
       "      <td>74.07</td>\n",
       "      <td>46.68</td>\n",
       "      <td>62.22</td>\n",
       "      <td>8.89</td>\n",
       "      <td>7.93</td>\n",
       "      <td>106.11</td>\n",
       "      <td>22.7</td>\n",
       "      <td>23.9</td>\n",
       "      <td>20.5</td>\n",
       "      <td>23.2</td>\n",
       "      <td>90.89</td>\n",
       "      <td>0.4635</td>\n",
       "      <td>14.95</td>\n",
       "      <td>27.20</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>99.55</td>\n",
       "      <td>33.2</td>\n",
       "      <td>78.6</td>\n",
       "      <td>0.4249</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>17.9</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.7465</td>\n",
       "      <td>11.6</td>\n",
       "      <td>31.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>15.7</td>\n",
       "      <td>19.4</td>\n",
       "      <td>90.3</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.3095</td>\n",
       "      <td>71.46</td>\n",
       "      <td>49.48</td>\n",
       "      <td>59.83</td>\n",
       "      <td>6.48</td>\n",
       "      <td>9.46</td>\n",
       "      <td>107.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>27.4</td>\n",
       "      <td>25.2</td>\n",
       "      <td>95.89</td>\n",
       "      <td>0.5196</td>\n",
       "      <td>13.66</td>\n",
       "      <td>25.80</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>111.92</td>\n",
       "      <td>37.7</td>\n",
       "      <td>82.8</td>\n",
       "      <td>0.4566</td>\n",
       "      <td>10.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>0.3763</td>\n",
       "      <td>21.7</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.7811</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.9</td>\n",
       "      <td>42.9</td>\n",
       "      <td>22.8</td>\n",
       "      <td>8.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>107.4</td>\n",
       "      <td>0.5665</td>\n",
       "      <td>0.3437</td>\n",
       "      <td>0.3414</td>\n",
       "      <td>72.69</td>\n",
       "      <td>50.45</td>\n",
       "      <td>60.73</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.26</td>\n",
       "      <td>105.69</td>\n",
       "      <td>26.7</td>\n",
       "      <td>23.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>28.5</td>\n",
       "      <td>93.53</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>13.95</td>\n",
       "      <td>21.94</td>\n",
       "      <td>0.1998</td>\n",
       "      <td>113.31</td>\n",
       "      <td>41.5</td>\n",
       "      <td>82.9</td>\n",
       "      <td>0.5044</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>0.3828</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>0.7823</td>\n",
       "      <td>8.1</td>\n",
       "      <td>34.3</td>\n",
       "      <td>42.4</td>\n",
       "      <td>25.7</td>\n",
       "      <td>7.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>14.8</td>\n",
       "      <td>17.7</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.5915</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>74.26</td>\n",
       "      <td>50.99</td>\n",
       "      <td>61.82</td>\n",
       "      <td>8.30</td>\n",
       "      <td>6.85</td>\n",
       "      <td>101.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>27.9</td>\n",
       "      <td>22.9</td>\n",
       "      <td>22.4</td>\n",
       "      <td>23.1</td>\n",
       "      <td>87.74</td>\n",
       "      <td>0.4884</td>\n",
       "      <td>12.86</td>\n",
       "      <td>32.49</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>110.14</td>\n",
       "      <td>36.0</td>\n",
       "      <td>82.6</td>\n",
       "      <td>0.4376</td>\n",
       "      <td>8.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>17.1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>13.9</td>\n",
       "      <td>29.2</td>\n",
       "      <td>43.1</td>\n",
       "      <td>22.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>13.6</td>\n",
       "      <td>18.7</td>\n",
       "      <td>97.5</td>\n",
       "      <td>0.5271</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>0.2805</td>\n",
       "      <td>71.36</td>\n",
       "      <td>51.28</td>\n",
       "      <td>62.69</td>\n",
       "      <td>7.45</td>\n",
       "      <td>9.00</td>\n",
       "      <td>107.43</td>\n",
       "      <td>26.5</td>\n",
       "      <td>25.3</td>\n",
       "      <td>24.6</td>\n",
       "      <td>23.4</td>\n",
       "      <td>90.22</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>11.32</td>\n",
       "      <td>23.64</td>\n",
       "      <td>0.1931</td>\n",
       "      <td>110.24</td>\n",
       "      <td>36.4</td>\n",
       "      <td>82.9</td>\n",
       "      <td>0.4412</td>\n",
       "      <td>11.8</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.3879</td>\n",
       "      <td>15.7</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0.7519</td>\n",
       "      <td>10.4</td>\n",
       "      <td>29.7</td>\n",
       "      <td>40.1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>20.1</td>\n",
       "      <td>100.3</td>\n",
       "      <td>0.5473</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>74.23</td>\n",
       "      <td>47.88</td>\n",
       "      <td>52.07</td>\n",
       "      <td>9.31</td>\n",
       "      <td>7.64</td>\n",
       "      <td>109.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>24.6</td>\n",
       "      <td>26.6</td>\n",
       "      <td>23.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>90.18</td>\n",
       "      <td>0.5210</td>\n",
       "      <td>12.99</td>\n",
       "      <td>26.43</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>111.00</td>\n",
       "      <td>37.6</td>\n",
       "      <td>78.3</td>\n",
       "      <td>0.4819</td>\n",
       "      <td>6.1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>18.8</td>\n",
       "      <td>26.3</td>\n",
       "      <td>0.7152</td>\n",
       "      <td>10.6</td>\n",
       "      <td>29.5</td>\n",
       "      <td>40.1</td>\n",
       "      <td>21.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>13.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>100.1</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.1905</td>\n",
       "      <td>0.3389</td>\n",
       "      <td>70.89</td>\n",
       "      <td>49.33</td>\n",
       "      <td>57.26</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.40</td>\n",
       "      <td>105.30</td>\n",
       "      <td>24.4</td>\n",
       "      <td>29.1</td>\n",
       "      <td>23.2</td>\n",
       "      <td>24.4</td>\n",
       "      <td>92.56</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>15.32</td>\n",
       "      <td>30.31</td>\n",
       "      <td>0.2257</td>\n",
       "      <td>109.28</td>\n",
       "      <td>38.5</td>\n",
       "      <td>79.4</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>6.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.7114</td>\n",
       "      <td>11.8</td>\n",
       "      <td>29.3</td>\n",
       "      <td>41.1</td>\n",
       "      <td>21.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>20.4</td>\n",
       "      <td>101.1</td>\n",
       "      <td>0.5605</td>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>68.45</td>\n",
       "      <td>50.40</td>\n",
       "      <td>56.33</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.83</td>\n",
       "      <td>114.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Host_HostRank  Host_GameRank  Guest_GuestRank  Guest_GameRank  \\\n",
       "0             13             21               13              22   \n",
       "1             13             21               13              23   \n",
       "2             12             20               13              22   \n",
       "3             12             20               13              21   \n",
       "4             11             24               16              22   \n",
       "\n",
       "   host_win_count  host_lose_count  guest_win_count  guest_lose_count  \\\n",
       "0               9               11               11                10   \n",
       "1              11                9                7                15   \n",
       "2               9               10               17                 4   \n",
       "3              11                8               15                 5   \n",
       "4               7               16               10                11   \n",
       "\n",
       "   game_behind  host_place_streak  guest_place_streak  hq1_avg10  hq2_avg10  \\\n",
       "0         -1.5                  1                   1       29.6       25.5   \n",
       "1          5.0                  1                   2       23.5       23.6   \n",
       "2         -7.0                  2                   2       24.0       30.8   \n",
       "3         -3.5                  4                   1       27.9       22.9   \n",
       "4         -4.0                  1                   4       24.6       26.6   \n",
       "\n",
       "   hq3_avg10  hq4_avg10  hPace_avg10  heFG%_avg10  hTOV%_avg10  hORB%_avg10  \\\n",
       "0       24.2       23.1        93.57       0.5059        13.28        32.11   \n",
       "1       24.5       22.6        90.77       0.4756        10.42        23.05   \n",
       "2       27.4       25.2        95.89       0.5196        13.66        25.80   \n",
       "3       22.4       23.1        87.74       0.4884        12.86        32.49   \n",
       "4       23.3       25.6        90.18       0.5210        12.99        26.43   \n",
       "\n",
       "   hFT/FGA_avg10  hORtg_avg10  hFG_avg10  hFGA_avg10  hFG%_avg10  h3P_avg10  \\\n",
       "0         0.2379       109.54       37.3        81.8      0.4549        8.4   \n",
       "1         0.1818       103.67       36.7        85.0      0.4328        7.2   \n",
       "2         0.2645       111.92       37.7        82.8      0.4566       10.3   \n",
       "3         0.2083       110.14       36.0        82.6      0.4376        8.4   \n",
       "4         0.2430       111.00       37.6        78.3      0.4819        6.1   \n",
       "\n",
       "   h3PA_avg10  h3P%_avg10  hFT_avg10  hFTA_avg10  hFT%_avg10  hORB_avg10  \\\n",
       "0        23.5      0.3560       19.4        29.8      0.6611        13.9   \n",
       "1        18.6      0.3796       15.3        21.0      0.7442        10.5   \n",
       "2        28.3      0.3763       21.7        27.9      0.7811        11.0   \n",
       "3        24.1      0.3473       17.1        23.0      0.7380        13.9   \n",
       "4        14.9      0.4079       18.8        26.3      0.7152        10.6   \n",
       "\n",
       "   hDRB_avg10  hTRB_avg10  hAST_avg10  hSTL_avg10  hBLK_avg10  hTOV_avg10  \\\n",
       "0        34.3        48.2        23.1         7.9         6.0        14.5   \n",
       "1        29.9        40.4        22.8         8.3         5.0        11.0   \n",
       "2        31.9        42.9        22.8         8.4         4.6        15.0   \n",
       "3        29.2        43.1        22.5         6.6         5.8        13.6   \n",
       "4        29.5        40.1        21.5         7.1         4.7        13.5   \n",
       "\n",
       "   hPF_avg10  hPTS_avg10  hTS%_avg10  h3PAR_avg10  hFTr_avg10  hDRB%_avg10  \\\n",
       "0       17.2       102.4      0.5399       0.2880      0.3681        74.79   \n",
       "1       20.4        95.9      0.5111       0.2205      0.2496        74.07   \n",
       "2       19.7       107.4      0.5665       0.3437      0.3414        72.69   \n",
       "3       18.7        97.5      0.5271       0.2908      0.2805        71.36   \n",
       "4       21.0       100.1      0.5585       0.1905      0.3389        70.89   \n",
       "\n",
       "   hTRB%_avg10  hAST%_avg10  hSTL%_avg10  hBLK%_avg10  hDRtg_avg10  gq1_avg10  \\\n",
       "0        53.76        61.45         8.45         9.09        98.77       23.5   \n",
       "1        46.68        62.22         8.89         7.93       106.11       22.7   \n",
       "2        50.45        60.73         8.76         7.26       105.69       26.7   \n",
       "3        51.28        62.69         7.45         9.00       107.43       26.5   \n",
       "4        49.33        57.26         7.84         7.40       105.30       24.4   \n",
       "\n",
       "   gq2_avg10  gq3_avg10  gq4_avg10  gPace_avg10  geFG%_avg10  gTOV%_avg10  \\\n",
       "0       22.0       25.1       24.8        90.35       0.4791        13.96   \n",
       "1       23.9       20.5       23.2        90.89       0.4635        14.95   \n",
       "2       23.2       27.3       28.5        93.53       0.5585        13.95   \n",
       "3       25.3       24.6       23.4        90.22       0.5121        11.32   \n",
       "4       29.1       23.2       24.4        92.56       0.5267        15.32   \n",
       "\n",
       "   gORB%_avg10  gFT/FGA_avg10  gORtg_avg10  gFG_avg10  gFGA_avg10  gFG%_avg10  \\\n",
       "0        31.63         0.2237       106.00       36.7        84.3      0.4352   \n",
       "1        27.20         0.2308        99.55       33.2        78.6      0.4249   \n",
       "2        21.94         0.1998       113.31       41.5        82.9      0.5044   \n",
       "3        23.64         0.1931       110.24       36.4        82.9      0.4412   \n",
       "4        30.31         0.2257       109.28       38.5        79.4      0.4870   \n",
       "\n",
       "   g3P_avg10  g3PA_avg10  g3P%_avg10  gFT_avg10  gFTA_avg10  gFT%_avg10  \\\n",
       "0        7.4        18.9      0.3891       18.2        24.2      0.7611   \n",
       "1        6.0        16.9      0.3673       17.9        24.0      0.7465   \n",
       "2        9.0        22.8      0.3828       16.0        20.2      0.7823   \n",
       "3       11.8        30.1      0.3879       15.7        20.8      0.7519   \n",
       "4        6.3        17.3      0.3612       17.8        25.0      0.7114   \n",
       "\n",
       "   gORB_avg10  gDRB_avg10  gTRB_avg10  gAST_avg10  gSTL_avg10  gBLK_avg10  \\\n",
       "0        14.6        29.0        43.6        22.4         8.0         7.1   \n",
       "1        11.6        31.4        43.0        20.0         5.9         5.8   \n",
       "2         8.1        34.3        42.4        25.7         7.8         4.9   \n",
       "3        10.4        29.7        40.1        19.0         8.5         4.7   \n",
       "4        11.8        29.3        41.1        21.7         7.1         5.0   \n",
       "\n",
       "   gTOV_avg10  gPF_avg10  gPTS_avg10  gTS%_avg10  g3PAR_avg10  gFTr_avg10  \\\n",
       "0        15.4       21.0        99.0      0.5206       0.2230      0.2981   \n",
       "1        15.7       19.4        90.3      0.5077       0.2144      0.3095   \n",
       "2        14.8       17.7       108.0      0.5915       0.2743      0.2518   \n",
       "3        11.7       20.1       100.3      0.5473       0.3595      0.2544   \n",
       "4        16.4       20.4       101.1      0.5605       0.2173      0.3177   \n",
       "\n",
       "   gDRB%_avg10  gTRB%_avg10  gAST%_avg10  gSTL%_avg10  gBLK%_avg10  \\\n",
       "0        69.22        50.05        61.57         8.63        10.31   \n",
       "1        71.46        49.48        59.83         6.48         9.46   \n",
       "2        74.26        50.99        61.82         8.30         6.85   \n",
       "3        74.23        47.88        52.07         9.31         7.64   \n",
       "4        68.45        50.40        56.33         7.67         7.83   \n",
       "\n",
       "   gDRtg_avg10  \n",
       "0       110.87  \n",
       "1       107.91  \n",
       "2       101.41  \n",
       "3       109.24  \n",
       "4       114.86  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check to make sure we don't have any categorical predictors:\n",
    "#if we don't have any categorical predictors, then we DON'T have to dummify\n",
    "#(remember, it's normally fine to have a categorical target variable)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP THREE: Train/test split and StandardScaler( ).  In this case, instead of using train_test_split, we're going to use the most recent season as our testing data (2016 data), and previous seasons as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create your training and testing sets\n",
    "X_train = X[data['Season'].isin([2013,2014,2015])]  #or, you could use X[data['Season]<2016]\n",
    "X_test = X[data['Season'] == 2016]\n",
    "\n",
    "y_train = y[data['Season'].isin([2013,2014,2015])]\n",
    "y_test = y[data['Season'] == 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardise your predictor matrices\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_ss = ss.transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP FOUR: Use cross-validation to optimise the hyperparameters for your model. You might try different types of models at this stage as well, and you might use GridSearchCV (or any of the other CVs like RidgeCV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can fit a default `KNeighborsClassifier` to predict 'host_wins'.\n",
    "\n",
    "We can use cross-validation with our training data to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59066427, 0.57989228, 0.58527828, 0.57553957, 0.61690647])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up a default KNN model and cross-validate it on the training data\n",
    "#use 5 cross-validation folds\n",
    "knn = KNeighborsClassifier()\n",
    "cross_val_score(knn, X_train_ss, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validated accuracy for default knn: 0.589656174521783\n",
      "Baseline accuracy: 1    0.594214\n",
      "0    0.405786\n",
      "Name: host_wins, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#find the mean for your cross_val_scores\n",
    "knn_cv_accuracy = cross_val_score(knn, X_train_ss, y_train, cv=5).mean()\n",
    "print('Mean cross-validated accuracy for default knn:',knn_cv_accuracy)\n",
    "print('Baseline accuracy:',baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our default KNN performs quite poorly on the test data. But what if we changed the number of neighbors? The weighting? The distance metric?\n",
    "\n",
    "These are all hyperparameters of the KNN. How would we do this manually? We would need to evaluate on the training data the set of hyperparameters that perform best, and then use this set of hyperparameters to fit the final model and score on the testing set.\n",
    "\n",
    "#### Gridsearch pseudocode for our KNN\n",
    "\n",
    "```python\n",
    "accuracies = {}\n",
    "for k in neighbors_to_test:\n",
    "    for w in weightings_to_test:\n",
    "        for d in distance_metrics_to_test:\n",
    "            hyperparam_set = (k, w, d)\n",
    "            knn = KNeighborsClassifier(n_neighbors=n, weights=w, metric=d)\n",
    "            cv_accuracies = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "            accuracies[hyperparam_set] = np.mean(cv_accuracies)\n",
    "```\n",
    "\n",
    "In the pseudocode above, we would find the key in the dictionary (a hyperparameter set) that has the larget value (mean cross-validated accuracy).\n",
    "\n",
    "\n",
    "\n",
    "#### Using `GridSearchCV`\n",
    "\n",
    "This would be an annoying process to have to do manually. Luckily sklearn comes with a convenience class for performing gridsearch:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "\n",
    "The `GridSearchCV` has a handful of important arguments:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`estimator`** | The sklearn instance of the model to fit on |\n",
    "| **`param_grid`** | A dictionary where keys are hyperparameters for the model and values are lists of values to test |\n",
    "| **`cv`** | The number of internal cross-validation folds to run for each set of hyperparameters |\n",
    "| **`n_jobs`** | How many cores to use on your computer to run the folds (-1 means use all cores) |\n",
    "| **`verbose`** | How much output to display (0 is none, 1 is limited, 2 is printouts for every internal fit) |\n",
    "\n",
    "\n",
    "Below is an example for how one might set up the gridsearch for our KNN:\n",
    "\n",
    "```python\n",
    "knn_parameters = {\n",
    "    'n_neighbors':[1,3,5,7,9],\n",
    "    'weights':['uniform','distance']\n",
    "}\n",
    "\n",
    "knn_gridsearcher = GridSearchCV(KNeighborsClassifier(), knn_parameters, cv=4, verbose=1)\n",
    "knn_gridsearcher.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Try out the sklearn gridsearch below on the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=uniform ................\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=uniform ................\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=uniform ................\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=uniform ................\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=uniform, score=0.5906642728904847, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=uniform ................\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=uniform, score=0.585278276481149, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=distance ...............\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=uniform, score=0.5755395683453237, total=   0.5s\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=uniform, score=0.5798922800718133, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=distance ...............\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=distance ...............\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=distance, score=0.5906642728904847, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=distance ...............\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=uniform, score=0.6169064748201439, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=5, weights=distance ...............\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=distance, score=0.5798922800718133, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=uniform ................\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=distance, score=0.585278276481149, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=uniform ................\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=distance, score=0.6169064748201439, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=uniform ................\n",
      "[CV]  metric=euclidean, n_neighbors=5, weights=distance, score=0.5755395683453237, total=   0.6s\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=uniform, score=0.6229802513464991, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=uniform ................\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=uniform ................\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=uniform, score=0.6050269299820467, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=distance ...............\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=uniform, score=0.5953237410071942, total=   0.5s\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=uniform, score=0.5870736086175943, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=distance ...............\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=uniform, score=0.6294964028776978, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=distance ...............\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=distance ...............\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=distance, score=0.6229802513464991, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=9, weights=distance ...............\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=distance, score=0.5953237410071942, total=   0.5s\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=distance, score=0.6050269299820467, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=distance, score=0.6294964028776978, total=   0.4s\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=uniform ...............\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=9, weights=distance, score=0.5870736086175943, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=15, weights=uniform, score=0.6032315978456014, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=15, weights=uniform, score=0.6157989228007181, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=15, weights=uniform, score=0.5971223021582733, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=15, weights=uniform, score=0.5960502692998204, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=distance ..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   15.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  metric=euclidean, n_neighbors=15, weights=uniform, score=0.6223021582733813, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=15, weights=distance, score=0.5960502692998204, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=15, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=15, weights=distance, score=0.6157989228007181, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=15, weights=distance, score=0.6032315978456014, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=15, weights=distance, score=0.5971223021582733, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=15, weights=distance, score=0.6223021582733813, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=uniform, score=0.6175942549371634, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=uniform, score=0.6122082585278277, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=uniform, score=0.6151079136690647, total=   0.6s\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=uniform, score=0.6157989228007181, total=   0.7s\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=distance ..............\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=uniform, score=0.5845323741007195, total=   0.7s\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=distance, score=0.6122082585278277, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=25, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=distance, score=0.6175942549371634, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=distance, score=0.6140035906642729, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=distance, score=0.5845323741007195, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=25, weights=distance, score=0.6151079136690647, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=uniform, score=0.6175942549371634, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=uniform, score=0.6157989228007181, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=uniform, score=0.6157989228007181, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=uniform, score=0.5827338129496403, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=uniform, score=0.6169064748201439, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=distance, score=0.6193895870736086, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=distance, score=0.6122082585278277, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=distance, score=0.6211849192100538, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=distance, score=0.6205035971223022, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=40, weights=distance, score=0.5881294964028777, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=uniform, score=0.6247755834829444, total=   0.5s\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=uniform, score=0.6086175942549371, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=uniform ...............\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=uniform, score=0.6122082585278277, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=uniform, score=0.591726618705036, total=   0.5s\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=distance, score=0.6211849192100538, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=distance ..............\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=uniform, score=0.6079136690647482, total=   0.5s\n",
      "[CV] metric=euclidean, n_neighbors=50, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=distance, score=0.6175942549371634, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=distance, score=0.599640933572711, total=   0.8s\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=distance, score=0.5953237410071942, total=   0.7s\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=uniform ...............\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=50, weights=distance, score=0.6097122302158273, total=   0.6s\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=uniform, score=0.6229802513464991, total=   0.8s\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=uniform ...............\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=uniform, score=0.6050269299820467, total=   0.7s\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=uniform, score=0.6409335727109515, total=   0.8s\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=uniform, score=0.5881294964028777, total=   0.9s\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=uniform, score=0.6187050359712231, total=   0.7s\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=distance, score=0.6175942549371634, total=   0.7s\n",
      "[CV] metric=euclidean, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=distance, score=0.6122082585278277, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=uniform ................\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=distance, score=0.6463195691202872, total=   0.8s\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=uniform ................\n",
      "[CV]  metric=manhattan, n_neighbors=5, weights=uniform, score=0.6104129263913824, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=uniform ................\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=distance, score=0.5953237410071942, total=   0.6s\n",
      "[CV]  metric=manhattan, n_neighbors=5, weights=uniform, score=0.6157989228007181, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=uniform ................\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=uniform ................\n",
      "[CV]  metric=euclidean, n_neighbors=60, weights=distance, score=0.6115107913669064, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=distance ...............\n",
      "[CV]  metric=manhattan, n_neighbors=5, weights=uniform, score=0.59245960502693, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=distance ...............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  metric=manhattan, n_neighbors=5, weights=uniform, score=0.579136690647482, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=distance ...............\n",
      "[CV]  metric=manhattan, n_neighbors=5, weights=uniform, score=0.6187050359712231, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=distance ...............\n",
      "[CV]  metric=manhattan, n_neighbors=5, weights=distance, score=0.6104129263913824, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=5, weights=distance ...............\n",
      "[CV]  metric=manhattan, n_neighbors=5, weights=distance, score=0.6157989228007181, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=uniform ................\n",
      "[CV]  metric=manhattan, n_neighbors=5, weights=distance, score=0.579136690647482, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=uniform ................\n",
      "[CV]  metric=manhattan, n_neighbors=5, weights=distance, score=0.59245960502693, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=uniform ................\n",
      "[CV]  metric=manhattan, n_neighbors=5, weights=distance, score=0.6187050359712231, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=uniform ................\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=uniform, score=0.6211849192100538, total=   0.8s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=uniform ................\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=uniform, score=0.6014362657091562, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=distance ...............\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=uniform, score=0.6050269299820467, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=distance ...............\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=uniform, score=0.5665467625899281, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=distance ...............\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=uniform, score=0.5989208633093526, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=distance ...............\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=distance, score=0.6211849192100538, total=   0.8s\n",
      "[CV] metric=manhattan, n_neighbors=9, weights=distance ...............\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=distance, score=0.6014362657091562, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=distance, score=0.6050269299820467, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=distance, score=0.5665467625899281, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=9, weights=distance, score=0.5989208633093526, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=uniform, score=0.6229802513464991, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=uniform, score=0.5942549371633752, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=uniform, score=0.599640933572711, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=uniform, score=0.5971223021582733, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=uniform, score=0.6241007194244604, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=distance, score=0.6229802513464991, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=15, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=distance, score=0.5906642728904847, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=distance, score=0.599640933572711, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=distance, score=0.5971223021582733, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=15, weights=distance, score=0.6241007194244604, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=uniform, score=0.6373429084380611, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=uniform, score=0.5960502692998204, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=uniform, score=0.6157989228007181, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=uniform, score=0.5935251798561151, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=uniform, score=0.6061151079136691, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=distance, score=0.6373429084380611, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=25, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=distance, score=0.5942549371633752, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=distance, score=0.6140035906642729, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=distance, score=0.5953237410071942, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=25, weights=distance, score=0.6061151079136691, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=uniform, score=0.63016157989228, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=uniform, score=0.59245960502693, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=uniform, score=0.6247755834829444, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=uniform, score=0.6151079136690647, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=uniform, score=0.6492805755395683, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=distance, score=0.6337522441651705, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=40, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=distance, score=0.6373429084380611, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=distance, score=0.5906642728904847, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=distance, score=0.6079136690647482, total=   0.6s\n",
      "[CV]  metric=manhattan, n_neighbors=40, weights=distance, score=0.64568345323741, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=uniform ...............\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=uniform ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  metric=manhattan, n_neighbors=50, weights=uniform, score=0.6373429084380611, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=50, weights=uniform, score=0.6104129263913824, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=50, weights=uniform, score=0.6319569120287253, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=50, weights=uniform, score=0.6115107913669064, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=50, weights=uniform, score=0.6420863309352518, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=50, weights=distance, score=0.6355475763016158, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=50, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=50, weights=distance, score=0.6050269299820467, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=50, weights=distance, score=0.6337522441651705, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=50, weights=distance, score=0.6402877697841727, total=   0.4s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=50, weights=distance, score=0.6079136690647482, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=uniform, score=0.6355475763016158, total=   0.5s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=uniform ...............\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=uniform, score=0.6122082585278277, total=   0.4s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=uniform, score=0.5971223021582733, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=uniform, score=0.6229802513464991, total=   0.6s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=uniform, score=0.6492805755395683, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=distance, score=0.6373429084380611, total=   0.7s\n",
      "[CV] metric=manhattan, n_neighbors=60, weights=distance ..............\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=distance, score=0.6319569120287253, total=   0.7s\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=distance, score=0.6068222621184919, total=   0.7s\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=distance, score=0.5935251798561151, total=   0.7s\n",
      "[CV]  metric=manhattan, n_neighbors=60, weights=distance, score=0.6438848920863309, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [5, 9, 15, 25, 40, 50, 60], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors': [5,9,15,25,40,50,60],\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']}\n",
    "\n",
    "knn_gridsearch = GridSearchCV(KNeighborsClassifier(), \n",
    "                              knn_params, \n",
    "                              n_jobs=-1, cv=5, verbose=1)\n",
    "\n",
    "knn_gridsearch.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  We've fit our GridSearch.\n",
    "\n",
    "#### Examing the results of GridSearch( )\n",
    "\n",
    "Once the gridsearch has fit (this can take awhile!) we can pull out a variety of information and useful objects from the gridsearch object, stored as attributes:\n",
    "\n",
    "| Property | Use |\n",
    "| --- | ---|\n",
    "| **`results.param_grid`** | Displays parameters searched over. |\n",
    "| **`results.best_score_`** | Best mean cross-validated score achieved. |\n",
    "| **`results.best_estimator_`** | Reference to model with best score.  Is usable / callable. |\n",
    "| **`results.best_params_`** | The parameters that have been found to perform with the best score. |\n",
    "| **`results.grid_scores_`** | Display score attributes with corresponding parameters. | \n",
    "\n",
    "**Print out the best score found in the search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6266618756737333"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out the best mean cross-validated accuracy from the gridsearch\n",
    "#hopefully this should be much better than our default mean cross-validated accuracy \n",
    "knn_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validated accuracy for default knn: 0.589656174521783\n",
      "Baseline accuracy: 1    0.594214\n",
      "0    0.405786\n",
      "Name: host_wins, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Mean cross-validated accuracy for default knn:',knn_cv_accuracy)\n",
    "print('Baseline accuracy:',baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the set of hyperparameters that achieved the best score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 50, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out your best hyperparameters\n",
    "knn_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = knn_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_estimator_type',\n",
       " '_fit',\n",
       " '_fit_X',\n",
       " '_fit_method',\n",
       " '_get_param_names',\n",
       " '_init_params',\n",
       " '_pairwise',\n",
       " '_tree',\n",
       " '_y',\n",
       " 'algorithm',\n",
       " 'classes_',\n",
       " 'effective_metric_',\n",
       " 'effective_metric_params_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'kneighbors',\n",
       " 'kneighbors_graph',\n",
       " 'leaf_size',\n",
       " 'metric',\n",
       " 'metric_params',\n",
       " 'n_jobs',\n",
       " 'n_neighbors',\n",
       " 'outputs_2d_',\n",
       " 'p',\n",
       " 'predict',\n",
       " 'predict_proba',\n",
       " 'radius',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'weights']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP FIVE: Once you're happy with your hyperparameters, fit your model on your whole training data and test it on your whole testing data.  (When you use a gridsearch's `.best_estimator_`, it will already have fit a model with the best hyperparameters on your training data, so all you have to do is score it on your testing data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign the best fit model (`best_estimator_`) to a variable and score it on the test data.**\n",
    "\n",
    "Compare this model to the baseline accuracy and your default KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6467005076142132"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assign your best_estimator_ to the variable, then use .score( ) on your testing data\n",
    "best_knn = knn_gridsearch.best_estimator_\n",
    "best_knn.score(X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP SIX: Then you might want to evaluate the performance of the model (R2 score, accuracy, confusion matrix, etc); find the actual predictions that your model is providing and store them in a dataframe; plot your predictions against your actual target variable to visualise how well your model performed; investigate feature importance with .coef_ if you have a parametric model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_home_win</th>\n",
       "      <th>predicted_home_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True_home_win</th>\n",
       "      <td>501</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_home_loss</th>\n",
       "      <td>255</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted_home_win  predicted_home_loss\n",
       "True_home_win                  501                   93\n",
       "True_home_loss                 255                  136"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there's lots of stuff you can do to follow up and investigate when you've found your best model,\n",
    "#but let's just look at some different ways to assess a classifier now using what you learned yesterday:\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = best_knn.predict(X_test_ss)\n",
    "confusion = confusion_matrix(y_test,predictions,labels=[1,0])\n",
    "pd.DataFrame(confusion, \n",
    "             columns=['predicted_home_win','predicted_home_loss'], \n",
    "             index=['True_home_win','True_home_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8434343434343434"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall is concerned with how much you catch all the positives:\n",
    "#it's more important with cancer detection tests, for example: \n",
    "#     \"Let's make sure we RECALL all the patients with positives to do further testing!\" \n",
    "#     \"Yes!  But let's make sure we're SENSITIVE about how we tell them they might have cancer.\"\n",
    "#it's the true positives, divided by all the actual positives\n",
    "501/(501+93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8434343434343434"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check that:\n",
    "recall_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626984126984127"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision is concerned with how precisely you predict positives; in other words,\n",
    "#when you predict a positive, are you pretty sure about that prediction?\n",
    "#    \"Whhaaattttt. An email from the Home Office went to my spam filter?!\n",
    "#    \"Spam filters are great, but they damn sure better be PRECISE about what emails they think are spam!\"\n",
    "#it's the true positives, divided by all the predicted positives\n",
    "501/(501+255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626984126984127"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check that:\n",
    "precision_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7422222222222222"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the F1 score is the harmonic mean between these two metrics:\n",
    "f1_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkout (in pair):** How can you modify the recall or the precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_proba = best_knn.predict_proba(X_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = [int(p[1]>.3) for p in predictions_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983164983164983"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test,new_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6063394683026585"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test,new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practice'></a>\n",
    "\n",
    "## Independent practice: gridsearch regularization penalties with logistic regression\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression models can also apply the Lasso and Ridge penalties. The `LogisticRegression` class takes these regularization-relevant hyperparameters:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`penalty`** | `'l1'` for Lasso, `'l2'` for Ridge |\n",
    "| **`solver`** | Must be set to `'liblinear'` for the Lasso penalty to work. |\n",
    "| **`C`** | The regularization strength. Equivalent to `1./alpha` |\n",
    "\n",
    "**You should:**\n",
    "1. Fit and validate the accuracy of a default logistic regression on the basketball data.\n",
    "- Perform a gridsearch over different regularization strengths and Lasso and Ridge penalties.\n",
    "- Compare the accuracy on the test set of your optimized logistic regression to the baseline accuracy and the default model.\n",
    "- Look at the best parameters found. What was chosen? What does this suggest about our data?\n",
    "- Look at the coefficients and associated predictors for your optimized model. What appears to be the most important predictors of winning the game?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64452424, 0.62118492, 0.68402154, 0.64028777, 0.65827338])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up a logistic regression model, \n",
    "#and find its cross-validated scores for your training data\n",
    "#you should get accuracies higher than 0.6, which is better than KNN!\n",
    "lr = LogisticRegression()\n",
    "cross_val_score(lr, X_train_ss, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.649658370251734"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the mean of those cross-validated scores:\n",
    "cross_val_score(lr, X_train_ss, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters. \n",
    "# Use a list with 'l1' and 'l2' for the penalties,\n",
    "# Use a list with 'liblinear' for the solver,\n",
    "# Use a logspace from -3 to 0, with 50 different values\n",
    "\n",
    "# fill the dictionary of parameters\n",
    "gs_params = {'penalty':['l1','l2'],\n",
    "             'solver':['liblinear'],\n",
    "             'C':np.logspace(-3,0,50)}\n",
    "\n",
    "#create your gridsearch object\n",
    "lr_gridsearch = GridSearchCV(LogisticRegression(), \n",
    "                             gs_params, \n",
    "                             n_jobs=-1, cv=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "#fit your gridsearch object on your training data\n",
    "lr_gridsearch.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best mean cross-validated score that your gridsearch found:\n",
    "# (this should be better than the mean cross-validated score for your default logistic regression above)\n",
    "lr_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best hyperparameters that your gridsearch found:\n",
    "lr_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the best estimator to a variable:\n",
    "best_lr = lr_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score your best estimator on the testing data:\n",
    "best_lr.score(X_test_ss, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's analyse the features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to look at the coefficients\n",
    "coef_df = pd.DataFrame({'coef': best_lr.coef_[0],\n",
    "                        'feature': X.columns,\n",
    "                        'abs_coef': np.abs(best_lr.coef_[0])})\n",
    "\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by absolute value of coefficient (magnitude)\n",
    "coef_df.sort_values('abs_coef', ascending=False, inplace=True)\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## KEY TAKEAWAYS:\n",
    "\n",
    "- You always want to use your training data to search for your best hyperparameters! You can do this with GridSearchCV, or with other sklearn objects like RidgeCV, LassoCV, ElasticNetCV, or LogisticRegressionCV.  \n",
    "\n",
    "\n",
    "- You instantiate GridSearchCV with:\n",
    "    - a model\n",
    "    - a dictionary for that model's parameters\n",
    "    - the number of cross-validation folds you want it to perform (`cv=`)\n",
    "    - how many cores to use on your computer for this job (`n_jobs=`)\n",
    "    - whether you want your model to give you some print-outs as it works (`verbose=`)\n",
    "    \n",
    "\n",
    "- Once you've instantiated the GridSearch object, you can fit it on your training data\n",
    "\n",
    "\n",
    "- Once it's finished searching, you can access some useful attributes:\n",
    "    - `.best_score_`, to find the mean cross-validated score of the best estimator\n",
    "    - `.best_params_`, to find the best hyperparameters \n",
    "    - `.best_estimator_`, which you will assign to a variable in order to use `.score()`, `.predict()`, `.coef_`, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PROGRESS CHECKPOINT:\n",
    "\n",
    "- EVERYONE should be able to:\n",
    "    - give an example of a hyperparameter\n",
    "    - explain the basic intuition behind how GridSearch works\n",
    "    - copy and paste the GridSearching from this lesson and adapt it to a different lab to find the best hyperparameters for a model\n",
    "\n",
    "\n",
    "\n",
    "- MANY of you should be able to:\n",
    "    -  use your best estimator from the gridsearch to evaluate model effectiveness and investigate feature importance\n",
    "    -  come up with reasonable parameter dictionaries by yourself for KNN and Logistic Regression\n",
    "    -  create a confusion matrix using your predictions from your best estimator, and understand how to interpret it\n",
    "\n",
    "\n",
    "- SOME of you should be able to:\n",
    "    - look at the documentation for GridSearchCV on sklearn's website, and investigate the different options that are available (for example, the attribute `.cv_results_`)\n",
    "    - create a ROC curve and a Precision-Recall curve for your predictions, and be able to interpret them\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
