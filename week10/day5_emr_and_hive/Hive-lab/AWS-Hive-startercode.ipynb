{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# HIVE Lab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the past labs we have introduced Hadoop and performed more and more complex map-reduce jobs using this tool.\n",
    "\n",
    "It would be nice however to be able to use the familiar SQL syntax we have learned using relational databases when dealing with Hadoop. Luckily, the Hadoop ecosystem contains different sub-projects (tools) such as Sqoop, Pig, and Hive that are used to help Hadoop modules and offer that functionality. In particular:\n",
    "\n",
    "- _Sqoop_ is used to import and export data to and from and between HDFS and RDBMS.\n",
    "- _Pig_ is a procedural language platform used to develop a script for MapReduce operations.\n",
    "- _Hive_ is a platform used to develop SQL type scripts to do MapReduce operations.\n",
    "\n",
    "In this lab we will focus on **Hive**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive\n",
    "Hive enables analysis of large data sets using a language very similar to standard ANSI SQL. This means anyone who can write SQL queries can access data stored on the Hadoop cluster. Hive offers a simple interface for:\n",
    "\n",
    "- Log processing\n",
    "- Text mining\n",
    "- Document indexing\n",
    "- Customer-facing business intelligence (e.g., Google Analytics)\n",
    "- Predictive modeling, hypothesis testing\n",
    "\n",
    "Let's start hive by typing hive to our remote machine prompt.\n",
    "\n",
    "\n",
    "You should see a prompt like this:\n",
    "\n",
    "```bash\n",
    "hive>\n",
    "```\n",
    "\n",
    "The `SHOW TABLES;` command displays the tables contained:\n",
    "\n",
    "```bash\n",
    "hive> SHOW TABLES;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you remember the equivalent postgres command?**\n",
    "\n",
    "Let's create a table called gutenberg where we'll store the word counts for the project_gutenberg documents.\n",
    "\n",
    "```SQL\n",
    "CREATE EXTERNAL TABLE gutenberg (\n",
    "    word STRING,\n",
    "    count INT\n",
    ")\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\n",
    "LOCATION '/user/hadoop/output_gutenberg';\n",
    "```\n",
    "\n",
    "We have just created a table called gutenberg that references the output folder of the project_gutenberg hadoop map reduce job we've executed in the past hours.\n",
    "\n",
    "Go back to the file browser to check what the content of that folder is:\n",
    "\n",
    "```SQL\n",
    "hadoop fs -cat output_gutenberg/part*\n",
    "```\n",
    "\n",
    "Now that we have defined the table in Hive, we can query it using a SQL-like statement:\n",
    "\n",
    "```SQL\n",
    "hive> SELECT * FROM gutenberg ORDER BY count DESC LIMIT 10;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see, this starts a Map reduce job on the output files and should return something like this:\n",
    "\n",
    "```bash\n",
    "Total MapReduce CPU Time Spent: 4 seconds 460 msec\n",
    "OK\n",
    "the 63656\n",
    "of  34367\n",
    "and 32787\n",
    "to  31399\n",
    "a   24811\n",
    "in  18168\n",
    "I   18070\n",
    "his 13485\n",
    "he  13299\n",
    "was 13029\n",
    "Time taken: 37.311 seconds, Fetched: 10 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count in Hive\n",
    "\n",
    "Let's go ahead and perform the word count for one of the books in project Gutenberg using Hive.\n",
    "\n",
    "### 1. Alice in Wonderland word count\n",
    "\n",
    "Let's start by counting the words of Alice in Wonderland (pg11.txt).\n",
    "\n",
    "- create a table called alice_text that will map to the text file lines\n",
    "- create a table called alice that counts the words\n",
    "    - hint: you will need to use the `LATERAL VIEW` keywords to parse the text file table\n",
    "\n",
    "### alice table\n",
    "\n",
    "```SQL\n",
    "DROP TABLE IF EXISTS alice_text;\n",
    "\n",
    "CREATE TABLE alice_text(\n",
    "text string\n",
    ") row format delimited fields terminated by '\\n' stored AS textfile;\n",
    "\n",
    "load data local inpath 'project_gutenberg/pg11.txt' overwrite INTO TABLE alice_text;\n",
    "\n",
    "DROP TABLE IF EXISTS alice;\n",
    "\n",
    "CREATE TABLE alice AS\n",
    "SELECT word, COUNT(*) AS cnt FROM alice_text LATERAL VIEW explode(split(text, ' ')) lTable AS word GROUP BY word;\n",
    "```\n",
    "\n",
    "\n",
    "You can use these 3 resources as reference to find the appropriate commands:\n",
    "\n",
    "- https://www.linkedin.com/pulse/word-count-program-using-r-spark-map-reduce-pig-hive-python-sahu\n",
    "- http://www.hadooplessons.info/2014/12/in-this-post-i-am-going-to-discuss-how.html\n",
    "- http://stackoverflow.com/questions/10039949/word-count-program-in-hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Peter Pan word count\n",
    "\n",
    "Repeat the operation creating a new table called peter where you will store the word counts from Peter Pan (pg16.txt).\n",
    "\n",
    "Note that you can get the definition of a table by using the `describe` command:\n",
    "\n",
    "\n",
    "```SQL\n",
    "    hive> describe alice;\n",
    "    hive> describe peter;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### peter table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Joins in Hive \n",
    "\n",
    "The advantage of having a SQL-like interface is that it makes join operations much easier to perform.\n",
    "\n",
    "Find the common words to alice and peter table and sort them by the sum of their total count in decreasing order. Limit the display to the first 20 most common words.\n",
    "\n",
    "The result should look like:\n",
    "\n",
    "|word|alice_count|peter_count|sum|\n",
    "|---|---|---|---|\n",
    "|the|1664|2331|3995|\n",
    "|and|780|1396|2176|\n",
    "|...|...|...|...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Serde example](https://community.hortonworks.com/articles/8313/apache-hive-csv-serde-example.html)\n",
    "- [Logs Page](http://ita.ee.lbl.gov/html/contrib/ClarkNet-HTTP.html)\n",
    "- [Cloudera Twitter example](https://github.com/cloudera/cdh-twitter-example)\n",
    "- [AWS Serde example](http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/emr-gs.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
