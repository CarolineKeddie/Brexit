{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# AWS Elastic Map Reduce\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEARNING OBJECTIVES\n",
    "\n",
    "- Spin up a cluster on AWS\n",
    "- Browse HDFS using Hadoop User Environment\n",
    "- Launch a HIVE Shell and execute HIVE queries on an EMR cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON GUIDE\n",
    "\n",
    "- [Introduction to EMR](#introduction)\n",
    "- [EMR Pricing](#pricing)\n",
    "- [EMR cluster](#guided-practice)\n",
    "    1. [Prerequisites](#prerequisites)\n",
    "    1. [Launching the cluster](#launch)\n",
    "    1. [Prepare sample data and script](#prepare)\n",
    "    1. [Process sample data](#process)\n",
    "    1. [Check results](#check)\n",
    "- [More EMR cluster](#more)\n",
    "- [Configure Web Connection](#configure)\n",
    "    - [Setting access and foxyproxy](#access)\n",
    "- [Hadoop User Environment (HUE)](#hue) \n",
    "- [Example: AWS sample: cloudfront logs](#example)\n",
    "- [Ind-practice](#ind-practice) \n",
    "- [Conclusion](#conclusion)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Intro to EMR \n",
    "\n",
    "In a previous lesson we have discovered two very important AWS services: EC2 and S3. Today we will see how to spin up a computer cluster on Amazon. \n",
    "\n",
    "**What is a cluster?**\n",
    "\n",
    "**What is a typical topology for a Big Data computing cluster?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Elastic MapReduce was introduced in April 2009 to automate _provisioning_ of the Hadoop cluster, running and terminating jobs, and handling data transfer between EC2 (VM) and S3 (Object Storage). It simplifies the management of a Hadoop cluster, making it available to anyone at the click of a button.\n",
    "\n",
    "EMR offers several pre-installed software packages including:\n",
    "\n",
    "- Hadoop\n",
    "- HBase\n",
    "- Pig\n",
    "- Hive\n",
    "- Hue\n",
    "- Spark\n",
    "and many others.\n",
    "\n",
    "EMR also supports spot Instances since 2011. It is recommended to only run the Task Instance Group on spot instances to take advantage of the lower cost while maintaining availability.\n",
    "\n",
    "**Which of these have you already encountered on your local VM?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"pricing\"></a>\n",
    "## EMR Pricing\n",
    "\n",
    "EMR Pricing is based on the type of instances forming the cluster and it's divided in tiers. The pricing adds to the cost of spinning up the instances in EC2.\n",
    "\n",
    "Also, very importantly, costs are calculated in hourly increments, so if we plan to use the cluster for two sessions of half an hour, we should have it up for one hour consecutively instead of spinning it up and down twice.\n",
    "\n",
    "EMR is not included in the AWS free tier that you've used in the previous class, so it's always a good practice to do some price checking before you spin up a cluster.\n",
    "\n",
    "We can use the [AWS cost calculator](https://calculator.s3.amazonaws.com/index.html) to estimate the cost of a  three-node cluster with medium size instances `(m3.xlarge)`. The image below shows the cost for one hour: it's slightly more than one dollar.\n",
    "\n",
    "![](./assets/images/emrcost.png)\n",
    "\n",
    "If we were to keep the cluster alive for a month, that would result in a pretty high price, that's why it's so convenient to spin up and down clusters as they are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## EMR cluster \n",
    "\n",
    "Let's spin up an EMR cluster with Hive and let's use it to perform a simple word count using Hive like we did on the local VM. We will be following the [example provided by Amazon here](http://docs.aws.amazon.com//ElasticMapReduce/latest/ManagementGuide/emr-gs.html).\n",
    "\n",
    "Let's first log-in to AWS and go to the EMR service page:\n",
    "\n",
    "![](./assets/images/emr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prerequisites\"></a>\n",
    "### 1. Prerequisites\n",
    "\n",
    "As a first step we will create 2 folders in an S3 bucket of ours and call them:\n",
    "- input\n",
    "- output\n",
    "\n",
    "**We can do this manually:**\n",
    "\n",
    "![](./assets/images/bucket.png)\n",
    "\n",
    "**Or via the command line:**\n",
    "\n",
    "```bash\n",
    "$ aws s3 ls\n",
    "```\n",
    "\n",
    "```bash\n",
    "$aws s3 mb s3://bucket-name\n",
    "# you can remove it using aws s3 rb s3://bucket-name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"launch\"></a>\n",
    "### 2.  Launch Cluster\n",
    "\n",
    "![](./assets/images/clusterstart.png)\n",
    "\n",
    "**Remember to choose the key pair you have already stored on your computer.**\n",
    "\n",
    "\n",
    "![](./assets/images/clusterstarting.png)\n",
    "\n",
    "**Notice also that like for EC2 we can list the clusters using the Cluster List pane:**\n",
    "\n",
    "![](./assets/images/clusterlist.png)\n",
    "\n",
    "**The cluster will take several minutes to boot completely. Press the circular refresh button in the top right of the console summary (\"Cluster list\") to refresh your view and see if the cluster is ready.**\n",
    "\n",
    "**In the meantime, let's do a couple of review checks:**\n",
    "\n",
    "---\n",
    "\n",
    "**Do you remember what exercise we did with HIVE?**\n",
    "\n",
    "**Do you remember how to connect to an instance on EC2?**\n",
    "\n",
    "**Do you remember which commands we used in AWSCLI?**\n",
    "\n",
    "---\n",
    "**Once the cluster is ready we will see it in green:**\n",
    "\n",
    "![](./assets/images/clusterready.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prepare\"></a>\n",
    "### 3. Prepare sample data and script\n",
    "\n",
    "We will analyse log data in a similar way as we did in the HIVE exercise. The major difference here is that both our data and the computing power are somewhere in the cloud, instead of being on a virtual machine running on our laptop.\n",
    "\n",
    "The sample data is a series of Amazon CloudFront web distribution log files. This lists the time, date, and various information about all users' activities (including the operating system they used to access the server). The data is stored in Amazon S3 at `s3://us-west-2.elasticmapreduce.samples` (make sure the region is your region).\n",
    "Each entry in the CloudFront log files provides details about a single user request in the following format:\n",
    "\n",
    "    2014-07-05 20:00:00 LHR3 4260 10.0.0.15 GET eabcd12345678.cloudfront.net /test-image-1.jpeg 200 - Mozilla/5.0%20(MacOS;%20U;%20Windows%20NT%205.1;%20en-US;%20rv:1.9.0.9)%20Gecko/2009040821%20IE/3.0.9\n",
    "\n",
    "**A sample HIVE script is also provided here:**\n",
    "\n",
    "    s3://us-west-2.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q\n",
    "\n",
    "**The sample Hive script does the following:**\n",
    "\n",
    "- Creates a Hive table named cloudfront_logs.\n",
    "- Reads the CloudFront log files from Amazon S3 using EMRFS and parses the CloudFront log files using the regular expression serializer/deserializer (RegEx SerDe).\n",
    "- Writes the parsed results to the Hive table cloudfront_logs.\n",
    "- Submits a HiveQL query against the data to retrieve the total requests per operating system for a given time frame.\n",
    "- Writes the query results to your Amazon S3 output bucket.\n",
    "\n",
    "**The Hive code that creates the table looks like the following:**\n",
    "\n",
    "```SQL\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS cloudfront_logs ( \n",
    "\tDate Date, \n",
    "\tTime STRING, \n",
    "\tLocation STRING, \n",
    "\tBytes INT, \n",
    "\tRequestIP STRING, \n",
    "\tMethod STRING, \n",
    "\tHost STRING, \n",
    "\tUri STRING, \n",
    "\tStatus INT, \n",
    "\tReferrer STRING, \n",
    "\tOS String, \n",
    "\tBrowser String, \n",
    "\tBrowserVersion String \n",
    ")\n",
    "```\n",
    "\n",
    "**The Hive code that parses the log files using the RegEx SerDe looks like the following:**\n",
    "\n",
    "```SQL\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' \n",
    "WITH SERDEPROPERTIES ( \"input.regex\" = \"^(?!#)([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+([^ ]+)\\\\s+[^\\(]+[\\(]([^\\;]+).*\\%20([^\\/]+)[\\/](.*)$\" ) LOCATION 's3://us-west-2.elasticmapreduce.samples/cloudfront/data/';\n",
    "```\n",
    "\n",
    "**The Hive query looks like the following:**\n",
    "\n",
    "```sql\n",
    "SELECT os, COUNT(*) count FROM cloudfront_logs WHERE date BETWEEN '2014-07-05' AND '2014-08-05' GROUP BY os;\n",
    "```\n",
    "\n",
    "The output file summarises the count of different operating\n",
    "systems people used to access the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"process\"></a>\n",
    "### 4. Process Sample Data\n",
    "\n",
    "Following the instructions [here](http://docs.aws.amazon.com//ElasticMapReduce/latest/ManagementGuide/emr-gs-process-sample-data.html) we can create a new job step based on the hive script by adding a `step` and assigning input, output and script buckets.\n",
    "\n",
    "\n",
    "Once it's ready click on the cluster from the **Cluster list** page and click **Add step**. On **Step Type** select **Hive program**. For Script s3 location input:\n",
    "\n",
    "`s3://eu-west-1.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q`\n",
    "\n",
    "where you must replace eu-west-1 with your region. For Input s3 location input:\n",
    "\n",
    "`s3://eu-west-1.elasticmapreduce.samples`\n",
    "\n",
    "with the same replacement. For output s3 location you should put your s3 bucket (you can use the navigation to find it).\n",
    "\n",
    "In the last box (arguments) place (this allows column names that are the same as reserved words):\n",
    "\n",
    "- hiveconf hive.support.sql11.reserved.keywords=false\n",
    "\n",
    "and select \"Add\".\n",
    "\n",
    "- By selecting the circular refresh button you can see if it has completed (it will only take about one minute).\n",
    "\n",
    "- Go to your s3 bucket to the os_requests folder and there will be a file (probably it has a name like 00000) which you can download with a right-click select and view. It will contain counts of operating systems.\n",
    "\n",
    "\n",
    "![](./assets/images/steppending.png)\n",
    "\n",
    "![](./assets/images/steprunning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"check\"></a>\n",
    "### 5. Check results\n",
    "\n",
    "\n",
    "![](./assets/images/results.png)\n",
    "\n",
    "\n",
    "You can navigate to your S3 bucket and check the results. There should be a new file, with the content:\n",
    "\n",
    "    Android    855\n",
    "    Linux      813\n",
    "    MacOS      852\n",
    "    OSX        799\n",
    "    Windows    883\n",
    "    iOS        794\n",
    "\n",
    "\n",
    "Wonderful! We have just run a HIVE script on EMR!!\n",
    "\n",
    "**We have run a HIVE script by defining a step. Do you think we could simply run hive commands from the HIVE command line?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"more\"></a>\n",
    "## Accessing the EMR cluster \n",
    "\n",
    "Go ahead and SSH to your master node and launch Hive. Then try to query the table you just created (`cloudfront_logs`).\n",
    "\n",
    "To do so, go to your ec2 console. You will see three new ec2 instances. Select the descriptions. You can see that one of the instances is the EMR-master and the other two are the EMR-slaves. \n",
    "\n",
    "Log in to the master using ssh in a similar format as a normal single ec2 instance (even though this is a cluster of three instances). Note that now you have to indicate **hadoop** instead of ec2-user:\n",
    "\n",
    "```bash\n",
    "$ ssh -i your_key_file.pem hadoop@your_public_DNS_for_the_master\n",
    "```\n",
    "\n",
    "Now you are inside the instance and you can type:\n",
    "\n",
    "```bash\n",
    "$ hive\n",
    "```\n",
    "\n",
    "Now you can type Hive-SQL:\n",
    "\n",
    "```SQL\n",
    "SHOW tables;\n",
    "SELECT * FROM cloudfront_logs LIMIT 10;\n",
    "SELECT COUNT(*) FROM cloudfront_logs;\n",
    "```\n",
    "\n",
    "The result of the count is equal to the sum of the different OS systems\n",
    "you had in your text file output of the Hive query earlier. Note that the Hive-SQL operates fairly slowly here given the size of the table (only 5000 rows) but that will scale more sensibly for larger tables. \n",
    "\n",
    "- Try running queries to check if the counts match the output text file that was stored in s3. \n",
    "\n",
    "Note that your normal single ec2 instance wouldn't come with Hive etc since it doesn't require a distributed file system. Even though we are using ssh to access the master ec2, we are actually querying data stored across all three machines in the cluster. This is seamless because of the Hadoop system that is in place and comes installed.\n",
    "\n",
    "\n",
    "Go to \"Security Groups\" under \"Network & Security\" on the left side-bar and add an inbound rule for:\n",
    "\n",
    "SSH | TCP | 22 | Custom | 0.0.0.0/0\n",
    "\n",
    "ensuring you select \"Save\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"configure\"></a>\n",
    "## Configure Web Connection \n",
    "\n",
    "**So far we have learned two ways of running HIVE. Can you list them?**\n",
    "\n",
    "We will now learn about HUE, or Hadoop User Interface, which is a great way to interact with a Hadoop cluster.\n",
    "\n",
    "Before we can do that, we will have to go trough one more step. In fact, the default security settings for EMR are pretty tight and do not allow for external web connections to our cluster. In order to connect with a browser we will have to set up an _ssh tunnel_, i.e. have our browser communicate to the cluster via an encrypted channel. \n",
    "\n",
    "Luckily, Amazon provides us with simple instructions:\n",
    "\n",
    "![](./assets/images/webconnection.png)\n",
    "\n",
    "![](./assets/images/sshtunnel.png)\n",
    "\n",
    "<a name=\"access\"></a>\n",
    "### In order to follow them we first need to complete two steps:\n",
    "\n",
    "#### 1. enable SSH access to our master node. This is done in the Security Groups pane of the EC2 services page.\n",
    "\n",
    "![](./assets/images/securitygroups.png)\n",
    "\n",
    "#### 2. Install and configure Foxy-Proxy as explained [here](https://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/emr-connect-master-node-proxy.html).\n",
    "\n",
    "Once we have enabled SSH access, we can go ahead and connect:\n",
    "\n",
    "```bash\n",
    "ssh -i ~/.ssh/MyFirstKey.pem -ND 8157 hadoop@<YOUR_MASTER_DNS>\n",
    "```\n",
    "\n",
    "Note that this command will not end because it's keeping the tunnel alive.\n",
    "\n",
    "If the tunnel and Foxy-proxy are well configured, we should be able to connect to several web services. The one we are interested in is HUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"hue\"></a>\n",
    "## Hadoop User Environment (HUE)\n",
    "\n",
    "[Hue](http://gethue.com/) aggregates the most common Apache Hadoop components into a single interface and targets the user experience. Its main goal is to have the users \"just use\" Hadoop without worrying about the underlying complexity or using a command line.\n",
    "\n",
    "It's accessible at the port 8888 of our master node through an SSH tunnel. Since it's the first time we use it, we'll have to set up username (choose **hdfs** as username, otherwise you might get problems with access rights) and password. Let's go ahead and do that.\n",
    "\n",
    "\n",
    "    http://<YOUR_MASTER_DNS>.compute.amazonaws.com:8888/\n",
    "\n",
    "\n",
    "![](./assets/images/hueuser.png)\n",
    "\n",
    "**Let's also install all the examples:**\n",
    "\n",
    "![](./assets/images/hueinstall.png)\n",
    "\n",
    "**And we can finally open the Hue home page:**\n",
    "\n",
    "![](./assets/images/huehome.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"example\"></a>\n",
    "### Example: AWS Sample: CloudFront Logs\n",
    "\n",
    "Amongst the examples there's one that looks familiar. It's the cloudfront sample logs script we've just executed in HIVE. Let's see what happens if we run it from HUE. Hit the EXECUTE button.\n",
    "\n",
    "We will see the log of the MR being executed:\n",
    "\n",
    "![](./assets/images/huecloudfront.png)\n",
    "\n",
    "#### And the results:\n",
    "\n",
    "![](./assets/images/huecfresults.png)\n",
    "\n",
    "#### HUE also generates a nice chart for us:\n",
    "\n",
    "![](./assets/images/huechart.png)\n",
    "\n",
    "Note that you can progress to the `next` button to execute the next queries in the script.\n",
    "\n",
    "Finally, note that we can also explore the HDFS like we were doing on the local VM by pointing our browser to the 50070 port (click [here](http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html) for further browser interfaces like yarn):\n",
    "\n",
    "    http://<YOUR_MASTER_DNS>:50070/dfshealth.html#tab-overview\n",
    "    \n",
    "![](./assets/images/hdfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent practice with HUE\n",
    "\n",
    "The HUE Home offers several other examples. In pairs choose an example and work through the code. Make sure you understand what it does and how you execute it. Here are some questions to guide your discovery:\n",
    "\n",
    "- What tables are present?\n",
    "- How are they defined? what's the schema? how do you check it in HUE?\n",
    "- What does the query do?\n",
    "- How long does it take to execute?\n",
    "- How much data does it process?\n",
    "- What are the results?\n",
    "\n",
    "\n",
    "<a name=\"conclusion\"></a>\n",
    "## Conclusions\n",
    "\n",
    "We have learned how to spin up a cluster on AWS and how to run HIVE queries on it using a script or using HUE.\n",
    "\n",
    "**Make sure you terminate your cluster now:**\n",
    "\n",
    "![](./assets/images/terminate.png)\n",
    "\n",
    "**Delete the buckets from S3, to avoid paying for storage space.**\n",
    "\n",
    "![](./assets/images/deletebucket.png)\n",
    "\n",
    "\n",
    "#### Now that you're enabled with the ability to process very large datasets in the cloud, what problems would you like to tackle?\n",
    "\n",
    "\n",
    "<a name=\"resources\"></a>\n",
    "## ADDITIONAL RESOURCES\n",
    "\n",
    "- [AWS EMR tutorial](http://docs.aws.amazon.com//ElasticMapReduce/latest/ManagementGuide/emr-gs.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
