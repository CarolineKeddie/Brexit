{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Neural Networks\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Get a quick overview of neural networks\n",
    "- Build a Multilayer Perceptron Feed-Forward network with sklearn\n",
    "- Compare to other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [What are Neural Networks?](#what-are-neural-networks)\n",
    "- [Pros vs. Cons](#pros-vs-cons)\n",
    "- [Features](#features)\n",
    "- [Outputs](#outputs)\n",
    "- [Hidden Layers](#hidden-layers)\n",
    "- [Activation Function](#activation-function)\n",
    "\t- [ReLU](#relu)\n",
    "\t- [Softmax](#softmax)\n",
    "- [Backpropagation](#backpropagation)\n",
    "- [Epochs and Batch Sizes](#epochs-and-batch-sizes)\n",
    "- [Train a Multilayer Perceptron](#multilayer)\n",
    "- [Additional Resources](#additionl-resources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.set_printoptions(precision=4) \n",
    "    \n",
    "plt.style.use('ggplot')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import Imputer, LabelBinarizer, StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "Neural networks are incredibly powerful and constantly talked about these days -- they've handled tasks such as image classification, [playing Go](http://www.nature.com/news/google-ai-algorithm-masters-ancient-game-of-go-1.19234), and [creating tweets in the style of President Trump](https://twitter.com/deepdrumpf?lang=en) with relatively little effort.\n",
    "\n",
    "\n",
    "Neural networks were first studied in the 1940s (!) as a model of biological neural networks and had various ups and downs in the research mainstream.\n",
    "\n",
    "Currently, this is a rapidly evolving field and represents some of the newest parts of Data Science, thanks to the increase in processing power and scale of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-are-neural-networks\"></a>\n",
    "## What are Neural Networks?\n",
    "---\n",
    "\n",
    "Neural networks, in a single line, attempt to iteratively train a set (or sets) of weights that, when used together, return the most accurate predictions for a set of inputs. Just like many of our past models, the model is trained using a loss function, which our model will attempt to minimize over iterations. Remember that a loss function is some function that takes in our predictions and the actual values and returns some sort of aggregate value that shows how accurate (or not) we were.\n",
    "\n",
    "Neural networks do this by establishing sets of neurons (known as hidden layers) that take in some sort of input(s), apply a weight, and pass that output onward. As we feed more data into the network, it adjusts those weights based on the output of the loss function, until we have highly trained and specific weights.\n",
    "\n",
    "Why does one neuron turn out one way and a second neuron another? That's not generally something we can understand (though attempts have been made, such as Google's Deep Dream). You can understand this as a kind of (very advanced) mathematical optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pros-vs-cons\"></a>\n",
    "## Pros vs. Cons\n",
    "---\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Exceptionally accurate because we can learn complicated decision boundaries\n",
    "- Appropriate for a vast range of techniques\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- Long training time\n",
    "- Requires more data than most algorithms\n",
    "- Can become very complex and hard to interpret\n",
    "- Less user-friendly coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"features\"></a>\n",
    "## Features\n",
    "---\n",
    "\n",
    "Much like our other machine learning techniques, we do need to feed data into the network. While neural networks are pretty good at taking data in any form, it can help the network a lot to reduce the number of inputs when necessary - particularly with image data. A smaller quantity of inputs can often already give as good results as a larger number without much change in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"outputs\"></a>\n",
    "## Outputs\n",
    "---\n",
    "\n",
    "Much like other techniques, we do want some sort of output at the end as well. In most cases:\n",
    "\n",
    "- for a regression style technique, one output is usually fine\n",
    "- for a classification technique, one output per class is a good idea (in other words, we model a one-versus-all approach)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hidden-layers\"></a>\n",
    "## Hidden Layers\n",
    "---\n",
    "\n",
    "What makes neural networks tick is the idea of hidden layers. Hidden does not mean anything particularly devious here, just that it is not the input or the output layer.\n",
    "\n",
    "Hidden layers can have any number of neurons per layer and you can include any number of layers in a neural network. Inputs into a neuron have different weights that are modified across iterations of the model and have a bias term as well -- you can almost imagine them as mini-linear models (though, that linearity does not need to hold at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"activation-function\"></a>\n",
    "## Activation Function\n",
    "---\n",
    "\n",
    "\n",
    "Neurons process the input they receive in a standard way. Each of them first processes the input data in the following way:\n",
    "\n",
    "$$\n",
    "z = b+\\sum_i w_i X_i\n",
    "$$\n",
    "\n",
    "Weights and intercepts are specific to each neuron and have to be determined through an iterative procedure.\n",
    "\n",
    "\n",
    "Once the neuron has formed $z$ it applies a user-defined activation function to it. Some examples are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"relu\"></a>\n",
    "### ReLU\n",
    "\n",
    "Also known as a [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks), this returns 0 if the output is less than 0, otherwise it simply returns the input, i.e., \n",
    "\n",
    "- take the input and feed it through $f(z) = {\\rm max}(0, z)$. \n",
    "\n",
    "This means that the neuron is activated when its output is positive and not activated otherwise.\n",
    "\n",
    "**The ReLu function**\n",
    "![](./assets/images/relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"softmax\"></a>\n",
    "### Softmax\n",
    "\n",
    "The softmax function you know from logistic regression - for two classes it reduces to the sigmoid. It returns values between 0 and 1 as desired for assigning probabilities of falling into any of the given classes ([more information here](https://en.wikipedia.org/wiki/Softmax_function)).\n",
    "\n",
    "There's a wealth of information on different types of activation functions within [this article](https://en.wikipedia.org/wiki/Activation_function) - different activation functions, hidden layers, and neurons per layer can change how effective your neural network will be!\n",
    "\n",
    "\n",
    "Of course there are a whole lot of other activation functions, for example the identity (just returning the input again) or the hyperbolic tangent.\n",
    "\n",
    "One of the advantages of the ReLU is that its slope is constant but non-vanishing on the positive side, whereas functions like the sigmoid become very flat as they asymptote towards 1 or 0 which challenges optimization algorithms like gradient descent (see the [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"backpropagation\"></a>\n",
    "## Backpropagation\n",
    "---\n",
    "\n",
    "While there are many ways that a neural network learns, we'll focus  on the easiest to understand method. Backpropagation is the method to adjust the weights in each hidden layer according to how well the network performed compared to the actual outputs in each iteration step.\n",
    "\n",
    "How do we make good or bad choices within the network? We compare the outputs of the predictions (using the loss function), and make tiny changes to compare the outputs. Most frequently we use a learning rate and a gradient descent method to estimate the changes that our successive models have used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"epochs-and-batch-sizes\"></a>\n",
    "## Epochs and Batch Sizes\n",
    "---\n",
    "\n",
    "- **Epochs:** The number of iterations of full model fitting (i.e., how many times one runs through the fitting process). There's no upper limit, but generally there will be a point where additional epochs do not generate new insights.\n",
    "- **Batch Size:** Neural networks tend to work best when you feed portions of your data in at a time (versus the full set) and adjust weights in between. Smaller batches allow for more frequent updates but may be less consistent in what changes are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"the-perceptron\"></a>\n",
    "### The Perceptron\n",
    "\n",
    "This is the original model oriented on how the neurons in the brain might work.\n",
    "\n",
    "- Each neuron is connected to many other neurons in a network.\n",
    "- These neurons both send and receive signals from connected neurons.\n",
    "- When a neuron receives a signal it can either fire or not, depending on whether the incoming signal is above some threshold.\n",
    "\n",
    "A single perceptron, like a neuron, can be thought of as a decision-making unit. If the weight of the incoming signals is above a threshold, the perceptron fires, and if not it doesn't. In this case firing equals outputting a value of 1 and not firing equals outputting a value of 0.\n",
    "\n",
    "<img src=\"images/ann-perceptron.png\" width=500>\n",
    "\n",
    "The graph shows how inputs are fed with some weights into a neuron. \n",
    "The neuron processes these inputs. It multiplies each input by its weight, sums them up together with a bias and checks if that sum is larger than zero. If it is, it produces a signal, otherwise not.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "b + \\sum_i w_i X_i &>& 0 \\Rightarrow 1 \\\\\n",
    "b + \\sum_i w_i X_i &<& 0 \\Rightarrow 0\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "The activation function used in the case of the perceptron is the Heaviside step function $\\theta(z)$, giving 1 if $z>0$ and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression would work in the same way, only choosing a different activation function, the sigmoid (or the softmax function in the case of more than two classes).\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "z &=& b+\\sum_i w_i X_i\\\\\n",
    "\\sigma(z) &=& \\frac{1}{1+e^{-z}}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "**How would the activation function look like for linear regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"multilayer\"></a>\n",
    "## Train a Multilayer Perceptron\n",
    "---\n",
    "\n",
    "- A feedforward multilayer perceptron is one of the most well known neural network architectures\n",
    "- They are structured just like the picture in the intro\n",
    "    - We have an input layer of features\n",
    "    - These input features are passed into neurons in the hidden layers\n",
    "    - Each neuron is a perceptron, kind of like a bunch of small linear regressions\n",
    "    - We pass information from one layer of neurons to the next layer of neurons until we hit the output layer\n",
    "    - The output layer does one calculation to output a prediction for the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'start with a simple linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(df,y,test_size = 0.3,random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.829609248605056"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train,y_train)\n",
    "metrics.mean_squared_error(y_test,model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836482437943237"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.r2_score(y_test,model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8262  1.4271  0.4085  0.6805 -2.5339  1.9297  0.1034 -3.2349  2.6968\n",
      " -1.9156 -2.1581  0.593  -4.141 ]\n",
      "22.339830508474606\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sklearn's multi-layer perceptron for this regression problem. Let's first reproduce the linear regression result. To do so, we use the identity activation function. The default solver is `adam`, for small datasets `lbfgs` might work better however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.829613095150115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = MLPRegressor(hidden_layer_sizes=1,solver='lbfgs',activation='identity',max_iter=1000,random_state=1)\n",
    "nnet.fit(X_train,y_train)\n",
    "metrics.mean_squared_error(y_test,nnet.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the neural network coefficients (the weights for each edge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.5094],\n",
      "       [-0.8799],\n",
      "       [-0.252 ],\n",
      "       [-0.4195],\n",
      "       [ 1.5624],\n",
      "       [-1.1898],\n",
      "       [-0.0637],\n",
      "       [ 1.9947],\n",
      "       [-1.6629],\n",
      "       [ 1.1813],\n",
      "       [ 1.3307],\n",
      "       [-0.3656],\n",
      "       [ 2.5531]]), array([[-1.6219]])]\n"
     ]
    }
   ],
   "source": [
    "print(nnet.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-9.0868]), array([7.6019])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.intercepts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We multiply the first entries by the second to obtain the linear regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8262  1.4271  0.4086  0.6804 -2.5341  1.9297  0.1032 -3.2352  2.6971\n",
      " -1.9159 -2.1582  0.593  -4.1409]\n"
     ]
    }
   ],
   "source": [
    "print((nnet.coefs_[0]*nnet.coefs_[1]).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get very good agreement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.0056e-05 -6.8109e-06 -1.3393e-04  1.0810e-04  2.3199e-04  3.8637e-05\n",
      "  1.4538e-04  3.3354e-04 -3.0156e-04  3.6917e-04  1.5923e-04 -9.2895e-07\n",
      " -9.8528e-05]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_-(nnet.coefs_[0]*nnet.coefs_[1]).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same for the intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.3398]]\n"
     ]
    }
   ],
   "source": [
    "print(nnet.intercepts_[0]*nnet.coefs_[1]+nnet.intercepts_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.4899e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(model.intercept_ - (nnet.intercepts_[0]*nnet.coefs_[1]+nnet.intercepts_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a few hidden layers and a non-trivial activation function to see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.455161951094174"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = MLPRegressor(hidden_layer_sizes=(10,10,10),solver='lbfgs',activation='relu',random_state=1)\n",
    "nnet.fit(X_train,y_train)\n",
    "metrics.mean_squared_error(y_test,nnet.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more model coefficients now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, 10), (10, 10), (10, 10), (10, 1)]\n",
      "340\n"
     ]
    }
   ],
   "source": [
    "print([coef.shape for coef in nnet.coefs_])\n",
    "print(sum([np.prod(coef.shape) for coef in nnet.coefs_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the full list of the first set of coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.393   0.2272 -0.2243  0.1135 -0.7329 -0.4566 -0.7866 -0.6251  0.0682\n",
      "   0.3956]\n",
      " [-0.0198  0.9524 -0.9206  0.6652 -0.5628 -0.9042 -0.2413  0.8059 -0.8367\n",
      "   0.307 ]\n",
      " [ 0.2966  0.3557 -0.0991 -0.4841 -0.1875  0.5684 -0.4308 -0.3834  0.2487\n",
      "   0.3143]\n",
      " [-0.6005 -0.0409  1.1553 -0.1094 -0.2673 -0.7373 -0.6231 -0.5248 -1.1516\n",
      "   0.8766]\n",
      " [ 0.4853 -0.6792 -0.8762 -0.7296  0.5356  0.4203  0.6987 -1.2268 -0.3795\n",
      "  -0.5007]\n",
      " [-0.912  -0.3973 -0.0096 -0.3797  0.5824 -0.5669  0.926  -0.7645  0.7282\n",
      "  -0.2356]\n",
      " [-0.5224 -0.0702  0.2336  0.164  -1.2548 -0.9444 -0.7949  0.5867  0.4373\n",
      "   0.0828]\n",
      " [ 0.5682 -2.1178 -1.673  -0.4395  0.0121 -0.6657  0.9492 -1.0564 -0.1392\n",
      "   0.4557]\n",
      " [ 0.5237  0.6498 -0.0246 -0.7877 -0.5531  0.6493 -0.9693  1.0615  0.3887\n",
      "   0.7602]\n",
      " [-0.6973 -0.5462 -0.7218  0.8881 -1.0415 -0.4868 -0.2169  0.37   -0.1886\n",
      "   0.4734]\n",
      " [-0.4464 -0.5386  0.9107 -0.7168  0.0676  0.9284 -1.2061  0.6931  0.3981\n",
      "  -0.0392]\n",
      " [-1.646  -1.196   0.7899  0.9793 -0.0071 -0.0659 -0.2709 -0.2232  0.2298\n",
      "   0.8352]\n",
      " [-1.2011 -1.4946  0.1741 -0.8676 -0.5591 -0.9758  0.4791 -0.4795 -0.3779\n",
      "  -1.6101]]\n"
     ]
    }
   ],
   "source": [
    "print(nnet.coefs_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also many intercepts now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10,), (10,), (10,), (1,)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[intercept.shape  for intercept in nnet.intercepts_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the total amount of layers (including input and output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.n_layers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the regression model we have a single output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.n_outputs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'identity'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.out_activation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get predictions and scores (R2) in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.7623, 23.5729, 14.4505, 20.6724, 20.1204, 21.0417, 32.8136,\n",
       "       15.0943, 21.5044, 25.0493])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.predict(X_test)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8531969096488475"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-in-the-titanic-data\"></a>\n",
    "### Load in the titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('assets/datasets/titanic_train.csv')\n",
    "X = data.drop('Survived', axis=1)\n",
    "y = data[['Survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"do-a-bit-of-data-cleaning\"></a>\n",
    "### Do a bit of data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51.8625],\n",
       "       [15.5   ],\n",
       "       [41.5792],\n",
       "       [14.4542],\n",
       "       [10.5167]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a helper class to extract features one by one in a pipeline\n",
    "class FeatureExtractor(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return x[self.column].values.reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "FeatureExtractor('Fare').fit_transform(X_train)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn `LabelBinarizer` does not fit in a pipeline, let's use [this](https://github.com/scikit-learn/scikit-learn/issues/3112) customized version instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class CustomBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None,**fit_params):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return LabelBinarizer().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to binarize labels and impute missing values with an appropriate method\n",
    "pclass_pipe = make_pipeline(\n",
    "    FeatureExtractor('Pclass'),\n",
    "    CustomBinarizer(),\n",
    "    Imputer(strategy='most_frequent'),\n",
    "    StandardScaler()\n",
    ")\n",
    "sex_pipe = make_pipeline(\n",
    "    FeatureExtractor('Sex'),\n",
    "    CustomBinarizer(),\n",
    "    Imputer(strategy='most_frequent'),\n",
    "    StandardScaler()\n",
    ")\n",
    "age_pipe = make_pipeline(\n",
    "    FeatureExtractor('Age'),\n",
    "    Imputer(strategy='mean'),\n",
    "    StandardScaler()\n",
    ")\n",
    "sibsp_pipe = make_pipeline(\n",
    "    FeatureExtractor('SibSp'),\n",
    "    Imputer(strategy='most_frequent'),\n",
    "    StandardScaler()\n",
    ")\n",
    "parch_pipe = make_pipeline(\n",
    "    FeatureExtractor('Parch'),\n",
    "    Imputer(strategy='most_frequent'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "fu = make_union(pclass_pipe, sex_pipe, \n",
    "                age_pipe, sibsp_pipe, parch_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X, y, train, and test\n",
    "def multi_binarizer(data):\n",
    "    data = data.copy()\n",
    "    data['Survived Class 0'] = data['Survived'].apply(lambda x: 1 if x == 0 else 0)\n",
    "    return data[['Survived Class 0', 'Survived']].values\n",
    "\n",
    "train_X = fu.fit_transform(X_train)\n",
    "train_Y = multi_binarizer(y_train)\n",
    "test_X = fu.transform(X_test)\n",
    "ttest_Y = multi_binarizer(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((596, 7), (295, 7))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7627118644067796\n",
      "0.8271186440677966\n",
      "0.8338983050847457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = Perceptron(tol=10**(-6))\n",
    "model.fit(train_X,train_Y[:,0])\n",
    "print(model.score(test_X,ttest_Y[:,0]))\n",
    "\n",
    "model = LogisticRegression(fit_intercept=True,C=10**10,solver='lbfgs',random_state=1)\n",
    "model.fit(train_X,train_Y[:,0])\n",
    "print(model.score(test_X,ttest_Y[:,0]))\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators=500)\n",
    "model_rf.fit(train_X,train_Y[:,0])\n",
    "print(model_rf.score(test_X,ttest_Y[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reproduce logistic regression. The hidden layer contains the identity function whereas the output layer makes automatically use of the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8271186440677966\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', \n",
    "                    alpha=10**(-1),\n",
    "                    hidden_layer_sizes=1, \n",
    "                    activation='identity', \n",
    "                    random_state=1,\n",
    "                    batch_size='auto')\n",
    "clf.fit(train_X, train_Y)\n",
    "print(metrics.accuracy_score(y_test['Survived'], clf.predict(test_X)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.n_layers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tweak the model a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8406779661016949"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='adam', \n",
    "                    alpha=10**(-6),\n",
    "                    hidden_layer_sizes=(8,8,8,8,8), \n",
    "                    activation='relu', \n",
    "                    random_state=42,\n",
    "                    batch_size=100)\n",
    "clf.fit(train_X, train_Y)\n",
    "metrics.accuracy_score(y_test['Survived'], clf.predict(test_X)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have many layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.n_layers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have even more coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 2)]\n",
      "328\n"
     ]
    }
   ],
   "source": [
    "print([coef.shape for coef in clf.coefs_])\n",
    "print(sum([np.prod(coef.shape) for coef in clf.coefs_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "- Tune the models above for the boston housing data set and the  titanic data set. Explore the all the different tuning options.\n",
    "\n",
    "- Practice with further datasets.\n",
    "\n",
    "- Use the Multi-Layer-Perceptron in the context of your capstone project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"additionl-resources\"></a>\n",
    "## Additional Resources\n",
    "---\n",
    "\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "- [Deep Learning](http://www.deeplearningbook.org/)\n",
    "- [Tensorflow Tutorials](https://github.com/pkmital/tensorflow_tutorials)\n",
    "- [Awesome Tensorflow](https://github.com/jtoy/awesome-tensorflow)\n",
    "- [Tensorflow Examples](https://github.com/aymericdamien/TensorFlow-Examples)\n",
    "- [Mind: How to Build a Neural Network](https://stevenmiller888.github.io/mind-how-to-build-a-neural-network/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "337px",
    "left": "0px",
    "right": "604.295px",
    "top": "62px",
    "width": "126px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
